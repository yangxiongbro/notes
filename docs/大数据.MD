# Hadoop

Hadoop 是一个开源的分布式计算和存储框架，由 Apache 基金会开发和维护。

## 相关概念

### HDFS

Hadoop Distributed File System，Hadoop 分布式文件系统，简称 HDFS。

**命名节点 (NameNode)**

​		命名节点 (NameNode) 是用于指挥其它节点存储的节点。任何一个"文件系统"(File System, FS) 都需要具备根据文件路径映射到文件的功能，命名节点就是用于储存这些映射信息并提供映射服务的计算机，在整个 HDFS 系统中扮演"管理员"的角色，因此一个 HDFS 集群中只有一个命名节点。

**数据节点 (DataNode)**

​		数据节点 (DataNode) 使用来储存数据块的节点。当一个文件被命名节点承认并分块之后将会被储存到被分配的数据节点中去。数据节点具有储存数据、读写数据的功能，其中存储的数据块比较类似于硬盘中的"扇区"概念，是 HDFS 存储的基本单位。

**副命名节点 (Secondary NameNode)**

​		副命名节点 (Secondary NameNode) 别名"次命名节点"，是命名节点的"秘书"。这个形容很贴切，因为它并不能代替命名节点的工作，无论命名节点是否有能力继续工作。它主要负责分摊命名节点的压力、备份命名节点的状态并执行一些管理工作，如果命名节点要求它这样做的话。如果命名节点坏掉了，它也可以提供备份数据以恢复命名节点。副命名节点可以有多个。

![](../pictures/202507/20250709_001.png)





### MapReduce

MapReduce 的含义就像它的名字一样浅显：Map 和 Reduce (映射和规约) 



## 安装配置

[Apache Hadoop](https://hadoop.apache.org/releases.html)

3.4.x 只支持Java 8和Java 11

### 前置环境

```shell
# Java 运行环境
scp H:\Packages\Linux\Java\jdk-8u441-linux-x64.tar.gz young@192.168.31.101:/home/young/packages/
sudo tar -zxvf /home/young/packages/jdk-8u441-linux-x64.tar.gz -C /opt/apps/
sudo mv /opt/apps/jdk1.8.0_441 /opt/apps/jdk1.8.0

# SSH 环境
systemctl enable ssh && systemctl start ssh
```

```shell
scp E:\Need2Sync\Packages\Linux\Hadoop\hadoop-3.4.1.tar.gz young@192.168.31.101:/home/young/packages/

cd /opt/apps/
sudo tar -zxvf /home/young/packages/hadoop-3.4.1.tar.gz -C ./
sudo mv hadoop-3.4.1 hadoop

# 配置环境变量
sudo vim /etc/profile
# export HADOOP_HOME="/opt/apps/hadoop"
# export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source /etc/profile

# 是否成功
hadoop version
# Hadoop 3.4.1
# Source code repository https://github.com/apache/hadoop.git -r 4d7825309348956336b8f06a08322b78422849b1
# Compiled by mthakur on 2024-10-09T14:57Z
# Compiled on platform linux-x86_64
# Compiled with protoc 3.23.4
# From source with checksum 7292fe9dba5e2e44e3a9f763fce3e680
# This command was run using /opt/apps/hadoop/share/hadoop/common/hadoop-common-3.4.1.jar

# 创建运行 Hadoop 的用户
sudo useradd -s /bin/bash -m hadoop
sudo passwd hadoop

sudo mkdir /opt/data/hadoop
sudo mkdir /opt/temp/hadoop
sudo chown -R hadoop /opt/apps/hadoop
sudo chown -R hadoop /opt/data/hadoop
sudo chown -R hadoop /opt/temp/hadoop

# 配置管理员权限
sudo vim /etc/sudoers
hadoop  ALL=(ALL)       ALL
```



### 配置

- workers	记录所有的数据节点的主机名或 IP 地址
- core-site.xml	Hadoop 核心配置
- hdfs-site.xml	HDFS 配置项
- mapred-site.xml	MapReduce 配置项
- yarn-site.xml	YARN 配置项，为 MapReduce 提供资源管理服务



在每个节点进行以下配置



#### workers

`sudo vim $HADOOP_HOME/etc/hadoop/workers `

```shell
192.168.31.102
192.168.31.103
```



#### core-site.xml

`sudo vim $HADOOP_HOME/etc/hadoop/core-site.xml `

```xml
    <!-- 配置 HDFS 主机地址与端口号 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.31.101:9000</value>
    </property>
    <!-- 配置 Hadoop 的临时文件目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:///opt/data/hadoop/tmp</value>
    </property>
```



#### hdfs-site.xml

`sudo vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml `

```xml
    <!-- 每个数据块复制 2 份存储 -->
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    
    <!-- 设置储存命名信息的目录 -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///opt/data/hadoop/hdfs/name</value>
    </property>
```



#### hadoop-env.sh

`sudo vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh`

```shell
export JAVA_HOME=/opt/apps/jdk1.8.0
export HADOOP_PID_DIR=/opt/temp/hadoop

# NameNode 内存限制
export HDFS_NAMENODE_OPTS="-Xmx4g -Xms4g"

# DataNode 内存限制
export HDFS_DATANODE_OPTS="-Xmx2g -Xms2g"

# ResourceManager 内存限制
export YARN_RESOURCEMANAGER_OPTS="-Xmx2g -Xms2g"

# NodeManager 内存限制
export YARN_NODEMANAGER_OPTS="-Xmx2g -Xms2g"

# 历史服务器内存
export MAPRED_HISTORYSERVER_OPTS="-Xmx2g"
```



### 设置SSH免密登录

```shell
su hadoop
# 在NameNode上生成密钥对
ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
# 并将公钥复制到所有节点（包括自己）
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@localhost
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@192.168.31.102
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@192.168.31.103

# 测试，应该不需要密码
ssh hadoop@192.168.31.102
```




### 启动

```shell
su hadoop

# 进入命名节点
# 格式化 HDFS
hdfs namenode -format

# 启动 HDFS
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/stop-dfs.sh
# 启动分三个步骤，分别启动 NameNode、DataNode 和 Secondary NameNode
# 命名节点不存在 DataNode 进程
# 运行 jps 查看 Java 进程
```



配置 systemctl 服务自动启动

[hadoop.service](../files/scripts/server/hadoop.service)





# Flink

