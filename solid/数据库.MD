

# 数据库系统原理





# MySql

## 数据库系统管理

```sql
# 连接数据库
mysql -u root -p

use mysql;

# 查看用户及允许登录的 ip
select user,host from mysql.user;

# 授权用户 root 在所有 ip 登录，拥有所有库的所有表的权限
# grant option：允许该用户在登录数据库时，能给其它用户进行授权操作
# mysql 8 不支持该写法
grant all privileges on *.* to 'root'@'%' identified by 'mysql' with grant option;
# mysql 8 的写法，需要先使用 alter user 创建相同'username'@'ip'才能执行
# 否则会报错 ERROR 1410 (42000): You are not allowed to create a user with GRANT
grant all privileges on *.* to 'root'@'%' with grant option;
grant all privileges on *.* to 'root'@'192.168.31.%' with grant option;
grant all privileges on *.* to 'root'@'1.12.251.106' with grant option;
# 授权的账号需要能访问所有主机才能给其它账号授权，否则会报错

# 更新用户允许登录 mysql 的主机
update user set host='%' where user='root' and host = '%';

# 新建数据库用户，用户允许在ip(localhost)登陆，(允许所有ip则替换为%)
create user 'username'@'ip' identified by 'password';

# 修改密码\创建用户
alter user 'root'@'localhost' identified by 'mysql';

# 删除用户并删除授权
drop user 'username'@'ip';

# 刷新权限
FLUSH PRIVILEGES;

# 查看用户允许登录主机
select user, host from user;

```



## 常用操作

#### 连续的数字、字符、时间





#### 分组合并非分组字段

```sql
group_concat(distinct apn separator '、')
```



#### 分割字符串一行转多行

```sql
CREATE TABLE `reason` (
    `reason` varchar(255) COMMENT '原因',
    `matchers` varchar(255) COMMENT '原因匹配字段'
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

INSERT INTO d_dy_ue_product_reason (reason, matchers) VALUES
('人工客服接入慢', '人工客服接入慢'),
('签到提醒方式不够丰富（如短信提醒、日历提醒等）', '签到提醒方式不够丰富、如短信提醒、日历提醒等');

select
	reason_join.reason as reason,
	concat('%', substring_index(substring_index(reason_join.matchers, '、', help_join.help_topic_id + 1 ), '、', - 1), '%') as reason_matcher
from reason reason_join
join mysql.help_topic help_join on help_join.help_topic_id < (char_length(reason_join.matchers) - char_length(replace(reason_join.matchers, '、', '')) + 1)
```



### 时间

#### 日期格式

```shell
%M 月名字(January……December)
%W 星期名字(Sunday……Saturday)
%D 有英语前缀的月份的日期(1st, 2nd, 3rd, 等等。)
%Y 年, 数字, 4 位
%y 年, 数字, 2 位
%a 缩写的星期名字(Sun……Sat)
%d 月份中的天数, 数字(00……31)
%e 月份中的天数, 数字(0……31)
%m 月, 数字(01……12)
%c 月, 数字(1……12)
%b 缩写的月份名字(Jan……Dec)
%j 一年中的天数(001……366)
%H 小时(00……23)
%k 小时(0……23)
%h 小时(01……12)
%I 小时(01……12)
%l 小时(1……12)
%i 分钟, 数字(00……59)
%r 时间,12 小时(hh:mm:ss [AP]M)
%T 时间,24 小时(hh:mm:ss)
%S 秒(00……59)
%s 秒(00……59)
%p AM或PM
%w 一个星期中的天数(0=Sunday ……6=Saturday )
%U 星期(0……52), 这里星期天是星期的第一天
%u 星期(0……52), 这里星期一是星期的第一
```



#### 字符串转时间

```sql
select str_to_date('2019-01-20 16:01:45', '%Y-%m-%d %H:%i:%s');
```



#### 时间转字符串

```sql
select date_format(now(), '%Y-%m-%d %H:%i:%s');
```



#### 时间转时间戳(10位，秒)

```sql
select unix_timestamp(now());
```



#### 时间戳转时间

```sql
select from_unixtime(1739326170);
```



#### 字符串转时间戳

```sql
select unix_timestamp('2019-01-20');
```



#### 时间戳转字符串

```sql
select from_unixtime(1739326170,'%Y-%m-%d %H:%i:%s');
```





## Linux 系统安装 Mysql

### 下载安装包

https://dev.mysql.com/downloads/mysql



### debian 10 安装 mysql 8

#### 安装

```shell
#先从mysql官网那个下载安装包
#Select Operating System: Linux-Generic
#Select OS Version: Linux-Generic(glibc 2.12)(x86,64-bit)
#Compressed TAR Archive

#解压缩到目录/opt/mysql
sudo tar -xvJf mysql-8.0.19-linux-glibc2.12-x86_64.tar.xz -C /opt/mysql
sudo mv /opt/mysql/mysql-8.0.19-linux-glibc2.12-x86_64 /opt/mysql/8.0.19

#建立data文件夹用于存放数据库文件
sudo mkdir /opt/mysql/data/8.0.19
sudo mkdir /opt/mysql/share/8.0.19
sudo mkdir /opt/mysql/temp/8.0.19
sudo mkdir /var/logs/mysql/8.0.19

#创建软链接（方便操作）
sudo ln -s /opt/mysql/8.0.19 /opt/mysql/mysql
sudo ln -s /opt/mysql/data/8.0.19 /opt/mysql/my-data
sudo ln -s /opt/mysql/share/8.0.19 /opt/mysql/my-share
sudo ln -s /opt/mysql/temp/8.0.19 /opt/mysql/my-temp
sudo ln -s /var/logs/mysql/8.0.19 /var/logs/mysql/logs
sudo touch /var/logs/mysql/logs/.err
sudo touch /opt/mysql/my-share/errmsg.sys
```



#### 配置

##### 数据库配置

[/etc/my.cnf](../files/config/mysql.8.cnf)



##### 服务管理脚本配置修改

`/opt/mysql/mysql/support-files/mysql.server`

```properties
# basedir=
# datadir=
# 更新这两个配置为:
basedir='/opt/mysql/mysql'
datadir='/opt/mysql/my-data'
```



#### 创建用户（组）

```shell
#添加 mysql 用户组(-r 创建系统工作组,系统工作组的组 ID 小于 500)
sudo groupadd -r mysql
#添加 mysql 用户(-r 建立系统帐号,-s /bin/false 参数指定 mysql 用户仅拥有所有权,而没有登录权限)
sudo useradd -r -g mysql -s /bin/false mysql
#修改当前目录拥有者为新建的 mysql 用户
sudo chown -R mysql:mysql /opt/mysql
sudo chown -R mysql:mysql /var/logs/mysql
```




#### 数据库初始化

```shell
#sudo /opt/mysql/mysql/bin/mysqld --verbose --help
#查看 mysqld 相关参数
#安装mysql
sudo /opt/mysql/mysql/bin/mysqld --defaults-file=/etc/my.cnf --user=mysql --basedir=/opt/mysql/mysql --datadir=/opt/mysql/my-data --initialize
#如果报错
#/opt/mysql/8.0.31/bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory
#则执行下面两句
#sudo apt install numactl
#sudo apt install libaio1 libaio-dev

#正常安装之后会显示如下结果：
#2018-09-19T16:31:03.993062Z 0 [System] [MY-013169] [Server] /usr/local/mysql-8.0.12-linux-glibc2.12-x86_64/bin/mysqld (mysqld 8.0.12) initializing of server in progress as process 19261
#2018-09-19T16:31:19.499443Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 4iqfTdLwV9-a
#记下随机产生的密码，我的是jilfIS%a_9d*
%2dNBj;*rCFL
```



#### 服务管理

```shell
#将mysql进程放入系统进程中
sudo ln -s /opt/mysql/mysql/support-files/mysql.server /etc/init.d/mysqld
#注册服务
sudo update-rc.d mysqld defaults

#启动、停止、重新启动mysql服务
sudo service mysqld start
sudo service mysqld stop
sudo service mysqld restart
sudo service mysqld status
#或
sudo systemctl start mysqld.service
sudo systemctl stop mysqld.service
sudo systemctl restart mysqld.service
sudo systemctl status mysqld.service
```




#### 连接数据库/usr/bin

```shell
#在/usr/bin下建立指向mysql的软连接之后使用随机密码登录mysql数据库 
sudo ln -s /opt/mysql/mysql/bin/mysql /usr/bin/mysql

mysql -u root -p
#如报错:mysql: error while loading shared libraries: libtinfo.so.5: cannot open shared object file: No such file or directory
#则查找libtinfo.so*
#sudo find / -name "libtinfo.so*"
#sudo ln -s /usr/lib/x86_64-linux-gnu/libtinfo.so.6 /usr/lib/x86_64-linux-gnu/libtinfo.so.5
```



#### 配置端口监听

```shell
ss -pl
# 看到mysql那行，如果输出 172.0.0.1:mysql 则现在只监听了 localhost 的连接
# 解决方法:修改/etc/mysql/my.cnf文件注释掉bind-address  = 127.0.0.1这行然后重启即可
# 重新ss -pl(*:mysql)说明可以远程连接了
```




#### 注意事项

```shell

```



### my.cnf

[/etc/my.cnf](../files/config/mysql.all.cnf)



## MySQL高级课程简介

| 序号 | Day01                | Day02       | Day03          | Day04          |
| :--: | -------------------- | ----------- | -------------- | -------------- |
|  1   | Linux 系统安装 MySQL | 体系结构    | 应用优化       | MySQL 常用工具 |
|  2   | 索引                 | 存储引擎    | 查询缓存优化   | MySQL 日志     |
|  3   | 视图                 | 优化SQL步骤 | 内存管理及优化 | MySQL 主从复制 |
|  4   | 存储过程和函数       | 索引使用    | MySQL锁问题    | 综合案例       |
|  5   | 触发器               | SQL优化     | 常用SQL技巧    |                |



## 2. 索引

### 2.1 索引概述

MySQL官方对索引的定义为：索引（index）是帮助MySQL高效获取数据的数据结构（有序）。在数据之外，数据库系统还维护者满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。如下面的示意图所示 : 

![1555902055367](D:/documents/notes/pictures/20230315/078.jpg) 

左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找快速获取到相应数据。

一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。索引是数据库中用来提高性能的最常用的工具。



### 2.2 索引优势劣势

优势

1） 类似于书籍的目录索引，提高数据检索的效率，降低数据库的IO成本。

2） 通过索引列对数据进行排序，降低数据排序的成本，降低CPU的消耗。

劣势

1） 实际上索引也是一张表，该表中保存了主键与索引字段，并指向实体类的记录，所以索引列也是要占用空间的。

2） 虽然索引大大提高了查询效率，同时却也降低更新表的速度，如对表进行INSERT、UPDATE、DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。



### 2.3 索引结构

索引是在MySQL的存储引擎层中实现的，而不是在服务器层实现的。所以每种存储引擎的索引都不一定完全相同，也不是所有的存储引擎都支持所有的索引类型的。MySQL目前提供了以下4种索引：

- BTREE 索引 ： 最常见的索引类型，大部分索引都支持 B 树索引。
- HASH 索引：只有Memory引擎支持 ， 使用场景简单 。
- R-tree 索引（空间索引）：空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少，不做特别介绍。
- Full-text （全文索引） ：全文索引也是MyISAM的一个特殊索引类型，主要用于全文索引，InnoDB从Mysql5.6版本开始支持全文索引。

<center><b>MyISAM、InnoDB、Memory三种存储引擎对各种索引类型的支持</b></center>

| 索引        | InnoDB引擎      | MyISAM引擎 | Memory引擎 |
| ----------- | --------------- | ---------- | ---------- |
| BTREE索引   | 支持            | 支持       | 支持       |
| HASH 索引   | 不支持          | 不支持     | 支持       |
| R-tree 索引 | 不支持          | 支持       | 不支持     |
| Full-text   | 5.6版本之后支持 | 支持       | 不支持     |

我们平常所说的索引，如果没有特别指明，都是指B+树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为 索引。



#### 2.3.1 BTREE 结构

BTree又叫多路平衡搜索树，一颗m叉的BTree特性如下：

- 树中每个节点最多包含m个孩子。
- 除根节点与叶子节点外，每个节点至少有[ceil(m/2)]个孩子。
- 若根节点不是叶子节点，则至少有两个孩子。
- 所有的叶子节点都在同一层。
- 每个非叶子节点由n个key与n+1个指针组成，其中[ceil(m/2)-1] <= n <= m-1 。



以5叉BTree为例，key的数量：公式推导[ceil(m/2)-1] <= n <= m-1。所以 2 <= n <=4 。当n>4时，中间节点分裂到父节点，两边节点分裂。

插入 C N G A H E K Q M F W L T Z D P R X Y S 数据为例。

演变过程如下：

1). 插入前4个字母 C N G A 

![1555944126588](D:/documents/notes/pictures/20230315/079.jpg) 

2). 插入H，n>4，中间元素G字母向上分裂到新的节点

![1555944549825](D:/documents/notes/pictures/20230315/080.jpg) 

3). 插入E，K，Q不需要分裂

![1555944596893](D:/documents/notes/pictures/20230315/081.jpg) 

4). 插入M，中间元素M字母向上分裂到父节点G

![1555944652560](D:/documents/notes/pictures/20230315/082.jpg) 

5). 插入F，W，L，T不需要分裂

![1555944686928](D:/documents/notes/pictures/20230315/083.jpg) 

6). 插入Z，中间元素T向上分裂到父节点中 

![1555944713486](D:/documents/notes/pictures/20230315/084.jpg) 

7). 插入D，中间元素D向上分裂到父节点中。然后插入P，R，X，Y不需要分裂

![1555944749984](D:/documents/notes/pictures/20230315/085.jpg) 

8). 最后插入S，NPQR节点n>5，中间节点Q向上分裂，但分裂后父节点DGMT的n>5，中间节点M向上分裂

![1555944848294](D:/documents/notes/pictures/20230315/086.jpg) 

到此，该BTREE树就已经构建完成了， BTREE树 和 二叉树 相比， 查询数据的效率更高， 因为对于相同的数据量来说，BTREE的层级结构比二叉树小，因此搜索速度快。



#### 2.3.3 B+TREE 结构

B+Tree为BTree的变种，B+Tree与BTree的区别为：

1). n叉B+Tree最多含有n个key，而BTree最多含有n-1个key。

2). B+Tree的叶子节点保存所有的key信息，依key大小顺序排列。

3). 所有的非叶子节点都可以看作是key的索引部分。

![1555906287178](D:/documents/notes/pictures/20230315/087.jpg) 

由于B+Tree只有叶子节点保存key信息，查询任何key都要从root走到叶子。所以B+Tree的查询效率更加稳定。



#### 2.3.3 MySQL中的B+Tree

MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。

MySQL中的 B+Tree 索引结构示意图: 

![1555906287178](D:/documents/notes/pictures/20230315/088.jpg)  



### 2.4 索引分类

1） 单值索引 ：即一个索引只包含单个列，一个表可以有多个单列索引

2） 唯一索引 ：索引列的值必须唯一，但允许有空值

3） 复合索引 ：即一个索引包含多个列



### 2.5 索引语法

索引在创建表的时候，可以同时创建， 也可以随时增加新的索引。

准备环境:

```SQL
create database demo_01 default charset=utf8mb4;

use demo_01;

CREATE TABLE `city` (
  `city_id` int(11) NOT NULL AUTO_INCREMENT,
  `city_name` varchar(50) NOT NULL,
  `country_id` int(11) NOT NULL,
  PRIMARY KEY (`city_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

CREATE TABLE `country` (
  `country_id` int(11) NOT NULL AUTO_INCREMENT,
  `country_name` varchar(100) NOT NULL,
  PRIMARY KEY (`country_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;


insert into `city` (`city_id`, `city_name`, `country_id`) values(1,'西安',1);
insert into `city` (`city_id`, `city_name`, `country_id`) values(2,'NewYork',2);
insert into `city` (`city_id`, `city_name`, `country_id`) values(3,'北京',1);
insert into `city` (`city_id`, `city_name`, `country_id`) values(4,'上海',1);

insert into `country` (`country_id`, `country_name`) values(1,'China');
insert into `country` (`country_id`, `country_name`) values(2,'America');
insert into `country` (`country_id`, `country_name`) values(3,'Japan');
insert into `country` (`country_id`, `country_name`) values(4,'UK');
```



#### 2.5.1 创建索引

语法 ： 	

```sql
CREATE 	[UNIQUE|FULLTEXT|SPATIAL]  INDEX index_name 
[USING  index_type]
ON tbl_name(index_col_name,...)


index_col_name : column_name[(length)][ASC | DESC]
```

示例 ： 为city表中的city_name字段创建索引 ；

![1551438009843](D:/documents/notes/pictures/20230315/089.jpg)    ​	  

​	

#### 2.5.2 查看索引

语法： 

```
show index  from  table_name;
```

示例：查看city表中的索引信息；

![1551440511890](D:/documents/notes/pictures/20230315/090.jpg) 

![1551440544483](D:/documents/notes/pictures/20230315/091.jpg) 	 



#### 2.5.3 删除索引

语法 ：

```
DROP  INDEX  index_name  ON  tbl_name;
```

示例 ： 想要删除city表上的索引idx_city_name，可以操作如下：

![1551438238293](D:/documents/notes/pictures/20230315/092.jpg) 	 



#### 2.5.4 ALTER命令

```
1). alter  table  tb_name  add  primary  key(column_list); 

	该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL
	
2). alter  table  tb_name  add  unique index_name(column_list);
	
	这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）
	
3). alter  table  tb_name  add  index index_name(column_list);

	添加普通索引， 索引值可以出现多次。
	
4). alter  table  tb_name  add  fulltext  index_name(column_list);
	
	该语句指定了索引为FULLTEXT， 用于全文索引
	
```



### 2.6 索引设计原则

​	索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率，更高效的使用索引。

- 对查询频次较高，且数据量比较大的表建立索引。

- 索引字段的选择，最佳候选列应当从where子句的条件中提取，如果where子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。

- 使用唯一索引，区分度越高，使用索引的效率越高。

- 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价自然也就水涨船高。对于插入、更新、删除等DML操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低DML操作的效率，增加相应操作的时间消耗。另外索引过多的话，MySQL也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但无疑提高了选择的代价。

- 使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的I/O效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升MySQL访问索引的I/O效率。

- 利用最左前缀，N个列组合而成的组合索引，那么相当于是创建了N个索引，如果查询时where子句中使用了组成该索引的前几个字段，那么这条查询SQL可以利用组合索引来提升查询效率。

  ```
  创建复合索引:
  
  	CREATE INDEX idx_name_email_status ON tb_seller(NAME,email,STATUS);
  
  就相当于
  	对name 创建索引 ;
  	对name , email 创建了索引 ;
  	对name , email, status 创建了索引 ;
  ```


## 3. 视图

### 3.1 视图概述

​	视图（View）是一种虚拟存在的表。视图并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。

视图相对于普通的表的优势主要包括以下几项。

- 简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。
- 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。
- 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。

### 3.2 创建或者修改视图

创建视图的语法为：

```sql
CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]

VIEW view_name [(column_list)]

AS select_statement

[WITH [CASCADED | LOCAL] CHECK OPTION]
```

修改视图的语法为：

```sql
ALTER [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]

VIEW view_name [(column_list)]

AS select_statement

[WITH [CASCADED | LOCAL] CHECK OPTION]
```

```
选项 : 
	WITH [CASCADED | LOCAL] CHECK OPTION 决定了是否允许更新数据使记录不再满足视图的条件。
	
	LOCAL ： 只要满足本视图的条件就可以更新。
	CASCADED ： 必须满足所有针对该视图的所有视图的条件才可以更新。 默认值.
```

示例 , 创建city_country_view视图 , 执行如下SQL : 

```sql
create or replace view city_country_view 
as 
select t.*,c.country_name from country c , city t where c.country_id = t.country_id;

```

查询视图 : 

![1551503428635](D:/documents/notes/pictures/20230315/093.jpg) 	



### 3.3 查看视图

​	从 MySQL 5.1 版本开始，使用 SHOW TABLES 命令的时候不仅显示表的名字，同时也会显示视图的名字，而不存在单独显示视图的 SHOW VIEWS 命令。

![1551537565159](D:/documents/notes/pictures/20230315/094.jpg)	 

同样，在使用 SHOW TABLE STATUS 命令的时候，不但可以显示表的信息，同时也可以显示视图的信息。	

![1551537646323](D:/documents/notes/pictures/20230315/095.jpg) 

如果需要查询某个视图的定义，可以使用 SHOW CREATE VIEW 命令进行查看 ： 

![1551588962944](D:/documents/notes/pictures/20230315/096.jpg)  

### 3.4 删除视图

语法 : 

```sql
DROP VIEW [IF EXISTS] view_name [, view_name] ...[RESTRICT | CASCADE]	
```

示例 , 删除视图city_country_view :

```sql
DROP VIEW city_country_view ;
```



## 4. 存储过程和函数

### 4.1 存储过程和函数概述

​	存储过程和函数是  事先经过编译并存储在数据库中的一段 SQL 语句的集合，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。	

​	存储过程和函数的区别在于函数必须有返回值，而存储过程没有。

​	函数 ： 是一个有返回值的过程 ；

​	过程 ： 是一个没有返回值的函数 ；

### 4.2 创建存储过程

```sql
CREATE PROCEDURE procedure_name ([proc_parameter[,...]])
begin
	-- SQL语句
end ;
```



示例 ：

```sql 
delimiter $

create procedure pro_test1()
begin
	select 'Hello Mysql' ;
end$

delimiter ;
```



<strong><font color="red">知识小贴士</font></strong>

DELIMITER

​	该关键字用来声明SQL语句的分隔符 , 告诉 MySQL 解释器，该段命令是否已经结束了，mysql是否可以执行了。默认情况下，delimiter是分号;。在命令行客户端中，如果有一行命令以分号结束，那么回车后，mysql将会执行该命令。



### 4.3 调用存储过程

```sql
call procedure_name() ;	
```

### 4.4 查看存储过程

```sql
-- 查询db_name数据库中的所有的存储过程
select name from mysql.proc where db='db_name';


-- 查询存储过程的状态信息
show procedure status;


-- 查询某个存储过程的定义
show create procedure test.pro_test1 \G;
```

### 4.5 删除存储过程

```sql
DROP PROCEDURE  [IF EXISTS] sp_name ；
```



### 4.6 语法

存储过程是可以编程的，意味着可以使用变量，表达式，控制结构 ， 来完成比较复杂的功能。

#### 4.6.1 变量

- DECLARE

  通过 DECLARE 可以定义一个局部变量，该变量的作用范围只能在 BEGIN…END 块中。

```sql
DECLARE var_name[,...] type [DEFAULT value]
```

示例 : 

```sql
 delimiter $

 create procedure pro_test2() 
 begin 
 	declare num int default 5;
 	select num+ 10; 
 end$

 delimiter ; 
```



- SET

直接赋值使用 SET，可以赋常量或者赋表达式，具体语法如下：

```
  SET var_name = expr [, var_name = expr] ...
```

示例 : 

```sql
  DELIMITER $
  
  CREATE  PROCEDURE pro_test3()
  BEGIN
  	DECLARE NAME VARCHAR(20);
  	SET NAME = 'MYSQL';
  	SELECT NAME ;
  END$
  
  DELIMITER ;
```



也可以通过select ... into 方式进行赋值操作 :

```SQL
DELIMITER $

CREATE  PROCEDURE pro_test5()
BEGIN
	declare  countnum int;
	select count(*) into countnum from city;
	select countnum;
END$

DELIMITER ;
```



#### 4.6.2 if条件判断

语法结构 : 

```sql
if search_condition then statement_list

	[elseif search_condition then statement_list] ...
	
	[else statement_list]
	
end if;
```

需求： 

```
根据定义的身高变量，判定当前身高的所属的身材类型 

	180 及以上 ----------> 身材高挑

	170 - 180  ---------> 标准身材

	170 以下  ----------> 一般身材
```

示例 : 

```sql
delimiter $

create procedure pro_test6()
begin
  declare  height  int  default  175; 
  declare  description  varchar(50);
  
  if  height >= 180  then
    set description = '身材高挑';
  elseif height >= 170 and height < 180  then
    set description = '标准身材';
  else
    set description = '一般身材';
  end if;
  
  select description ;
end$

delimiter ;
```

调用结果为 : 

![1552057035580](D:/documents/notes/pictures/20230315/097.png) 



#### 4.6.3 传递参数

语法格式 : 

```
create procedure procedure_name([in/out/inout] 参数名   参数类型)
...


IN :   该参数可以作为输入，也就是需要调用方传入值 , 默认
OUT:   该参数作为输出，也就是该参数可以作为返回值
INOUT: 既可以作为输入参数，也可以作为输出参数
```

**IN - 输入**

需求 :

```
根据定义的身高变量，判定当前身高的所属的身材类型 
```

示例  : 

```sql
delimiter $

create procedure pro_test5(in height int)
begin
    declare description varchar(50) default '';
  if height >= 180 then
    set description='身材高挑';
  elseif height >= 170 and height < 180 then
    set description='标准身材';
  else
    set description='一般身材';
  end if;
  select concat('身高 ', height , '对应的身材类型为:',description);
end$

delimiter ;
```



**OUT-输出**

 需求 :

```
根据传入的身高变量，获取当前身高的所属的身材类型  
```

示例:

```SQL 
create procedure pro_test5(in height int , out description varchar(100))
begin
  if height >= 180 then
    set description='身材高挑';
  elseif height >= 170 and height < 180 then
    set description='标准身材';
  else
    set description='一般身材';
  end if;
end$	
```

调用:

```
call pro_test5(168, @description)$

select @description$
```

<font color='red'>**小知识** </font>

@description :  这种变量要在变量名称前面加上“@”符号，叫做用户会话变量，代表整个会话过程他都是有作用的，这个类似于全局变量一样。

@@global.sort_buffer_size : 这种在变量前加上 "@@" 符号, 叫做 系统变量 



#### 4.6.4 case结构

语法结构 : 

```SQL
方式一 : 

CASE case_value

  WHEN when_value THEN statement_list
  
  [WHEN when_value THEN statement_list] ...
  
  [ELSE statement_list]
  
END CASE;


方式二 : 

CASE

  WHEN search_condition THEN statement_list
  
  [WHEN search_condition THEN statement_list] ...
  
  [ELSE statement_list]
  
END CASE;

```

需求:

```
给定一个月份, 然后计算出所在的季度
```

示例  :

```sql
delimiter $


create procedure pro_test9(month int)
begin
  declare result varchar(20);
  case 
    when month >= 1 and month <=3 then 
      set result = '第一季度';
    when month >= 4 and month <=6 then 
      set result = '第二季度';
    when month >= 7 and month <=9 then 
      set result = '第三季度';
    when month >= 10 and month <=12 then 
      set result = '第四季度';
  end case;
  
  select concat('您输入的月份为 :', month , ' , 该月份为 : ' , result) as content ;
  
end$


delimiter ;
```



#### 4.6.5 while循环

语法结构: 

```sql
while search_condition do

	statement_list
	
end while;
```

需求:

```
计算从1加到n的值
```

示例  : 

```sql
delimiter $

create procedure pro_test8(n int)
begin
  declare total int default 0;
  declare num int default 1;
  while num<=n do
    set total = total + num;
	set num = num + 1;
  end while;
  select total;
end$

delimiter ;
```



#### 4.6.6 repeat结构

有条件的循环控制语句, 当满足条件的时候退出循环 。while 是满足条件才执行，repeat 是满足条件就退出循环。

语法结构 : 

```SQL
REPEAT

  statement_list

  UNTIL search_condition

END REPEAT;
```

需求: 

```
计算从1加到n的值
```

示例  : 

```sql
delimiter $

create procedure pro_test10(n int)
begin
  declare total int default 0;
  
  repeat 
    set total = total + n;
    set n = n - 1;
    until n=0  
  end repeat;
  
  select total ;
  
end$


delimiter ;
```



#### 4.6.7 loop语句

LOOP 实现简单的循环，退出循环的条件需要使用其他的语句定义，通常可以使用 LEAVE 语句实现，具体语法如下：

```sql
[begin_label:] LOOP

  statement_list

END LOOP [end_label]
```

如果不在 statement_list 中增加退出循环的语句，那么 LOOP 语句可以用来实现简单的死循环。



#### 4.6.8 leave语句

用来从标注的流程构造中退出，通常和 BEGIN ... END 或者循环一起使用。下面是一个使用 LOOP 和 LEAVE 的简单例子 , 退出循环：

```SQL
delimiter $

CREATE PROCEDURE pro_test11(n int)
BEGIN
  declare total int default 0;
  
  ins: LOOP
    
    IF n <= 0 then
      leave ins;
    END IF;
    
    set total = total + n;
    set n = n - 1;
  	
  END LOOP ins;
  
  select total;
END$

delimiter ;
```



#### 4.6.9 游标/光标

游标是用来存储查询结果集的数据类型 , 在存储过程和函数中可以使用光标对结果集进行循环的处理。光标的使用包括光标的声明、OPEN、FETCH 和 CLOSE，其语法分别如下。

声明光标：

```sql
DECLARE cursor_name CURSOR FOR select_statement ;
```

OPEN 光标：

```sql
OPEN cursor_name ;
```

FETCH 光标：

```sql
FETCH cursor_name INTO var_name [, var_name] ...
```

CLOSE 光标：

```sql
CLOSE cursor_name ;
```



示例 : 

初始化脚本:

``` sql
create table emp(
  id int(11) not null auto_increment ,
  name varchar(50) not null comment '姓名',
  age int(11) comment '年龄',
  salary int(11) comment '薪水',
  primary key(`id`)
)engine=innodb default charset=utf8 ;

insert into emp(id,name,age,salary) values(null,'金毛狮王',55,3800),(null,'白眉鹰王',60,4000),(null,'青翼蝠王',38,2800),(null,'紫衫龙王',42,1800);

```



``` SQL
-- 查询emp表中数据, 并逐行获取进行展示
create procedure pro_test11()
begin
  declare e_id int(11);
  declare e_name varchar(50);
  declare e_age int(11);
  declare e_salary int(11);
  declare emp_result cursor for select * from emp;
  
  open emp_result;
  
  fetch emp_result into e_id,e_name,e_age,e_salary;
  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);
  
  fetch emp_result into e_id,e_name,e_age,e_salary;
  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);
  
  fetch emp_result into e_id,e_name,e_age,e_salary;
  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);
  
  fetch emp_result into e_id,e_name,e_age,e_salary;
  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);
  
  fetch emp_result into e_id,e_name,e_age,e_salary;
  select concat('id=',e_id , ', name=',e_name, ', age=', e_age, ', 薪资为: ',e_salary);
  
  close emp_result;
end$

```



通过循环结构 , 获取游标中的数据 : 

```sql
DELIMITER $

create procedure pro_test12()
begin
  DECLARE id int(11);
  DECLARE name varchar(50);
  DECLARE age int(11);
  DECLARE salary int(11);
  DECLARE has_data int default 1;
  
  DECLARE emp_result CURSOR FOR select * from emp;
  -- 声明一个句柄事件，当游标走到尽头时，就会发生 NOT FOUND 事件
  DECLARE EXIT HANDLER FOR NOT FOUND set has_data = 0;
  
  open emp_result;
  
  repeat
    fetch emp_result into id , name , age , salary;
    select concat('id为',id, ', name 为' ,name , ', age为 ' ,age , ', 薪水为: ', salary);
    until has_data = 0
  end repeat;
  
  close emp_result;
end$

DELIMITER ; 
```



### 4.7 存储函数

语法结构:

``` 
CREATE FUNCTION function_name([param type ... ]) 
RETURNS type 
BEGIN
	...
END;
```

案例 : 

定义一个存储过程, 请求满足条件的总记录数 ;

```SQL
delimiter $

create function count_city(countryId int)
returns int
begin
  declare cnum int ;
  
  select count(*) into cnum from city where country_id = countryId;
  
  return cnum;
end$

delimiter ;
```

调用: 

```
select count_city(1);

select count_city(2);
```



## 5. 触发器

### 5.1 介绍

触发器是与表有关的数据库对象，指在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性 , 日志记录 , 数据校验等操作 。

使用别名 OLD 和 NEW 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。

| 触发器类型      | NEW 和 OLD的使用                                        |
| --------------- | ------------------------------------------------------- |
| INSERT 型触发器 | NEW 表示将要或者已经新增的数据                          |
| UPDATE 型触发器 | OLD 表示修改之前的数据 , NEW 表示将要或已经修改后的数据 |
| DELETE 型触发器 | OLD 表示将要或者已经删除的数据                          |



### 5.2 创建触发器

语法结构 : 

```sql
create trigger trigger_name 

before/after insert/update/delete

on tbl_name 

[ for each row ]  -- 行级触发器

begin

	trigger_stmt ;

end;
```



示例 

需求

```
通过触发器记录 emp 表的数据变更日志 , 包含增加, 修改 , 删除 ;
```

首先创建一张日志表 : 

```sql
create table emp_logs(
  id int(11) not null auto_increment,
  operation varchar(20) not null comment '操作类型, insert/update/delete',
  operate_time datetime not null comment '操作时间',
  operate_id int(11) not null comment '操作表的ID',
  operate_params varchar(500) comment '操作参数',
  primary key(`id`)
)engine=innodb default charset=utf8;
```

创建 insert 型触发器，完成插入数据时的日志记录 : 

```sql
DELIMITER $

create trigger emp_logs_insert_trigger
after insert 
on emp 
for each row 
begin
  insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'insert',now(),new.id,concat('插入后(id:',new.id,', name:',new.name,', age:',new.age,', salary:',new.salary,')'));	
end $

DELIMITER ;
```

创建 update 型触发器，完成更新数据时的日志记录 : 

``` sql
DELIMITER $

create trigger emp_logs_update_trigger
after update 
on emp 
for each row 
begin
  insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'update',now(),new.id,concat('修改前(id:',old.id,', name:',old.name,', age:',old.age,', salary:',old.salary,') , 修改后(id',new.id, 'name:',new.name,', age:',new.age,', salary:',new.salary,')'));                                                                      
end $

DELIMITER ;
```

创建delete 行的触发器 , 完成删除数据时的日志记录 : 

```sql
DELIMITER $

create trigger emp_logs_delete_trigger
after delete 
on emp 
for each row 
begin
  insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,'delete',now(),old.id,concat('删除前(id:',old.id,', name:',old.name,', age:',old.age,', salary:',old.salary,')'));                                                                      
end $

DELIMITER ;
```



测试：

```sql
insert into emp(id,name,age,salary) values(null, '光明左使',30,3500);
insert into emp(id,name,age,salary) values(null, '光明右使',33,3200);

update emp set age = 39 where id = 3;

delete from emp where id = 5;
```



### 5.3 删除触发器

语法结构 : 

```sql
drop trigger [schema_name.]trigger_name
```

如果没有指定 schema_name，默认为当前数据库 。

### 5.4 查看触发器

可以通过执行 SHOW TRIGGERS 命令查看触发器的状态、语法等信息。

语法结构 ： 

```sql
show triggers ；
```

 



## 1. Mysql的体系结构概览

![171214401286615](D:/documents/notes/pictures/20230315/098.png) 

整个MySQL Server由以下组成

- Connection Pool : 连接池组件
- Management Services & Utilities : 管理服务和工具组件
- SQL Interface : SQL接口组件
- Parser : 查询分析器组件
- Optimizer : 优化器组件
- Caches & Buffers : 缓冲池组件
- Pluggable Storage Engines : 存储引擎
- File System : 文件系统



1） 连接层

最上层是一些客户端和链接服务，包含本地sock 通信和大多数基于客户端/服务端工具实现的类似于 TCP/IP的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。

2） 服务层

第二层架构主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化，部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如 过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询的顺序，是否利用索引等， 最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。

3） 引擎层

存储引擎层， 存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API和存储引擎进行通信。不同的存储引擎具有不同的功能，这样我们可以根据自己的需要，来选取合适的存储引擎。

4）存储层

数据存储层， 主要是将数据存储在文件系统之上，并完成与存储引擎的交互。



和其他数据库相比，MySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎上，插件式的存储引擎架构，将查询处理和其他的系统任务以及数据的存储提取分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。



## 2. 存储引擎

### 2.1 存储引擎概述

​	和大多数的数据库不同, MySQL中有一个存储引擎的概念, 针对不同的存储需求可以选择最优的存储引擎。

​	存储引擎就是存储数据，建立索引，更新查询数据等等技术的实现方式 。存储引擎是基于表的，而不是基于库的。所以存储引擎也可被称为表类型。

​	Oracle，SqlServer等数据库只有一种存储引擎。MySQL提供了插件式的存储引擎架构。所以MySQL存在多种存储引擎，可以根据需要使用相应引擎，或者编写存储引擎。

​	MySQL5.0支持的存储引擎包含 ： InnoDB 、MyISAM 、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED等，其中InnoDB和BDB提供事务安全表，其他存储引擎是非事务安全表。

可以通过指定 show engines ， 来查询当前数据库支持的存储引擎 ： 

![1551186043529](D:/documents/notes/pictures/20230315/099.png) 

创建新表时如果不指定存储引擎，那么系统就会使用默认的存储引擎，MySQL5.5之前的默认存储引擎是MyISAM，5.5之后就改为了InnoDB。

查看Mysql数据库默认的存储引擎 ， 指令 ：

```
 show variables like '%storage_engine%' ； 
```

![1556086372754](D:/documents/notes/pictures/20230315/100.png)  	 



### 2.2 各种存储引擎特性

下面重点介绍几种常用的存储引擎， 并对比各个存储引擎之间的区别， 如下表所示 ： 

| 特点         | InnoDB               | MyISAM   | MEMORY | MERGE | NDB  |
| ------------ | -------------------- | -------- | ------ | ----- | ---- |
| 存储限制     | 64TB                 | 有       | 有     | 没有  | 有   |
| 事务安全     | ==支持==             |          |        |       |      |
| 锁机制       | ==行锁(适合高并发)== | ==表锁== | 表锁   | 表锁  | 行锁 |
| B树索引      | 支持                 | 支持     | 支持   | 支持  | 支持 |
| 哈希索引     |                      |          | 支持   |       |      |
| 全文索引     | 支持(5.6版本之后)    | 支持     |        |       |      |
| 集群索引     | 支持                 |          |        |       |      |
| 数据索引     | 支持                 |          | 支持   |       | 支持 |
| 索引缓存     | 支持                 | 支持     | 支持   | 支持  | 支持 |
| 数据可压缩   |                      | 支持     |        |       |      |
| 空间使用     | 高                   | 低       | N/A    | 低    | 低   |
| 内存使用     | 高                   | 低       | 中等   | 低    | 高   |
| 批量插入速度 | 低                   | 高       | 高     | 高    | 高   |
| 支持外键     | ==支持==             |          |        |       |      |

下面我们将重点介绍最常使用的两种存储引擎： InnoDB、MyISAM ， 另外两种 MEMORY、MERGE ， 了解即可。



#### 2.2.1 InnoDB

​	InnoDB存储引擎是Mysql的默认存储引擎。InnoDB存储引擎提供了具有提交、回滚、崩溃恢复能力的事务安全。但是对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。

InnoDB存储引擎不同于其他存储引擎的特点 ： 

**事务控制**

```sql
create table goods_innodb(
	id int NOT NULL AUTO_INCREMENT,
	name varchar(20) NOT NULL,
    primary key(id)
)ENGINE=innodb DEFAULT CHARSET=utf8;
```

```sql
start transaction;

insert into goods_innodb(id,name)values(null,'Meta20');

commit;
```

![1556075130115](D:/documents/notes/pictures/20230315/101.png) 

测试，发现在InnoDB中是存在事务的 ；



**外键约束**

​	MySQL支持外键的存储引擎只有InnoDB ， 在创建外键的时候， 要求父表必须有对应的索引 ， 子表在创建外键的时候， 也会自动的创建对应的索引。

​	下面两张表中 ， country_innodb是父表 ， country_id为主键索引，city_innodb表是子表，country_id字段为外键，对应于country_innodb表的主键country_id 。

```sql
create table country_innodb(
	country_id int NOT NULL AUTO_INCREMENT,
    country_name varchar(100) NOT NULL,
    primary key(country_id)
)ENGINE=InnoDB DEFAULT CHARSET=utf8;


create table city_innodb(
	city_id int NOT NULL AUTO_INCREMENT,
    city_name varchar(50) NOT NULL,
    country_id int NOT NULL,
    primary key(city_id),
    key idx_fk_country_id(country_id),
    CONSTRAINT `fk_city_country` FOREIGN KEY(country_id) REFERENCES country_innodb(country_id) ON DELETE RESTRICT ON UPDATE CASCADE
)ENGINE=InnoDB DEFAULT CHARSET=utf8;



insert into country_innodb values(null,'China'),(null,'America'),(null,'Japan');
insert into city_innodb values(null,'Xian',1),(null,'NewYork',2),(null,'BeiJing',1);

```

在创建索引时， 可以指定在删除、更新父表时，对子表进行的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION。

- RESTRICT和NO ACTION相同， 是指限制在子表有关联记录的情况下， 父表不能更新；

- CASCADE表示父表在更新或者删除时，更新或者删除子表对应的记录；

- SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被SET NULL 。


针对上面创建的两个表， 子表的外键指定是ON DELETE RESTRICT ON UPDATE CASCADE 方式的， 那么在主表删除记录的时候， 如果子表有对应记录， 则不允许删除， 主表在更新记录的时候， 如果子表有对应记录， 则子表对应更新 。

表中数据如下图所示 ： 

![1556087540767](D:/documents/notes/pictures/20230315/102.png) 



外键信息可以使用如下两种方式查看 ： 

```sql
show create table city_innodb ;
```

![1556087611295](D:/documents/notes/pictures/20230315/103.png) 	



删除country_id为1 的country数据： 

```sql
 delete from country_innodb where country_id = 1;
```

![1556087719145](D:/documents/notes/pictures/20230315/104.png) 

更新主表country表的字段 country_id : 

```sql
update country_innodb set country_id = 100 where country_id = 1;
```

![1556087759615](D:/documents/notes/pictures/20230315/105.png)  

更新后， 子表的数据信息为 ： 

![1556087793738](D:/documents/notes/pictures/20230315/106.png)  



**存储方式**	

InnoDB 存储表和索引有以下两种方式 ： 

①. 使用共享表空间存储， 这种方式创建的表的表结构保存在.frm文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。

②. 使用多表空间存储， 这种方式创建的表的表结构仍然存在 .frm 文件中，但是每个表的数据和索引单独保存在 .ibd 中。

![1556075336630](D:/documents/notes/pictures/20230315/107.png) 



#### 2.2.2 MyISAM

​	MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。有以下两个比较重要的特点： 

**不支持事务**

```sql
create table goods_myisam(
	id int NOT NULL AUTO_INCREMENT,
	name varchar(20) NOT NULL,
    primary key(id)
)ENGINE=myisam DEFAULT CHARSET=utf8;
```

![1551347590309](D:/documents/notes/pictures/20230315/108.png) 

通过测试，我们发现，在MyISAM存储引擎中，是没有事务控制的 ；



**文件存储方式**

每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但拓展名分别是 ： 

.frm (存储表定义)；

.MYD(MYData , 存储数据)；

.MYI(MYIndex , 存储索引)；

![1556075073836](D:/documents/notes/pictures/20230315/109.png) 



#### 2.2.3 MEMORY

​	Memory存储引擎将表的数据存放在内存中。每个MEMORY表实际对应一个磁盘文件，格式是.frm ，该文件中只存储表的结构，而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。MEMORY 类型的表访问非常地快，因为他的数据是存放在内存中的，并且默认使用HASH索引 ， 但是服务一旦关闭，表中的数据就会丢失。



#### 2.2.4 MERGE

​	MERGE存储引擎是一组MyISAM表的组合，这些MyISAM表必须结构完全相同，MERGE表本身并没有存储数据，对MERGE类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的MyISAM表进行的。

​	对于MERGE类型表的插入操作，是通过INSERT_METHOD子句定义插入的表，可以有3个不同的值，使用FIRST 或 LAST 值使得插入操作被相应地作用在第一或者最后一个表上，不定义这个子句或者定义为NO，表示不能对这个MERGE表执行插入操作。

​	可以对MERGE表进行DROP操作，但是这个操作只是删除MERGE表的定义，对内部的表是没有任何影响的。

![1556076359503](D:/documents/notes/pictures/20230315/110.png) 

下面是一个创建和使用MERGE表的示例 ： 

1）. 创建3个测试表 order_1990, order_1991, order_all , 其中order_all是前两个表的MERGE表 ： 

```sql
create table order_1990(
	order_id int ,
	order_money double(10,2),
	order_address varchar(50),
	primary key (order_id)
)engine = myisam default charset=utf8;


create table order_1991(
	order_id int ,
	order_money double(10,2),
	order_address varchar(50),
	primary key (order_id)
)engine = myisam default charset=utf8;


create table order_all(
	order_id int ,
	order_money double(10,2),
	order_address varchar(50),
	primary key (order_id)
)engine = merge union = (order_1990,order_1991) INSERT_METHOD=LAST default charset=utf8;


```

2）. 分别向两张表中插入记录 

```sql
insert into order_1990 values(1,100.0,'北京');
insert into order_1990 values(2,100.0,'上海');

insert into order_1991 values(10,200.0,'北京');
insert into order_1991 values(11,200.0,'上海');
```

3）. 查询3张表中的数据。

order_1990中的数据 ： 

![1551408083254](D:/documents/notes/pictures/20230315/111.png) 

order_1991中的数据 ： 

![1551408133323](D:/documents/notes/pictures/20230315/112.png)  

order_all中的数据 ：

![1551408216185](D:/documents/notes/pictures/20230315/113.png) 

​	 

4）. 往order_all中插入一条记录 ，由于在MERGE表定义时，INSERT_METHOD 选择的是LAST，那么插入的数据会想最后一张表中插入。

```sql
insert into order_all values(100,10000.0,'西安')；
```

![1551408519889](D:/documents/notes/pictures/20230315/114.png) 	 	



### 2.3 存储引擎的选择

​	在选择存储引擎时，应该根据应用系统的特点选择合适的存储引擎。对于复杂的应用系统，还可以根据实际情况选择多种存储引擎进行组合。以下是几种常用的存储引擎的使用环境。

- InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。
- MyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。
- MEMORY：将所有数据保存在RAM中，在需要快速定位记录和其他类似数据环境下，可以提供几块的访问。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据库异常终止后表中的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果。
- MERGE：用于将一系列等同的MyISAM表以逻辑方式组合在一起，并作为一个对象引用他们。MERGE表的优点在于可以突破对单个MyISAM表的大小限制，并且通过将不同的表分布在多个磁盘上，可以有效的改善MERGE表的访问效率。这对于存储诸如数据仓储等VLDB环境十分合适。



## 3. 优化SQL步骤

在应用的的开发过程中，由于初期数据量小，开发人员写 SQL 语句时更重视功能上的实现，但是当应用系统正式上线后，随着生产数据量的急剧增长，很多 SQL 语句开始逐渐显露出性能问题，对生产的影响也越来越大，此时这些有问题的 SQL 语句就成为整个系统性能的瓶颈，因此我们必须要对它们进行优化，本章将详细介绍在 MySQL 中优化 SQL 语句的方法。

当面对一个有 SQL 性能问题的数据库时，我们应该从何处入手来进行系统的分析，使得能够尽快定位问题 SQL 并尽快解决问题。

### 3.1 查看SQL执行频率

MySQL 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息。show [session|global] status 可以根据需要加上参数“session”或者“global”来显示 session 级（当前连接）的计结果和 global 级（自数据库上次启动至今）的统计结果。如果不写，默认使用参数是“session”。

下面的命令显示了当前 session 中所有统计参数的值：

```sql
show status like 'Com_______';
```

![1552487172501](D:/documents/notes/pictures/20230315/115.png)  

```sql
show status like 'Innodb_rows_%';
```

![1552487245859](D:/documents/notes/pictures/20230315/116.png)

Com_xxx 表示每个 xxx 语句执行的次数，我们通常比较关心的是以下几个统计参数。

| 参数                 | 含义                                                         |
| :------------------- | ------------------------------------------------------------ |
| Com_select           | 执行 select 操作的次数，一次查询只累加 1。                   |
| Com_insert           | 执行 INSERT 操作的次数，对于批量插入的 INSERT 操作，只累加一次。 |
| Com_update           | 执行 UPDATE 操作的次数。                                     |
| Com_delete           | 执行 DELETE 操作的次数。                                     |
| Innodb_rows_read     | select 查询返回的行数。                                      |
| Innodb_rows_inserted | 执行 INSERT 操作插入的行数。                                 |
| Innodb_rows_updated  | 执行 UPDATE 操作更新的行数。                                 |
| Innodb_rows_deleted  | 执行 DELETE 操作删除的行数。                                 |
| Connections          | 试图连接 MySQL 服务器的次数。                                |
| Uptime               | 服务器工作时间。                                             |
| Slow_queries         | 慢查询的次数。                                               |

Com_***      :  这些参数对于所有存储引擎的表操作都会进行累计。

Innodb_*** :  这几个参数只是针对InnoDB 存储引擎的，累加的算法也略有不同。



### 3.2 定位低效率执行SQL

可以通过以下两种方式定位执行效率较低的 SQL 语句。

- 慢查询日志 : 通过慢查询日志定位那些执行效率较低的 SQL 语句，用--log-slow-queries[=file_name]选项启动时，mysqld 写一个包含所有执行时间超过 long_query_time 秒的 SQL 语句的日志文件。具体可以查看本书第 26 章中日志管理的相关部分。
- show processlist  : 慢查询日志在查询结束以后才纪录，所以在应用反映执行效率出现问题的时候查询慢查询日志并不能定位问题，可以使用show processlist命令查看当前MySQL在进行的线程，包括线程的状态、是否锁表等，可以实时地查看 SQL 的执行情况，同时对一些锁表操作进行优化。

![1556098544349](D:/documents/notes/pictures/20230315/117.png) 

```
1） id列，用户登录mysql时，系统分配的"connection_id"，可以使用函数connection_id()查看

2） user列，显示当前用户。如果不是root，这个命令就只显示用户权限范围的sql语句

3） host列，显示这个语句是从哪个ip的哪个端口上发的，可以用来跟踪出现问题语句的用户

4） db列，显示这个进程目前连接的是哪个数据库

5） command列，显示当前连接的执行的命令，一般取值为休眠（sleep），查询（query），连接（connect）等

6） time列，显示这个状态持续的时间，单位是秒

7） state列，显示使用当前连接的sql语句的状态，很重要的列。state描述的是语句执行中的某一个状态。一个sql语句，以查询为例，可能需要经过copying to tmp table、sorting result、sending data等状态才可以完成

8） info列，显示这个sql语句，是判断问题语句的一个重要依据
```



### 3.3 explain分析执行计划

通过以上步骤查询到效率低的 SQL 语句后，可以通过 EXPLAIN或者 DESC命令获取 MySQL如何执行 SELECT 语句的信息，包括在 SELECT 语句执行过程中表如何连接和连接的顺序。

查询SQL语句的执行计划 ： 

```sql
explain  select * from tb_item where id = 1;
```

![1552487489859](D:/documents/notes/pictures/20230315/118.png)

```sql
explain  select * from tb_item where title = '阿尔卡特 (OT-979) 冰川白 联通3G手机3';
```

![1552487526919](D:/documents/notes/pictures/20230315/119.png)  

| 字段          | 含义                                                         |
| ------------- | ------------------------------------------------------------ |
| id            | select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。 |
| select_type   | 表示 SELECT 的类型，常见的取值有 SIMPLE（简单表，即不使用表连接或者子查询）、PRIMARY（主查询，即外层的查询）、UNION（UNION 中的第二个或者后面的查询语句）、SUBQUERY（子查询中的第一个 SELECT）等 |
| table         | 输出结果集的表                                               |
| type          | 表示表的连接类型，性能由好到差的连接类型为( system  --->  const  ----->  eq_ref  ------>  ref  ------->  ref_or_null---->  index_merge  --->  index_subquery  ----->  range  ----->  index  ------> all ) |
| possible_keys | 表示查询时，可能使用的索引                                   |
| key           | 表示实际使用的索引                                           |
| key_len       | 索引字段的长度                                               |
| rows          | 扫描行的数量                                                 |
| extra         | 执行情况的说明和描述                                         |



#### 3.3.1 环境准备

![1556122799330](D:/documents/notes/pictures/20230315/120.png) 

```sql
CREATE TABLE `t_role` (
  `id` varchar(32) NOT NULL,
  `role_name` varchar(255) DEFAULT NULL,
  `role_code` varchar(255) DEFAULT NULL,
  `description` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_role_name` (`role_name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;


CREATE TABLE `t_user` (
  `id` varchar(32) NOT NULL,
  `username` varchar(45) NOT NULL,
  `password` varchar(96) NOT NULL,
  `name` varchar(45) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_user_username` (`username`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;


CREATE TABLE `user_role` (
  `id` int(11) NOT NULL auto_increment ,
  `user_id` varchar(32) DEFAULT NULL,
  `role_id` varchar(32) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `fk_ur_user_id` (`user_id`),
  KEY `fk_ur_role_id` (`role_id`),
  CONSTRAINT `fk_ur_role_id` FOREIGN KEY (`role_id`) REFERENCES `t_role` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION,
  CONSTRAINT `fk_ur_user_id` FOREIGN KEY (`user_id`) REFERENCES `t_user` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION
) ENGINE=InnoDB DEFAULT CHARSET=utf8;




insert into `t_user` (`id`, `username`, `password`, `name`) values('1','super','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','超级管理员');
insert into `t_user` (`id`, `username`, `password`, `name`) values('2','admin','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','系统管理员');
insert into `t_user` (`id`, `username`, `password`, `name`) values('3','itcast','$2a$10$8qmaHgUFUAmPR5pOuWhYWOr291WJYjHelUlYn07k5ELF8ZCrW0Cui','test02');
insert into `t_user` (`id`, `username`, `password`, `name`) values('4','stu1','$2a$10$pLtt2KDAFpwTWLjNsmTEi.oU1yOZyIn9XkziK/y/spH5rftCpUMZa','学生1');
insert into `t_user` (`id`, `username`, `password`, `name`) values('5','stu2','$2a$10$nxPKkYSez7uz2YQYUnwhR.z57km3yqKn3Hr/p1FR6ZKgc18u.Tvqm','学生2');
insert into `t_user` (`id`, `username`, `password`, `name`) values('6','t1','$2a$10$TJ4TmCdK.X4wv/tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe','老师1');



INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('5','学生','student','学生');
INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('7','老师','teacher','老师');
INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('8','教学管理员','teachmanager','教学管理员');
INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('9','管理员','admin','管理员');
INSERT INTO `t_role` (`id`, `role_name`, `role_code`, `description`) VALUES('10','超级管理员','super','超级管理员');


INSERT INTO user_role(id,user_id,role_id) VALUES(NULL, '1', '5'),(NULL, '1', '7'),(NULL, '2', '8'),(NULL, '3', '9'),(NULL, '4', '8'),(NULL, '5', '10') ;


```



#### 3.3.2 explain 之 id

id 字段是 select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。id 情况有三种 ： 

1） id 相同表示加载表的顺序是从上到下。

```sql
explain select * from t_role r, t_user u, user_role ur where r.id = ur.role_id and u.id = ur.user_id ;
```

![1556102471304](D:/documents/notes/pictures/20230315/121.png)



2） id 不同id值越大，优先级越高，越先被执行。 

``` sql
EXPLAIN SELECT * FROM t_role WHERE id = (SELECT role_id FROM user_role WHERE user_id = (SELECT id FROM t_user WHERE username = 'stu1'))
```

![1556103009534](D:/documents/notes/pictures/20230315/122.png) 



3） id 有相同，也有不同，同时存在。id相同的可以认为是一组，从上往下顺序执行；在所有的组中，id的值越大，优先级越高，越先执行。

```sql 
EXPLAIN SELECT * FROM t_role r , (SELECT * FROM user_role ur WHERE ur.`user_id` = '2') a WHERE r.id = a.role_id ; 
```

![1556103294182](D:/documents/notes/pictures/20230315/123.png) 



#### 3.3.3 explain 之 select_type

 表示 SELECT 的类型，常见的取值，如下表所示：

| select_type  | 含义                                                         |
| ------------ | ------------------------------------------------------------ |
| SIMPLE       | 简单的select查询，查询中不包含子查询或者UNION                |
| PRIMARY      | 查询中若包含任何复杂的子查询，最外层查询标记为该标识         |
| SUBQUERY     | 在SELECT 或 WHERE 列表中包含了子查询                         |
| DERIVED      | 在FROM 列表中包含的子查询，被标记为 DERIVED（衍生） MYSQL会递归执行这些子查询，把结果放在临时表中 |
| UNION        | 若第二个SELECT出现在UNION之后，则标记为UNION ； 若UNION包含在FROM子句的子查询中，外层SELECT将被标记为 ： DERIVED |
| UNION RESULT | 从UNION表获取结果的SELECT                                    |



#### 3.3.4 explain 之 table

展示这一行的数据是关于哪一张表的 



#### 3.3.5 explain 之 type

type 显示的是访问类型，是较为重要的一个指标，可取值为： 

| type   | 含义                                                         |
| ------ | ------------------------------------------------------------ |
| NULL   | MySQL不访问任何表，索引，直接返回结果                        |
| system | 表只有一行记录(等于系统表)，这是const类型的特例，一般不会出现 |
| const  | 表示通过索引一次就找到了，const 用于比较primary key 或者 unique 索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL 就能将该查询转换为一个常量。const于将 "主键" 或 "唯一" 索引的所有部分与常量值进行比较 |
| eq_ref | 类似ref，区别在于使用的是唯一索引，使用主键的关联查询，关联查询出的记录只有一条。常见于主键或唯一索引扫描 |
| ref    | 非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，返回所有匹配某个单独值的所有行（多个） |
| range  | 只检索给定返回的行，使用一个索引来选择行。 where 之后出现 between ， < , > , in 等操作。 |
| index  | index 与 ALL的区别为  index 类型只是遍历了索引树， 通常比ALL 快， ALL 是遍历数据文件。 |
| all    | 将遍历全表以找到匹配的行                                     |

结果值从最好到最坏以此是：

```
NULL > system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL


system > const > eq_ref > ref > range > index > ALL
```

==一般来说， 我们需要保证查询至少达到 range 级别， 最好达到ref 。==



#### 3.3.6 explain 之  key

```
possible_keys : 显示可能应用在这张表的索引， 一个或多个。 

key ： 实际使用的索引， 如果为NULL， 则没有使用索引。

key_len : 表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下， 长度越短越好 。
```



#### 3.3.7 explain 之 rows

扫描行的数量。



#### 3.3.8 explain 之 extra

其他的额外的执行计划信息，在该列展示 。

| extra            | 含义                                                         |
| ---------------- | ------------------------------------------------------------ |
| using  filesort  | 说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取， 称为 “文件排序”, 效率低。 |
| using  temporary | 使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于 order by 和 group by； 效率低 |
| using  index     | 表示相应的select操作使用了覆盖索引， 避免访问表的数据行， 效率不错。 |



### 3.4 show profile分析SQL

Mysql从5.0.37版本开始增加了对 show profiles 和 show profile 语句的支持。show profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪里去了。

通过 have_profiling 参数，能够看到当前MySQL是否支持profile：

![1552488401999](D:/documents/notes/pictures/20230315/124.png) 

默认profiling是关闭的，可以通过set语句在Session级别开启profiling：

![1552488372405](D:/documents/notes/pictures/20230315/125.png) 

```sql
set profiling=1; //开启profiling 开关；
```

通过profile，我们能够更清楚地了解SQL执行的过程。

首先，我们可以执行一系列的操作，如下图所示：

```sql
show databases;

use db01;

show tables;

select * from tb_item where id < 5;

select count(*) from tb_item;
```

执行完上述命令之后，再执行show profiles 指令， 来查看SQL语句执行的耗时：

![1552489017940](D:/documents/notes/pictures/20230315/126.png)  

通过show  profile for  query  query_id 语句可以查看到该SQL执行过程中每个线程的状态和消耗的时间：

![1552489053763](D:/documents/notes/pictures/20230315/127.png) 

```tex
TIP ：
	Sending data 状态表示MySQL线程开始访问数据行并把结果返回给客户端，而不仅仅是返回给客户端。由于在Sending data状态下，MySQL线程往往需要做大量的磁盘读取操作，所以经常是整各查询中耗时最长的状态。
```



在获取到最消耗时间的线程状态后，MySQL支持进一步选择all、cpu、block io 、context switch、page faults等明细类型类查看MySQL在使用什么资源上耗费了过高的时间。例如，选择查看CPU的耗费时间  ：

![1552489671119](D:/documents/notes/pictures/20230315/128.png) 

| 字段       | 含义                           |
| ---------- | ------------------------------ |
| Status     | sql 语句执行的状态             |
| Duration   | sql 执行过程中每一个步骤的耗时 |
| CPU_user   | 当前用户占有的cpu              |
| CPU_system | 系统占有的cpu                  |



### 3.5 trace分析优化器执行计划

MySQL5.6提供了对SQL的跟踪trace, 通过trace文件能够进一步了解为什么优化器选择A计划, 而不是选择B计划。

打开trace ， 设置格式为 JSON，并设置trace最大能够使用的内存大小，避免解析过程中因为默认内存过小而不能够完整展示。

```sql
SET optimizer_trace="enabled=on",end_markers_in_json=on;
set optimizer_trace_max_mem_size=1000000;
```

执行SQL语句 ：

```sql
select * from tb_item where id < 4;
```

最后， 检查information_schema.optimizer_trace就可以知道MySQL是如何执行SQL的 ：

```sql
select * from information_schema.optimizer_trace\G;
```

```json
*************************** 1. row ***************************
QUERY: select * from tb_item where id < 4
TRACE: {
  "steps": [
    {
      "join_preparation": {
        "select#": 1,
        "steps": [
          {
            "expanded_query": "/* select#1 */ select `tb_item`.`id` AS `id`,`tb_item`.`title` AS `title`,`tb_item`.`price` AS `price`,`tb_item`.`num` AS `num`,`tb_item`.`categoryid` AS `categoryid`,`tb_item`.`status` AS `status`,`tb_item`.`sellerid` AS `sellerid`,`tb_item`.`createtime` AS `createtime`,`tb_item`.`updatetime` AS `updatetime` from `tb_item` where (`tb_item`.`id` < 4)"
          }
        ] /* steps */
      } /* join_preparation */
    },
    {
      "join_optimization": {
        "select#": 1,
        "steps": [
          {
            "condition_processing": {
              "condition": "WHERE",
              "original_condition": "(`tb_item`.`id` < 4)",
              "steps": [
                {
                  "transformation": "equality_propagation",
                  "resulting_condition": "(`tb_item`.`id` < 4)"
                },
                {
                  "transformation": "constant_propagation",
                  "resulting_condition": "(`tb_item`.`id` < 4)"
                },
                {
                  "transformation": "trivial_condition_removal",
                  "resulting_condition": "(`tb_item`.`id` < 4)"
                }
              ] /* steps */
            } /* condition_processing */
          },
          {
            "table_dependencies": [
              {
                "table": "`tb_item`",
                "row_may_be_null": false,
                "map_bit": 0,
                "depends_on_map_bits": [
                ] /* depends_on_map_bits */
              }
            ] /* table_dependencies */
          },
          {
            "ref_optimizer_key_uses": [
            ] /* ref_optimizer_key_uses */
          },
          {
            "rows_estimation": [
              {
                "table": "`tb_item`",
                "range_analysis": {
                  "table_scan": {
                    "rows": 9816098,
                    "cost": 2.04e6
                  } /* table_scan */,
                  "potential_range_indices": [
                    {
                      "index": "PRIMARY",
                      "usable": true,
                      "key_parts": [
                        "id"
                      ] /* key_parts */
                    }
                  ] /* potential_range_indices */,
                  "setup_range_conditions": [
                  ] /* setup_range_conditions */,
                  "group_index_range": {
                    "chosen": false,
                    "cause": "not_group_by_or_distinct"
                  } /* group_index_range */,
                  "analyzing_range_alternatives": {
                    "range_scan_alternatives": [
                      {
                        "index": "PRIMARY",
                        "ranges": [
                          "id < 4"
                        ] /* ranges */,
                        "index_dives_for_eq_ranges": true,
                        "rowid_ordered": true,
                        "using_mrr": false,
                        "index_only": false,
                        "rows": 3,
                        "cost": 1.6154,
                        "chosen": true
                      }
                    ] /* range_scan_alternatives */,
                    "analyzing_roworder_intersect": {
                      "usable": false,
                      "cause": "too_few_roworder_scans"
                    } /* analyzing_roworder_intersect */
                  } /* analyzing_range_alternatives */,
                  "chosen_range_access_summary": {
                    "range_access_plan": {
                      "type": "range_scan",
                      "index": "PRIMARY",
                      "rows": 3,
                      "ranges": [
                        "id < 4"
                      ] /* ranges */
                    } /* range_access_plan */,
                    "rows_for_plan": 3,
                    "cost_for_plan": 1.6154,
                    "chosen": true
                  } /* chosen_range_access_summary */
                } /* range_analysis */
              }
            ] /* rows_estimation */
          },
          {
            "considered_execution_plans": [
              {
                "plan_prefix": [
                ] /* plan_prefix */,
                "table": "`tb_item`",
                "best_access_path": {
                  "considered_access_paths": [
                    {
                      "access_type": "range",
                      "rows": 3,
                      "cost": 2.2154,
                      "chosen": true
                    }
                  ] /* considered_access_paths */
                } /* best_access_path */,
                "cost_for_plan": 2.2154,
                "rows_for_plan": 3,
                "chosen": true
              }
            ] /* considered_execution_plans */
          },
          {
            "attaching_conditions_to_tables": {
              "original_condition": "(`tb_item`.`id` < 4)",
              "attached_conditions_computation": [
              ] /* attached_conditions_computation */,
              "attached_conditions_summary": [
                {
                  "table": "`tb_item`",
                  "attached": "(`tb_item`.`id` < 4)"
                }
              ] /* attached_conditions_summary */
            } /* attaching_conditions_to_tables */
          },
          {
            "refine_plan": [
              {
                "table": "`tb_item`",
                "access_type": "range"
              }
            ] /* refine_plan */
          }
        ] /* steps */
      } /* join_optimization */
    },
    {
      "join_execution": {
        "select#": 1,
        "steps": [
        ] /* steps */
      } /* join_execution */
    }
  ] /* steps */
}
```



## 4. 索引的使用

索引是数据库优化最常用也是最重要的手段之一, 通过索引通常可以帮助用户解决大多数的MySQL的性能优化问题。



### 4.1 验证索引提升查询效率

在我们准备的表结构tb_item 中， 一共存储了 300 万记录；

A. 根据ID查询 

```
select * from tb_item where id = 1999\G;
```

![1553261992653](D:/documents/notes/pictures/20230315/129.png) 

查询速度很快， 接近0s ， 主要的原因是因为id为主键， 有索引；

![1553262044466](D:/documents/notes/pictures/20230315/130.png) 



2). 根据 title 进行精确查询

```SQL
select * from tb_item where title = 'iphoneX 移动3G 32G941'\G; 
```

![1553262215900](D:/documents/notes/pictures/20230315/131.png) 

查看SQL语句的执行计划 ： 

![1553262469785](D:/documents/notes/pictures/20230315/132.png) 



处理方案 ， 针对title字段， 创建索引 ： 

```SQL
create index idx_item_title on tb_item(title);
```

![1553263229523](D:/documents/notes/pictures/20230315/133.png) 



索引创建完成之后，再次进行查询 ： 

![1553263302706](D:/documents/notes/pictures/20230315/134.png) 

通过explain ， 查看执行计划，执行SQL时使用了刚才创建的索引 

![1553263355262](D:/documents/notes/pictures/20230315/135.png) 



### 4.2 索引的使用

#### 4.2.1 准备环境

```sql
create table `tb_seller` (
	`sellerid` varchar (100),
	`name` varchar (100),
	`nickname` varchar (50),
	`password` varchar (60),
	`status` varchar (1),
	`address` varchar (100),
	`createtime` datetime,
    primary key(`sellerid`)
)engine=innodb default charset=utf8mb4; 

insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('alibaba','阿里巴巴','阿里小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('baidu','百度科技有限公司','百度小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('huawei','华为科技有限公司','华为小店','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('itcast','传智播客教育科技有限公司','传智播客','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('itheima','黑马程序员','黑马程序员','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('luoji','罗技科技有限公司','罗技小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('oppo','OPPO科技有限公司','OPPO官方旗舰店','e10adc3949ba59abbe56e057f20f883e','0','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('ourpalm','掌趣科技股份有限公司','掌趣小店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('qiandu','千度科技','千度小店','e10adc3949ba59abbe56e057f20f883e','2','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('sina','新浪科技有限公司','新浪官方旗舰店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('xiaomi','小米科技','小米官方旗舰店','e10adc3949ba59abbe56e057f20f883e','1','西安市','2088-01-01 12:00:00');
insert into `tb_seller` (`sellerid`, `name`, `nickname`, `password`, `status`, `address`, `createtime`) values('yijia','宜家家居','宜家家居旗舰店','e10adc3949ba59abbe56e057f20f883e','1','北京市','2088-01-01 12:00:00');


create index idx_seller_name_sta_addr on tb_seller(name,status,address);
```



#### 4.2.2 避免索引失效

1).  全值匹配 ，对索引中所有列都指定具体值。

改情况下，索引生效，执行效率高。

```sql
explain select * from tb_seller where name='小米科技' and status='1' and address='北京市'\G;
```

![1556170997921](D:/documents/notes/pictures/20230315/136.png) 



2). 最左前缀法则

如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始，并且不跳过索引中的列。



匹配最左前缀法则，走索引：

![1556171348995](D:/documents/notes/pictures/20230315/137.png)  



违法最左前缀法则 ， 索引失效：

![1556171428140](D:/documents/notes/pictures/20230315/138.png) 



如果符合最左法则，但是出现跳跃某一列，只有最左列索引生效：

![1556171662203](D:/documents/notes/pictures/20230315/139.png) 



3). 范围查询右边的列，不能使用索引 。

![1556172256791](D:/documents/notes/pictures/20230315/140.png) 

根据前面的两个字段name ， status 查询是走索引的， 但是最后一个条件address 没有用到索引。



4). 不要在索引列上进行运算操作， 索引将失效。

![1556172813715](D:/documents/notes/pictures/20230315/141.png) 



5). 字符串不加单引号，造成索引失效。

![1556172967493](D:/documents/notes/pictures/20230315/142.png) 

由于，在查询时，没有对字符串加单引号，MySQL的查询优化器，会自动的进行类型转换，造成索引失效。



6). 尽量使用覆盖索引，避免select *

尽量使用覆盖索引（只访问索引的查询（索引列完全包含查询列）），减少select * 。

![1556173928299](D:/documents/notes/pictures/20230315/143.png) 

如果查询列，超出索引列，也会降低性能。

![1556173986068](D:/documents/notes/pictures/20230315/144.png) 

```
TIP : 
	
    using index ：使用覆盖索引的时候就会出现

    using where：在查找使用索引的情况下，需要回表去查询所需的数据

    using index condition：查找使用了索引，但是需要回表查询数据

    using index ; using where：查找使用了索引，但是需要的数据都在索引列中能找到，所以不需要回表查询数据
```



7). 用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。

示例，name字段是索引列 ， 而createtime不是索引列，中间是or进行连接是不走索引的 ： 

```sql
explain select * from tb_seller where name='黑马程序员' or createtime = '2088-01-01 12:00:00'\G;	
```

![1556174994440](D:/documents/notes/pictures/20230315/145.png) 



8).  以%开头的Like模糊查询，索引失效。

如果仅仅是尾部模糊匹配，索引不会失效。如果是头部模糊匹配，索引失效。

![1556175114369](D:/documents/notes/pictures/20230315/146.png) 

解决方案 ： 

通过覆盖索引来解决 

![1556247686483](D:/documents/notes/pictures/20230315/147.png) 



9). 如果MySQL评估使用索引比全表更慢，则不使用索引。

![1556175445210](D:/documents/notes/pictures/20230315/148.png) 



10). is  NULL ， is NOT NULL  <font color='red'>有时</font>索引失效。

![1556180634889](D:/documents/notes/pictures/20230315/149.png)  



11). in 走索引， not in 索引失效。

![1556249602732](D:/documents/notes/pictures/20230315/150.png)  



12). 单列索引和复合索引。

尽量使用复合索引，而少使用单列索引 。

创建复合索引 

```
create index idx_name_sta_address on tb_seller(name, status, address);

就相当于创建了三个索引 ： 
	name
	name + status
	name + status + address

```



创建单列索引 

```
create index idx_seller_name on tb_seller(name);
create index idx_seller_status on tb_seller(status);
create index idx_seller_address on tb_seller(address);
```

数据库会选择一个最优的索引（辨识度最高索引）来使用，并不会使用全部索引 。



### 4.3 查看索引使用情况

```sql
show status like 'Handler_read%';	

show global status like 'Handler_read%';	
```

![1552885364563](D:/documents/notes/pictures/20230315/151.png) 

```
Handler_read_first：索引中第一条被读的次数。如果较高，表示服务器正执行大量全索引扫描（这个值越低越好）。

Handler_read_key：如果索引正在工作，这个值代表一个行被索引值读的次数，如果值越低，表示索引得到的性能改善不高，因为索引不经常使用（这个值越高越好）。

Handler_read_next ：按照键顺序读下一行的请求数。如果你用范围约束或如果执行索引扫描来查询索引列，该值增加。

Handler_read_prev：按照键顺序读前一行的请求数。该读方法主要用于优化ORDER BY ... DESC。

Handler_read_rnd ：根据固定位置读一行的请求数。如果你正执行大量查询并需要对结果进行排序该值较高。你可能使用了大量需要MySQL扫描整个表的查询或你的连接没有正确使用键。这个值较高，意味着运行效率低，应该建立索引来补救。

Handler_read_rnd_next：在数据文件中读下一行的请求数。如果你正进行大量的表扫描，该值较高。通常说明你的表索引不正确或写入的查询没有利用索引。
```



## 5. SQL优化

### 5.1 大批量插入数据

环境准备 ： 

```sql
CREATE TABLE `tb_user_2` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `username` varchar(45) NOT NULL,
  `password` varchar(96) NOT NULL,
  `name` varchar(45) NOT NULL,
  `birthday` datetime DEFAULT NULL,
  `sex` char(1) DEFAULT NULL,
  `email` varchar(45) DEFAULT NULL,
  `phone` varchar(45) DEFAULT NULL,
  `qq` varchar(32) DEFAULT NULL,
  `status` varchar(32) NOT NULL COMMENT '用户状态',
  `create_time` datetime NOT NULL,
  `update_time` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_user_username` (`username`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ;
```



当使用load 命令导入数据的时候，适当的设置可以提高导入的效率。

![1556269346488](D:/documents/notes/pictures/20230315/152.png) 

对于 InnoDB 类型的表，有以下几种方式可以提高导入的效率：

1） 主键顺序插入

因为InnoDB类型的表是按照主键的顺序保存的，所以将导入的数据按照主键的顺序排列，可以有效的提高导入数据的效率。如果InnoDB表没有主键，那么系统会自动默认创建一个内部列作为主键，所以如果可以给表创建一个主键，将可以利用这点，来提高导入数据的效率。

```
脚本文件介绍 :
	sql1.log  ----> 主键有序
	sql2.log  ----> 主键无序
```

插入ID顺序排列数据：

![1555771750567](D:/documents/notes/pictures/20230315/153.png)

插入ID无序排列数据：

![1555771959734](D:/documents/notes/pictures/20230315/154.png) 



2） 关闭唯一性校验

在导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验，在导入结束后执行SET UNIQUE_CHECKS=1，恢复唯一性校验，可以提高导入的效率。

![1555772132736](D:/documents/notes/pictures/20230315/155.png) 



3） 手动提交事务

如果应用使用自动提交的方式，建议在导入前执行 SET AUTOCOMMIT=0，关闭自动提交，导入结束后再执行 SET AUTOCOMMIT=1，打开自动提交，也可以提高导入的效率。

![1555772351208](D:/documents/notes/pictures/20230315/156.png)



### 5.2 优化insert语句

当进行数据的insert操作的时候，可以考虑采用以下几种优化方案。

- 如果需要同时对一张表插入很多行数据时，应该尽量使用多个值表的insert语句，这种方式将大大的缩减客户端与数据库之间的连接、关闭等消耗。使得效率比分开执行的单个insert语句快。

  示例， 原始方式为：

  ```sql
  insert into tb_test values(1,'Tom');
  insert into tb_test values(2,'Cat');
  insert into tb_test values(3,'Jerry');
  ```

  优化后的方案为 ： 

  ```sql
  insert into tb_test values(1,'Tom'),(2,'Cat')，(3,'Jerry');
  ```

- 在事务中进行数据插入。

  ```sql
  start transaction;
  insert into tb_test values(1,'Tom');
  insert into tb_test values(2,'Cat');
  insert into tb_test values(3,'Jerry');
  commit;
  ```

- 数据有序插入

  ```sql
  insert into tb_test values(4,'Tim');
  insert into tb_test values(1,'Tom');
  insert into tb_test values(3,'Jerry');
  insert into tb_test values(5,'Rose');
  insert into tb_test values(2,'Cat');
  ```

  优化后

  ```sql
  insert into tb_test values(1,'Tom');
  insert into tb_test values(2,'Cat');
  insert into tb_test values(3,'Jerry');
  insert into tb_test values(4,'Tim');
  insert into tb_test values(5,'Rose');
  ```



### 5.3 优化order by语句

#### 5.3.1 环境准备

```SQL
CREATE TABLE `emp` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(100) NOT NULL,
  `age` int(3) NOT NULL,
  `salary` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4;

insert into `emp` (`id`, `name`, `age`, `salary`) values('1','Tom','25','2300');
insert into `emp` (`id`, `name`, `age`, `salary`) values('2','Jerry','30','3500');
insert into `emp` (`id`, `name`, `age`, `salary`) values('3','Luci','25','2800');
insert into `emp` (`id`, `name`, `age`, `salary`) values('4','Jay','36','3500');
insert into `emp` (`id`, `name`, `age`, `salary`) values('5','Tom2','21','2200');
insert into `emp` (`id`, `name`, `age`, `salary`) values('6','Jerry2','31','3300');
insert into `emp` (`id`, `name`, `age`, `salary`) values('7','Luci2','26','2700');
insert into `emp` (`id`, `name`, `age`, `salary`) values('8','Jay2','33','3500');
insert into `emp` (`id`, `name`, `age`, `salary`) values('9','Tom3','23','2400');
insert into `emp` (`id`, `name`, `age`, `salary`) values('10','Jerry3','32','3100');
insert into `emp` (`id`, `name`, `age`, `salary`) values('11','Luci3','26','2900');
insert into `emp` (`id`, `name`, `age`, `salary`) values('12','Jay3','37','4500');

create index idx_emp_age_salary on emp(age,salary);
```

#### 5.3.2 两种排序方式

1). 第一种是通过对返回数据进行排序，也就是通常说的 filesort 排序，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序。

![1556335817763](D:/documents/notes/pictures/20230315/157.png) 

2). 第二种通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。

![1556335866539](D:/documents/notes/pictures/20230315/158.png) 

多字段排序

![1556336352061](D:/documents/notes/pictures/20230315/159.png) 



了解了MySQL的排序方式，优化目标就清晰了：尽量减少额外的排序，通过索引直接返回有序数据。where 条件和Order by 使用相同的索引，并且Order By 的顺序和索引顺序相同， 并且Order  by 的字段都是升序，或者都是降序。否则肯定需要额外的操作，这样就会出现FileSort。



#### 5.3.3 Filesort 的优化

通过创建合适的索引，能够减少 Filesort 的出现，但是在某些情况下，条件限制不能让Filesort消失，那就需要加快 Filesort的排序操作。对于Filesort ， MySQL 有两种排序算法：

1） 两次扫描算法 ：MySQL4.1 之前，使用该方式排序。首先根据条件取出排序字段和行指针信息，然后在排序区 sort buffer 中排序，如果sort buffer不够，则在临时表 temporary table 中存储排序结果。完成排序之后，再根据行指针回表读取记录，该操作可能会导致大量随机I/O操作。

2）一次扫描算法：一次性取出满足条件的所有字段，然后在排序区 sort  buffer 中排序后直接输出结果集。排序时内存开销较大，但是排序效率比两次扫描算法要高。



MySQL 通过比较系统变量 max_length_for_sort_data 的大小和Query语句取出的字段总大小， 来判定是否那种排序算法，如果max_length_for_sort_data 更大，那么使用第二种优化之后的算法；否则使用第一种。

可以适当提高 sort_buffer_size  和 max_length_for_sort_data  系统变量，来增大排序区的大小，提高排序的效率。

![1556338367593](D:/documents/notes/pictures/20230315/160.png) 



### 5.4 优化group by 语句

由于GROUP BY 实际上也同样会进行排序操作，而且与ORDER BY 相比，GROUP BY 主要只是多了排序之后的分组操作。当然，如果在分组的时候还使用了其他的一些聚合函数，那么还需要一些聚合函数的计算。所以，在GROUP BY 的实现过程中，与 ORDER BY 一样也可以利用到索引。

如果查询包含 group by 但是用户想要避免排序结果的消耗， 则可以执行order by null 禁止排序。如下 ：

```SQL
drop index idx_emp_age_salary on emp;

explain select age,count(*) from emp group by age;
```

![1556339573979](D:/documents/notes/pictures/20230315/161.png)  

优化后

```sql
explain select age,count(*) from emp group by age order by null;
```

![1556339633161](D:/documents/notes/pictures/20230315/162.png)  

从上面的例子可以看出，第一个SQL语句需要进行"filesort"，而第二个SQL由于order  by  null 不需要进行 "filesort"， 而上文提过Filesort往往非常耗费时间。



创建索引 ：

```SQL
create index idx_emp_age_salary on emp(age,salary);
```

![1556339688158](D:/documents/notes/pictures/20230315/163.png) 



### 5.5 优化嵌套查询

Mysql4.1版本之后，开始支持SQL的子查询。这个技术可以使用SELECT语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的SQL操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询是可以被更高效的连接（JOIN）替代。

示例 ，查找有角色的所有的用户信息 : 

```SQL
 explain select * from t_user where id in (select user_id from user_role );
```

执行计划为 : 

![1556359399199](D:/documents/notes/pictures/20230315/164.png)   



优化后 :

```SQL
explain select * from t_user u , user_role ur where u.id = ur.user_id;
```

![1556359482142](D:/documents/notes/pictures/20230315/165.png)   



连接(Join)查询之所以更有效率一些 ，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上需要两个步骤的查询工作。



### 5.6 优化OR条件

对于包含OR的查询子句，如果要利用索引，则OR之间的每个条件列都必须用到索引 ， 而且不能使用到复合索引； 如果没有索引，则应该考虑增加索引。

获取 emp 表中的所有的索引 ： 

![1556354464657](D:/documents/notes/pictures/20230315/166.png)  

示例 ： 

```SQL
explain select * from emp where id = 1 or age = 30;
```

![1556354887509](D:/documents/notes/pictures/20230315/167.png)

![1556354920964](D:/documents/notes/pictures/20230315/168.png)  

建议使用 union 替换 or ： 

![1556355027728](D:/documents/notes/pictures/20230315/169.png) 

我们来比较下重要指标，发现主要差别是 type 和 ref 这两项

type 显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是：

```
system > const > eq_ref > ref > fulltext > ref_or_null  > index_merge > unique_subquery > index_subquery > range > index > ALL
```

UNION 语句的 type 值为 ref，OR 语句的 type 值为 range，可以看到这是一个很明显的差距

UNION 语句的 ref 值为 const，OR 语句的 type 值为 null，const 表示是常量值引用，非常快

这两项的差距就说明了 UNION 要优于 OR 。



### 5.7 优化分页查询

一般分页查询时，通过创建覆盖索引能够比较好地提高性能。一个常见又非常头疼的问题就是 limit 2000000,10  ，此时需要MySQL排序前2000010 记录，仅仅返回2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。

![1556361314783](D:/documents/notes/pictures/20230315/170.png) 

#### 5.7.1 优化思路一

在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。

![1556416102800](D:/documents/notes/pictures/20230315/171.png) 



#### 5.7.2 优化思路二

该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。

![1556363928151](D:/documents/notes/pictures/20230315/172.png) 



### 5.8 使用SQL提示

SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。

#### 5.8.1 USE INDEX

在查询语句中表名的后面，添加 use index 来提供希望MySQL去参考的索引列表，就可以让MySQL不再考虑其他可用的索引。

```
create index idx_seller_name on tb_seller(name);
```

![1556370971576](D:/documents/notes/pictures/20230315/173.png) 

#### 5.8.2 IGNORE INDEX

如果用户只是单纯的想让MySQL忽略一个或者多个索引，则可以使用 ignore index 作为 hint 。

```
 explain select * from tb_seller ignore index(idx_seller_name) where name = '小米科技';
```

![1556371004594](D:/documents/notes/pictures/20230315/174.png) 

#### 5.8.3 FORCE INDEX

为强制MySQL使用一个特定的索引，可在查询中使用 force index 作为hint 。 

``` SQL
create index idx_seller_address on tb_seller(address);
```

![1556371355788](D:/documents/notes/pictures/20230315/175.png) 





## 1. 应用优化

前面章节，我们介绍了很多数据库的优化措施。但是在实际生产环境中，由于数据库本身的性能局限，就必须要对前台的应用进行一些优化，来降低数据库的访问压力。

### 1.1 使用连接池

对于访问数据库来说，建立连接的代价是比较昂贵的，因为我们频繁的创建关闭连接，是比较耗费资源的，我们有必要建立数据库连接池，以提高访问的性能。



### 1.2 减少对MySQL的访问

#### 1.2.1 避免对数据进行重复检索

在编写应用代码时，需要能够理清对数据库的访问逻辑。能够一次连接就获取到结果的，就不用两次连接，这样可以大大减少对数据库无用的重复请求。

比如 ，需要获取书籍的id 和name字段 ， 则查询如下： 

```
 select id , name from tb_book;
```

之后，在业务逻辑中有需要获取到书籍状态信息， 则查询如下：

```
select id , status from tb_book;
```

这样，就需要向数据库提交两次请求，数据库就要做两次查询操作。其实完全可以用一条SQL语句得到想要的结果。

```
select id, name , status from tb_book;
```



#### 1.2.2 增加cache层

在应用中，我们可以在应用中增加 缓存 层来达到减轻数据库负担的目的。缓存层有很多种，也有很多实现方式，只要能达到降低数据库的负担又能满足应用需求就可以。

因此可以部分数据从数据库中抽取出来放到应用端以文本方式存储， 或者使用框架(Mybatis, Hibernate)提供的一级缓存/二级缓存，或者使用redis数据库来缓存数据 。



### 1.3 负载均衡 

负载均衡是应用中使用非常普遍的一种优化方法，它的机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上， 以此来降低单台服务器的负载，达到优化的效果。

#### 1.3.1 利用MySQL复制分流查询

通过MySQL的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力。

![1](D:/documents/notes/pictures/20230315/176.jpg) 

#### 1.3.2 采用分布式数据库架构

分布式数据库架构适合大数据量、负载高的情况，它有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率。





## 2. Mysql中查询缓存优化

### 2.1 概述

开启Mysql的查询缓存，当执行完全相同的SQL语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存。

### 2.2 操作流程

 ![20180919131632347](D:/documents/notes/pictures/20230315/177.png) 

1. 客户端发送一条查询给服务器；
2. 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段；
3. 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划；
4. MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询；
5. 将结果返回给客户端。

### 2.3 查询缓存配置

1. 查看当前的MySQL数据库是否支持查询缓存：

   ```SQL
   SHOW VARIABLES LIKE 'have_query_cache';	
   ```

   ![1555249929012](D:/documents/notes/pictures/20230315/178.png) 

2. 查看当前MySQL是否开启了查询缓存 ：

   ```SQL
   SHOW VARIABLES LIKE 'query_cache_type';
   ```

   ![1555250015377](D:/documents/notes/pictures/20230315/179.png) 

3. 查看查询缓存的占用大小 ：

   ```SQL
   SHOW VARIABLES LIKE 'query_cache_size';
   ```

   ![1555250142451](D:/documents/notes/pictures/20230315/180.png)  	

4. 查看查询缓存的状态变量：

   ```SQL
   SHOW STATUS LIKE 'Qcache%';
   ```

   ![1555250443958](D:/documents/notes/pictures/20230315/181.png) 

   各个变量的含义如下：

   | 参数                    | 含义                                                         |
   | ----------------------- | ------------------------------------------------------------ |
   | Qcache_free_blocks      | 查询缓存中的可用内存块数                                     |
   | Qcache_free_memory      | 查询缓存的可用内存量                                         |
   | Qcache_hits             | 查询缓存命中数                                               |
   | Qcache_inserts          | 添加到查询缓存的查询数                                       |
   | Qcache_lowmen_prunes    | 由于内存不足而从查询缓存中删除的查询数                       |
   | Qcache_not_cached       | 非缓存查询的数量（由于 query_cache_type 设置而无法缓存或未缓存） |
   | Qcache_queries_in_cache | 查询缓存中注册的查询数                                       |
   | Qcache_total_blocks     | 查询缓存中的块总数                                           |

### 2.4 开启查询缓存

MySQL的查询缓存默认是关闭的，需要手动配置参数 query_cache_type ， 来开启查询缓存。query_cache_type 该参数的可取值有三个 ：

| 值          | 含义                                                         |
| ----------- | ------------------------------------------------------------ |
| OFF 或 0    | 查询缓存功能关闭                                             |
| ON 或 1     | 查询缓存功能打开，SELECT的结果符合缓存条件即会缓存，否则，不予缓存，显式指定 SQL_NO_CACHE，不予缓存 |
| DEMAND 或 2 | 查询缓存功能按需进行，显式指定 SQL_CACHE 的SELECT语句才会缓存；其它均不予缓存 |

在 /usr/my.cnf 配置中，增加以下配置 ： 

![1555251383805](D:/documents/notes/pictures/20230315/182.png) 		

配置完毕之后，重启服务既可生效 ；

然后就可以在命令行执行SQL语句进行验证 ，执行一条比较耗时的SQL语句，然后再多执行几次，查看后面几次的执行时间；获取通过查看查询缓存的缓存命中数，来判定是否走查询缓存。



### 2.5 查询缓存SELECT选项

可以在SELECT语句中指定两个与查询缓存相关的选项 ：

SQL_CACHE : 如果查询结果是可缓存的，并且 query_cache_type 系统变量的值为ON或 DEMAND ，则缓存查询结果 。

SQL_NO_CACHE : 服务器不使用查询缓存。它既不检查查询缓存，也不检查结果是否已缓存，也不缓存查询结果。

例子：

```SQL
SELECT SQL_CACHE id, name FROM customer;
SELECT SQL_NO_CACHE id, name FROM customer;
```

​	

### 2.6 查询缓存失效的情况

1） SQL 语句不一致的情况， 要想命中查询缓存，查询的SQL语句必须一致。

```SQL
SQL1 : select count(*) from tb_item;
SQL2 : Select count(*) from tb_item;
```

2） 当查询语句中有一些不确定的时，则不会缓存。如 ： now() , current_date() , curdate() , curtime() , rand() , uuid() , user() , database() 。

```SQL
SQL1 : select * from tb_item where updatetime < now() limit 1;
SQL2 : select user();
SQL3 : select database();
```

3） 不使用任何表查询语句。

```SQL
select 'A';
```

4）  查询 mysql， information_schema或  performance_schema 数据库中的表时，不会走查询缓存。

```SQL
select * from information_schema.engines;
```

5） 在存储的函数，触发器或事件的主体内执行的查询。

6） 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除。这包括使用`MERGE`映射到已更改表的表的查询。一个表可以被许多类型的语句，如被改变 INSERT， UPDATE， DELETE， TRUNCATE TABLE， ALTER TABLE， DROP TABLE，或 DROP DATABASE 。



## 3. Mysql内存管理及优化

### 3.1 内存优化原则

1） 将尽量多的内存分配给MySQL做缓存，但要给操作系统和其他程序预留足够内存。

2） MyISAM 存储引擎的数据文件读取依赖于操作系统自身的IO缓存，因此，如果有MyISAM表，就要预留更多的内存给操作系统做IO缓存。

3） 排序区、连接区等缓存是分配给每个数据库会话（session）专用的，其默认值的设置要根据最大连接数合理分配，如果设置太大，不但浪费资源，而且在并发连接较高时会导致物理内存耗尽。



### 3.2 MyISAM 内存优化

myisam存储引擎使用 key_buffer 缓存索引块，加速myisam索引的读写速度。对于myisam表的数据块，mysql没有特别的缓存机制，完全依赖于操作系统的IO缓存。



#### key_buffer_size

key_buffer_size决定MyISAM索引块缓存区的大小，直接影响到MyISAM表的存取效率。可以在MySQL参数文件中设置key_buffer_size的值，对于一般MyISAM数据库，建议至少将1/4可用内存分配给key_buffer_size。

在/usr/my.cnf 中做如下配置：

```
key_buffer_size=512M
```



#### read_buffer_size

如果需要经常顺序扫描myisam表，可以通过增大read_buffer_size的值来改善性能。但需要注意的是read_buffer_size是每个session独占的，如果默认值设置太大，就会造成内存浪费。



#### read_rnd_buffer_size

对于需要做排序的myisam表的查询，如带有order by子句的sql，适当增加 read_rnd_buffer_size 的值，可以改善此类的sql性能。但需要注意的是 read_rnd_buffer_size 是每个session独占的，如果默认值设置太大，就会造成内存浪费。



### 3.3 InnoDB 内存优化

innodb用一块内存区做IO缓存池，该缓存池不仅用来缓存innodb的索引块，而且也用来缓存innodb的数据块。



#### innodb_buffer_pool_size

该变量决定了 innodb 存储引擎表数据和索引数据的最大缓存区大小。在保证操作系统及其他程序有足够内存可用的情况下，innodb_buffer_pool_size 的值越大，缓存命中率越高，访问InnoDB表需要的磁盘I/O 就越少，性能也就越高。

```
innodb_buffer_pool_size=512M
```



#### innodb_log_buffer_size

决定了innodb重做日志缓存的大小，对于可能产生大量更新记录的大事务，增加innodb_log_buffer_size的大小，可以避免innodb在事务提交前就执行不必要的日志写入磁盘操作。

```
innodb_log_buffer_size=10M
```



## 4. Mysql并发参数调整

从实现上来说，MySQL Server 是多线程结构，包括后台线程和客户服务线程。多线程可以有效利用服务器资源，提高数据库的并发性能。在Mysql中，控制并发连接和线程的主要参数包括 max_connections、back_log、thread_cache_size、table_open_cahce。

### 4.1 max_connections

采用max_connections 控制允许连接到MySQL数据库的最大数量，默认值是 151。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这是可以考虑增大max_connections 的值。

Mysql 最大可支持的连接数，取决于很多因素，包括给定操作系统平台的线程库的质量、内存大小、每个连接的负荷、CPU的处理速度，期望的响应时间等。在Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。



### 4.2 back_log

back_log 参数控制MySQL监听TCP端口时设置的积压请求栈大小。如果MySql的连接数达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默认值为 50 ， 之后的版本默认为 50 + （max_connections / 5）， 但最大不超过900。

如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。



### 4.3 table_open_cache

该参数用来控制所有SQL语句执行线程可打开表缓存的数量， 而在执行SQL语句时，每一个SQL执行线程至少要打开 1 个表缓存。该参数的值应该根据设置的最大连接数 max_connections 以及每个连接执行关联查询中涉及的表的最大数量来设定 ：

​	max_connections x N ；



### 4.4 thread_cache_size

为了加快连接数据库的速度，MySQL 会缓存一定数量的客户服务线程以备重用，通过参数 thread_cache_size 可控制 MySQL 缓存客户服务线程的数量。



### 4.5 innodb_lock_wait_timeout

该参数是用来设置InnoDB 事务等待行锁的时间，默认值是50ms ， 可以根据需要进行动态设置。对于需要快速反馈的业务系统来说，可以将行锁的等待时间调小，以避免事务长时间挂起； 对于后台运行的批量处理程序来说， 可以将行锁的等待时间调大， 以避免发生大的回滚操作。



## 5. Mysql锁问题

### 5.1 锁概述

锁是计算机协调多个进程或线程并发访问某一资源的机制（避免争抢）。

在数据库中，除传统的计算资源（如 CPU、RAM、I/O 等）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。



### 5.2 锁分类

从对数据操作的粒度分 ： 

1） 表锁：操作时，会锁定整个表。

2） 行锁：操作时，会锁定当前操作行。

从对数据操作的类型分：

1） 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响。

2） 写锁（排它锁）：当前操作没有完成之前，它会阻断其他写锁和读锁。



### 5.3 Mysql 锁

相对其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。下表中罗列出了各存储引擎对锁的支持情况：

| 存储引擎 | 表级锁 | 行级锁 | 页面锁 |
| -------- | ------ | ------ | ------ |
| MyISAM   | 支持   | 不支持 | 不支持 |
| InnoDB   | 支持   | 支持   | 不支持 |
| MEMORY   | 支持   | 不支持 | 不支持 |
| BDB      | 支持   | 不支持 | 支持   |

MySQL这3种锁的特性可大致归纳如下 ：

| 锁类型 | 特点                                                         |
| ------ | ------------------------------------------------------------ |
| 表级锁 | 偏向MyISAM 存储引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 |
| 行级锁 | 偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 |
| 页面锁 | 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 |

从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理（OLTP）系统。




### 5.2 MyISAM 表锁

MyISAM 存储引擎只支持表锁，这也是MySQL开始几个版本中唯一支持的锁类型。



#### 5.2.1 如何加表锁

MyISAM 在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。

显示加表锁语法：

```SQL
加读锁 ： lock table table_name read;

加写锁 ： lock table table_name write；
```



#### 5.2.2 读锁案例 

准备环境

```SQL
create database demo_03 default charset=utf8mb4;

use demo_03;

CREATE TABLE `tb_book` (
  `id` INT(11) auto_increment,
  `name` VARCHAR(50) DEFAULT NULL,
  `publish_time` DATE DEFAULT NULL,
  `status` CHAR(1) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=myisam DEFAULT CHARSET=utf8 ;

INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,'java编程思想','2088-08-01','1');
INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,'solr编程思想','2088-08-08','0');



CREATE TABLE `tb_user` (
  `id` INT(11) auto_increment,
  `name` VARCHAR(50) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=myisam DEFAULT CHARSET=utf8 ;

INSERT INTO tb_user (id, name) VALUES(NULL,'令狐冲');
INSERT INTO tb_user (id, name) VALUES(NULL,'田伯光');

```



客户端 一 ：

1）获得tb_book 表的读锁 

```
lock table tb_book read;
```



2） 执行查询操作

```
select * from tb_book;
```

![1553906896564](D:/documents/notes/pictures/20230315/183.png) 

可以正常执行 ， 查询出数据。



客户端 二 ：

3） 执行查询操作

```
select * from tb_book;
```

![1553907044500](D:/documents/notes/pictures/20230315/184.png) 



客户端 一 ：

4）查询未锁定的表

```
select name from tb_seller;
```

![1553908913515](D:/documents/notes/pictures/20230315/185.png) 



客户端 二 ：

5）查询未锁定的表

```
select name from tb_seller;
```

![1553908973840](D:/documents/notes/pictures/20230315/186.png) 

可以正常查询出未锁定的表；



客户端 一 ：

6） 执行插入操作 

```
insert into tb_book values(null,'Mysql高级','2088-01-01','1');
```

![1553907198462](D:/documents/notes/pictures/20230315/187.png) 

执行插入， 直接报错 ， 由于当前tb_book 获得的是 读锁， 不能执行更新操作。



客户端 二 ：

7） 执行插入操作 

```
insert into tb_book values(null,'Mysql高级','2088-01-01','1');
```

![1553907403957](D:/documents/notes/pictures/20230315/188.png) 



当在客户端一中释放锁指令 unlock tables  后 ， 客户端二中的 inesrt 语句 ， 立即执行 ；



#### 5.2.3 写锁案例

客户端 一 :

1）获得tb_book 表的写锁 

```
lock table tb_book write ;
```

2）执行查询操作

```
select * from tb_book ;
```

![1553907849829](D:/documents/notes/pictures/20230315/189.png) 

查询操作执行成功；

3）执行更新操作

```
update tb_book set name = 'java编程思想（第二版）' where id = 1;
```

![1553907875221](D:/documents/notes/pictures/20230315/190.png) 

更新操作执行成功 ；



客户端 二 :

4）执行查询操作

```
select * from tb_book ;
```

![1553908019755](D:/documents/notes/pictures/20230315/191.png) 



当在客户端一中释放锁指令 unlock tables  后 ， 客户端二中的 select 语句 ， 立即执行 ；

![1553908131373](D:/documents/notes/pictures/20230315/192.png) 



#### 5.2.4 结论

锁模式的相互兼容性如表中所示：

![1553905621992](D:/documents/note/assets/database/1553905621992.png) 

由上表可见： 

​	1） 对MyISAM 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；

​	2） 对MyISAM 表的写操作，则会阻塞其他用户对同一表的读和写操作；

​	简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁，则既会阻塞读，又会阻塞写。



此外，MyISAM 的读写锁调度是写优先，这也是MyISAM不适合做写为主的表的存储引擎的原因。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。



#### 5.2.5 查看锁的争用情况

``` 
show open tables；
```

![1556443073322](D:/documents/notes/pictures/20230315/193.png) 

In_user : 表当前被查询使用的次数。如果该数为零，则表是打开的，但是当前没有被使用。

Name_locked：表名称是否被锁定。名称锁定用于取消表或对表进行重命名等操作。



```
show status like 'Table_locks%';
```

![1556443170082](D:/documents/notes/pictures/20230315/194.png) 

Table_locks_immediate ： 指的是能够立即获得表级锁的次数，每立即获取锁，值加1。

Table_locks_waited ： 指的是不能立即获取表级锁而需要等待的次数，每等待一次，该值加1，此值高说明存在着较为严重的表级锁争用情况。



### 5.3 InnoDB 行锁

#### 5.3.1 行锁介绍

行锁特点 ：偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。

InnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是 采用了行级锁。



#### 5.3.2 背景知识

**事务及其ACID属性**

事务是由一组SQL语句组成的逻辑处理单元。

事务具有以下4个特性，简称为事务ACID属性。

| ACID属性             | 含义                                                         |
| -------------------- | ------------------------------------------------------------ |
| 原子性（Atomicity）  | 事务是一个原子操作单元，其对数据的修改，要么全部成功，要么全部失败。 |
| 一致性（Consistent） | 在事务开始和完成时，数据都必须保持一致状态。                 |
| 隔离性（Isolation）  | 数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的 “独立” 环境下运行。 |
| 持久性（Durable）    | 事务完成之后，对于数据的修改是永久的。                       |



**并发事务处理带来的问题**

| 问题                               | 含义                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| 丢失更新（Lost Update）            | 当两个或多个事务选择同一行，最初的事务修改的值，会被后面的事务修改的值覆盖。 |
| 脏读（Dirty Reads）                | 当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 |
| 不可重复读（Non-Repeatable Reads） | 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现和以前读出的数据不一致。 |
| 幻读（Phantom Reads）              | 一个事务按照相同的查询条件重新读取以前查询过的数据，却发现其他事务插入了满足其查询条件的新数据。 |



**事务隔离级别**

为了解决上述提到的事务并发问题，数据库提供一定的事务隔离机制来解决这个问题。数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使用事务在一定程度上“串行化” 进行，这显然与“并发” 是矛盾的。 

数据库的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏写、脏读、不可重复读、幻读这几类问题。

| 隔离级别                            | 丢失更新 | 脏读 | 不可重复读 | 幻读 |
| ----------------------------------- | -------- | ---- | ---------- | ---- |
| 读未提交（read-uncommitted）        | ×        | √    | √          | √    |
| 不可重复读（read-committed）        | ×        | ×    | √          | √    |
| 可重复读（repeatable-read）（默认） | ×        | ×    | ×          | √    |
| 串行化（serializable）              | ×        | ×    | ×          | ×    |

备注 ： √  代表可能出现 ， × 代表不会出现 。

Mysql 的数据库的默认隔离级别为 Repeatable read ， 查看方式：

```
show variables like 'tx_isolation';
```

![1554331600009](D:/documents/notes/pictures/20230315/195.png)  



#### 5.3.3 InnoDB 的行锁模式

InnoDB  实现了以下两种类型的行锁。

- 共享锁（S）：又称为读锁，简称S锁，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。
- 排他锁（X）：又称为写锁，简称X锁，排他锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。

对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；

对于普通SELECT语句，InnoDB不会加任何锁；



可以通过以下语句显示给记录集加共享锁或排他锁 。

```
共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE

排他锁（X) ：SELECT * FROM table_name WHERE ... FOR UPDATE
```



#### 5.3.4 案例准备工作

```sql
create table test_innodb_lock(
	id int(11),
	name varchar(16),
	sex varchar(1)
)engine = innodb default charset=utf8;

insert into test_innodb_lock values(1,'100','1');
insert into test_innodb_lock values(3,'3','1');
insert into test_innodb_lock values(4,'400','0');
insert into test_innodb_lock values(5,'500','1');
insert into test_innodb_lock values(6,'600','0');
insert into test_innodb_lock values(7,'700','0');
insert into test_innodb_lock values(8,'800','1');
insert into test_innodb_lock values(9,'900','1');
insert into test_innodb_lock values(1,'200','0');

create index idx_test_innodb_lock_id on test_innodb_lock(id);
create index idx_test_innodb_lock_name on test_innodb_lock(name);
```



#### 5.3.5 行锁基本演示

| Session-1                                                    | Session-2                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![1554354615030](../pictures/20230315/196.png)      关闭自动提交功能 | ![1554354601867](../pictures/20230315/197.png)  关闭自动提交功能 |
| ![1554354713628](../pictures/20230315/198.png) 可以正常的查询出全部的数据 | ![1554354717336](../pictures/20230315/199.png) 可以正常的查询出全部的数据 |
| ![1554354830589](../pictures/20230315/200.png)查询id 为3的数据 ； | ![1554354832708](../pictures/20230315/201.png)获取id为3的数据 ； |
| ![1554382789984](../pictures/20230315/202.png) 更新id为3的数据，但是不提交； | ![1554382905352](../pictures/20230315/203.png) 更新id为3 的数据， 出于等待状态 |
| ![1554382977653](../pictures/20230315/204.png) 通过commit， 提交事务 | ![1554383044542](../pictures/20230315/205.png) 解除阻塞，更新正常进行 |
| 以上， 操作的都是同一行的数据，接下来，演示不同行的数据 ：   |                                                              |
| ![1554385220580](../pictures/20230315/206.png) 更新id为3数据，正常的获取到行锁 ， 执行更新 ； | ![1554385236768](../pictures/20230315/207.png) 由于与Session-1 操作不是同一行，获取当前行锁，执行更新； |



#### 5.3.6 无索引行锁升级为表锁

如果不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，实际效果跟表锁一样。



查看当前表的索引 ： show  index  from test_innodb_lock ;

![1554385956215](D:/documents/notes/pictures/20230315/208.png) 

| Session-1                                                    | Session-2                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 关闭事务的自动提交![1554386287454](../pictures/20230315/209.png) | 关闭事务的自动提交![1554386312524](../pictures/20230315/210.png) |
| 执行更新语句 ：![1554386654793](../pictures/20230315/211.png) | 执行更新语句， 但处于阻塞状态：![1554386685610](../pictures/20230315/212.png) |
| 提交事务：![1554386721653](../pictures/20230315/213.png)     | 解除阻塞，执行更新成功 ：![1554386750004](../pictures/20230315/214.png) |
|                                                              | 执行提交操作 ：![1554386804807](../pictures/20230315/215.png) |

由于 执行更新时 ， name字段本来为varchar类型， 我们是作为数字类型使用，存在类型转换，索引失效，最终行锁变为表锁 ；



#### 5.3.7 间隙锁危害

当我们用范围条件，而不是使用相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据进行加锁； 对于键值在条件范围内但并不存在的记录，叫做 "间隙（GAP）" ， InnoDB也会对这个 "间隙" 加锁，这种锁机制就是所谓的 间隙锁（Next-Key锁） 。

示例 ： 

| Session-1                                                    | Session-2                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 关闭事务自动提交 ![1554387987130](../pictures/20230315/216.png) | 关闭事务自动提交![1554387994533](../pictures/20230315/217.png) |
| 根据id范围更新数据![1554388492478](../pictures/20230315/218.png) |                                                              |
|                                                              | 插入id为2的记录， 出于阻塞状态![1554388515936](../pictures/20230315/219.png) |
| 提交事务 ；![1554388149305](../pictures/20230315/220.png)    |                                                              |
|                                                              | 解除阻塞 ， 执行插入操作 ：![1554388548562](../pictures/20230315/221.png) |
|                                                              | 提交事务 ：                                                  |



#### 5.3.8 InnoDB 行锁争用情况

```sql
show  status like 'innodb_row_lock%';
```

![1556455943670](D:/documents/notes/pictures/20230315/222.png)

```
Innodb_row_lock_current_waits: 当前正在等待锁定的数量

Innodb_row_lock_time: 从系统启动到现在锁定总时间长度

Innodb_row_lock_time_avg:每次等待所花平均时长

Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间

Innodb_row_lock_waits: 系统启动后到现在总共等待的次数


当等待的次数很高，而且每次等待的时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。

```



#### 5.3.9 总结

InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高一些，但是在整体并发处理能力方面要远远由于MyISAM的表锁的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势。

但是，InnoDB的行级锁同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。



优化建议：

- 尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁。
- 合理设计索引，尽量缩小锁的范围
- 尽可能减少索引条件，及索引范围，避免间隙锁
- 尽量控制事务大小，减少锁定资源量和时间长度
- 尽可使用低级别事务隔离（但是需要业务层面满足需求）



## 6. 常用SQL技巧

### 6.1 SQL执行顺序

编写顺序

```SQL
SELECT DISTINCT
	<select list>
FROM
	<left_table> <join_type>
JOIN
	<right_table> ON <join_condition>
WHERE
	<where_condition>
GROUP BY
	<group_by_list>
HAVING
	<having_condition>
ORDER BY
	<order_by_condition>
LIMIT
	<limit_params>
```

执行顺序

``` sql
FROM	<left_table>

ON 		<join_condition>

<join_type>		JOIN	<right_table>

WHERE		<where_condition>

GROUP BY 	<group_by_list>

HAVING		<having_condition>

SELECT DISTINCT		<select list>

ORDER BY	<order_by_condition>

LIMIT		<limit_params>
```



### 6.2 正则表达式使用

正则表达式（Regular Expression）是指一个用来描述或者匹配一系列符合某个句法规则的字符串的单个字符串。

| 符号   | 含义                          |
| ------ | ----------------------------- |
| ^      | 在字符串开始处进行匹配        |
| $      | 在字符串末尾处进行匹配        |
| .      | 匹配任意单个字符, 包括换行符  |
| [...]  | 匹配出括号内的任意字符        |
| [^...] | 匹配不出括号内的任意字符      |
| a*     | 匹配零个或者多个a(包括空串)   |
| a+     | 匹配一个或者多个a(不包括空串) |
| a?     | 匹配零个或者一个a             |
| a1\|a2 | 匹配a1或a2                    |
| a(m)   | 匹配m个a                      |
| a(m,)  | 至少匹配m个a                  |
| a(m,n) | 匹配m个a 到 n个a              |
| a(,n)  | 匹配0到n个a                   |
| (...)  | 将模式元素组成单一元素        |

```
select * from emp where name regexp '^T';

select * from emp where name regexp '2$';

select * from emp where name regexp '[uvw]';
```



### 6.3 MySQL 常用函数

数字函数

| 函数名称        | 作 用                                                      |
| --------------- | ---------------------------------------------------------- |
| ABS             | 求绝对值                                                   |
| SQRT            | 求二次方根                                                 |
| MOD             | 求余数                                                     |
| CEIL 和 CEILING | 两个函数功能相同，都是返回不小于参数的最小整数，即向上取整 |
| FLOOR           | 向下取整，返回值转化为一个BIGINT                           |
| RAND            | 生成一个0~1之间的随机数，传入整数参数是，用来产生重复序列  |
| ROUND           | 对所传参数进行四舍五入                                     |
| SIGN            | 返回参数的符号                                             |
| POW 和 POWER    | 两个函数的功能相同，都是所传参数的次方的结果值             |
| SIN             | 求正弦值                                                   |
| ASIN            | 求反正弦值，与函数 SIN 互为反函数                          |
| COS             | 求余弦值                                                   |
| ACOS            | 求反余弦值，与函数 COS 互为反函数                          |
| TAN             | 求正切值                                                   |
| ATAN            | 求反正切值，与函数 TAN 互为反函数                          |
| COT             | 求余切值                                                   |

字符串函数

| 函数名称  | 作 用                                                        |
| --------- | ------------------------------------------------------------ |
| LENGTH    | 计算字符串长度函数，返回字符串的字节长度                     |
| CONCAT    | 合并字符串函数，返回结果为连接参数产生的字符串，参数可以使一个或多个 |
| INSERT    | 替换字符串函数                                               |
| LOWER     | 将字符串中的字母转换为小写                                   |
| UPPER     | 将字符串中的字母转换为大写                                   |
| LEFT      | 从左侧字截取符串，返回字符串左边的若干个字符                 |
| RIGHT     | 从右侧字截取符串，返回字符串右边的若干个字符                 |
| TRIM      | 删除字符串左右两侧的空格                                     |
| REPLACE   | 字符串替换函数，返回替换后的新字符串                         |
| SUBSTRING | 截取字符串，返回从指定位置开始的指定长度的字符换             |
| REVERSE   | 字符串反转（逆序）函数，返回与原始字符串顺序相反的字符串     |

日期函数

| 函数名称                | 作 用                                                        |
| ----------------------- | ------------------------------------------------------------ |
| CURDATE 和 CURRENT_DATE | 两个函数作用相同，返回当前系统的日期值                       |
| CURTIME 和 CURRENT_TIME | 两个函数作用相同，返回当前系统的时间值                       |
| NOW 和  SYSDATE         | 两个函数作用相同，返回当前系统的日期和时间值                 |
| MONTH                   | 获取指定日期中的月份                                         |
| MONTHNAME               | 获取指定日期中的月份英文名称                                 |
| DAYNAME                 | 获取指定曰期对应的星期几的英文名称                           |
| DAYOFWEEK               | 获取指定日期对应的一周的索引位置值                           |
| WEEK                    | 获取指定日期是一年中的第几周，返回值的范围是否为 0〜52 或 1〜53 |
| DAYOFYEAR               | 获取指定曰期是一年中的第几天，返回值范围是1~366              |
| DAYOFMONTH              | 获取指定日期是一个月中是第几天，返回值范围是1~31             |
| YEAR                    | 获取年份，返回值范围是 1970〜2069                            |
| TIME_TO_SEC             | 将时间参数转换为秒数                                         |
| SEC_TO_TIME             | 将秒数转换为时间，与TIME_TO_SEC 互为反函数                   |
| DATE_ADD 和 ADDDATE     | 两个函数功能相同，都是向日期添加指定的时间间隔               |
| DATE_SUB 和 SUBDATE     | 两个函数功能相同，都是向日期减去指定的时间间隔               |
| ADDTIME                 | 时间加法运算，在原始时间上添加指定的时间                     |
| SUBTIME                 | 时间减法运算，在原始时间上减去指定的时间                     |
| DATEDIFF                | 获取两个日期之间间隔，返回参数 1 减去参数 2 的值             |
| DATE_FORMAT             | 格式化指定的日期，根据参数返回指定格式的值                   |
| WEEKDAY                 | 获取指定日期在一周内的对应的工作日索引                       |

聚合函数

| 函数名称 | 作用                             |
| -------- | -------------------------------- |
| MAX      | 查询指定列的最大值               |
| MIN      | 查询指定列的最小值               |
| COUNT    | 统计查询结果的行数               |
| SUM      | 求和，返回指定列的总和           |
| AVG      | 求平均值，返回指定列数据的平均值 |





## 1. MySql中常用工具

### 1.1 mysql

该mysql不是指mysql服务，而是指mysql的客户端工具。

语法 ：

```
mysql [options] [database]
```

#### 1.1.1 连接选项

```
参数 ： 
	-u, --user=name			指定用户名
	-p, --password[=name]	指定密码
	-h, --host=name			指定服务器IP或域名
	-P, --port=#			指定连接端口

示例 ：
	mysql -h 127.0.0.1 -P 3306 -u root -p
	
	mysql -h127.0.0.1 -P3306 -uroot -p2143
	
```

#### 1.1.2 执行选项

```
-e, --execute=name		执行SQL语句并退出
```

此选项可以在Mysql客户端执行SQL语句，而不用连接到MySQL数据库再执行，对于一些批处理脚本，这种方式尤其方便。

```
示例：
	mysql -uroot -p2143 db01 -e "select * from tb_book";
```

![1555325632715](D:/documents/notes/pictures/20230315/223.png) 



### 1.2 mysqladmin

mysqladmin 是一个执行管理操作的客户端程序。可以用它来检查服务器的配置和当前状态、创建并删除数据库等。

可以通过 ： mysqladmin --help  指令查看帮助文档

![1555326108697](D:/documents/notes/pictures/20230315/224.png) 

```
示例 ：
	mysqladmin -uroot -p2143 create 'test01';  
	mysqladmin -uroot -p2143 drop 'test01';
	mysqladmin -uroot -p2143 version;
	
```



### 1.3 mysqlbinlog

由于服务器生成的二进制日志文件以二进制格式保存，所以如果想要检查这些文本的文本格式，就会使用到mysqlbinlog 日志管理工具。

语法 ：

```
mysqlbinlog [options]  log-files1 log-files2 ...

选项：
	
	-d, --database=name : 指定数据库名称，只列出指定的数据库相关操作。
	
	-o, --offset=# : 忽略掉日志中的前n行命令。
	
	-r,--result-file=name : 将输出的文本格式日志输出到指定文件。
	
	-s, --short-form : 显示简单格式， 省略掉一些信息。
	
	--start-datatime=date1  --stop-datetime=date2 : 指定日期间隔内的所有日志。
	
	--start-position=pos1 --stop-position=pos2 : 指定位置间隔内的所有日志。
```



### 1.4 mysqldump

mysqldump 客户端工具用来备份数据库或在不同数据库之间进行数据迁移。备份内容包含创建表，及插入表的SQL语句。

语法 ：

```
mysqldump [options] db_name [tables]

mysqldump [options] --database/-B db1 [db2 db3...]

mysqldump [options] --all-databases/-A
```

#### 1.4.1 连接选项

```
参数 ： 
	-u, --user=name			指定用户名
	-p, --password[=name]	指定密码
	-h, --host=name			指定服务器IP或域名
	-P, --port=#			指定连接端口
```



#### 1.4.2 输出内容选项

```
参数：
	--add-drop-database		在每个数据库创建语句前加上 Drop database 语句
	--add-drop-table		在每个表创建语句前加上 Drop table 语句 , 默认开启 ; 不开启 (--skip-add-drop-table)
	
	-n, --no-create-db		不包含数据库的创建语句
	-t, --no-create-info	不包含数据表的创建语句
	-d --no-data			不包含数据
	
	 -T, --tab=name			自动生成两个文件：一个.sql文件，创建表结构的语句；
	 						一个.txt文件，数据文件，相当于select into outfile  
```

```
示例 ： 
	mysqldump -uroot -p2143 db01 tb_book --add-drop-database --add-drop-table > a
	
	mysqldump -uroot -p2143 -T /tmp test city
```

![1555501806693](D:/documents/notes/solid/assets/database/1555501806693.png) 



### 1.5 mysqlimport/source

mysqlimport 是客户端数据导入工具，用来导入mysqldump 加 -T 参数后导出的文本文件。

语法：

```
mysqlimport [options]  db_name  textfile1  [textfile2...]
```

示例：

```
mysqlimport -uroot -p2143 test /tmp/city.txt
```



如果需要导入sql文件,可以使用mysql中的source 指令 : 

```
source /root/tb_book.sql
```



### 1.6 mysqlshow

mysqlshow 客户端对象查找工具，用来很快地查找存在哪些数据库、数据库中的表、表中的列或者索引。

语法：

```
mysqlshow [options] [db_name [table_name [col_name]]]
```

参数：

```
--count		显示数据库及表的统计信息（数据库，表 均可以不指定）

-i			显示指定数据库或者指定表的状态信息
```



示例：

```
#查询每个数据库的表的数量及表中记录的数量
mysqlshow -uroot -p2143 --count

#查询test库中每个表中的字段书，及行数
mysqlshow -uroot -p2143 test --count

#查询test库中book表的详细情况
mysqlshow -uroot -p2143 test book --count

```



## 2. Mysql 日志

在任何一种数据库中，都会有各种各样的日志，记录着数据库工作的方方面面，以帮助数据库管理员追踪数据库曾经发生过的各种事件。MySQL 也不例外，在 MySQL 中，有 4 种不同的日志，分别是错误日志、二进制日志（BINLOG 日志）、查询日志和慢查询日志，这些日志记录着数据库在不同方面的踪迹。

### 2.1 错误日志

错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，可以首先查看此日志。

该日志是默认开启的 ， 默认存放目录为 mysql 的数据目录（var/lib/mysql）, 默认的日志文件名为  hostname.err（hostname是主机名）。

查看日志位置指令 ： 

```sql
show variables like 'log_error%';
```

![1553993244446](D:/documents/notes/pictures/20230315/225.png) 



查看日志内容 ： 

```shell
tail -f /var/lib/mysql/xaxh-server.err
```

![1553993537874](D:/documents/notes/pictures/20230315/226.png) 



### 2.2 二进制日志

#### 2.2.1概述

二进制日志（BINLOG）记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但是不包括数据查询语句。此日志对于灾难时的数据恢复起着极其重要的作用，MySQL的主从复制， 就是通过该binlog实现的。

二进制日志，默认情况下是没有开启的，需要到MySQL的配置文件中开启，并配置MySQL日志的格式。 

配置文件位置 : /usr/my.cnf

日志存放位置 : 配置时，给定了文件名但是没有指定路径，日志默认写入Mysql的数据目录。

```
#配置开启binlog日志， 日志的文件前缀为 mysqlbin -----> 生成的文件名如 : mysqlbin.000001,mysqlbin.000002
log_bin=mysqlbin

#配置二进制日志的格式
binlog_format=STATEMENT

```



#### 2.2.2 日志格式

**STATEMENT**

该日志格式在日志文件中记录的都是SQL语句（statement），每一条对数据进行修改的SQL都会记录在日志文件中，通过Mysql提供的mysqlbinlog工具，可以清晰的查看到每条语句的文本。主从复制的时候，从库（slave）会将日志解析为原文本，并在从库重新执行一次。



**ROW**

该日志格式在日志文件中记录的是每一行的数据变更，而不是记录SQL语句。比如，执行SQL语句 ： update tb_book set status='1' , 如果是STATEMENT 日志格式，在日志中会记录一行SQL文件； 如果是ROW，由于是对全表进行更新，也就是每一行记录都会发生变更，ROW 格式的日志中会记录每一行的数据变更。



**MIXED**

这是目前MySQL默认的日志格式，即混合了STATEMENT 和 ROW两种格式。默认情况下采用STATEMENT，但是在一些特殊情况下采用ROW来进行记录。MIXED 格式能尽量利用两种模式的优点，而避开他们的缺点。



#### 2.2.3 日志读取

由于日志以二进制方式存储，不能直接读取，需要用mysqlbinlog工具来查看，语法如下 ：

```
mysqlbinlog log-file；

```



**查看STATEMENT格式日志** 

执行插入语句 ：

```SQL
insert into tb_book values(null,'Lucene','2088-05-01','0');
```

 查看日志文件 ：

![1554079717375](D:/documents/notes/pictures/20230315/227.png) 

mysqlbin.index : 该文件是日志索引文件 ， 记录日志的文件名；

mysqlbing.000001 ：日志文件

查看日志内容 ：

```
mysqlbinlog mysqlbing.000001；

```

![1554080016778](D:/documents/notes/pictures/20230315/229.png) 



**查看ROW格式日志**

配置 :

```
#配置开启binlog日志， 日志的文件前缀为 mysqlbin -----> 生成的文件名如 : mysqlbin.000001,mysqlbin.000002
log_bin=mysqlbin

#配置二进制日志的格式
binlog_format=ROW

```

插入数据 :

```sql
insert into tb_book values(null,'SpringCloud实战','2088-05-05','0');
```

如果日志格式是 ROW , 直接查看数据 , 是查看不懂的 ; 可以在mysqlbinlog 后面加上参数 -vv  

```SQL
mysqlbinlog -vv mysqlbin.000002 
```

![1554095452022](D:/documents/notes/pictures/20230315/230.png) 



#### 2.2.4 日志删除

对于比较繁忙的系统，由于每天生成日志量大 ，这些日志如果长时间不清除，将会占用大量的磁盘空间。下面我们将会讲解几种删除日志的常见方法 ：

**方式一** 

通过 Reset Master 指令删除全部 binlog 日志，删除之后，日志编号，将从 xxxx.000001重新开始 。

查询之前 ，先查询下日志文件 ： 

![1554118609489](D:/documents/notes/pictures/20230315/231.png)   

执行删除日志指令： 

```
Reset Master
```

执行之后， 查看日志文件 ：

![1554118675264](D:/documents/notes/pictures/20230315/232.png) 



**方式二**

执行指令 ``` purge  master logs to 'mysqlbin.******'``` ，该命令将删除  ``` ******``` 编号之前的所有日志。 



**方式三**

执行指令 ``` purge master logs before 'yyyy-mm-dd hh24:mi:ss'``` ，该命令将删除日志为 "yyyy-mm-dd hh24:mi:ss" 之前产生的所有日志 。



**方式四**

设置参数 --expire_logs_days=# ，此参数的含义是设置日志的过期天数， 过了指定的天数后日志将会被自动删除，这样将有利于减少DBA 管理日志的工作量。

配置如下 ： 

![1554125506938](D:/documents/notes/pictures/20230315/233.png) 



### 2.3 查询日志

查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的SQL语句。

默认情况下， 查询日志是未开启的。如果需要开启查询日志，可以设置以下配置 ：

```
#该选项用来开启查询日志 ， 可选值 ： 0 或者 1 ； 0 代表关闭， 1 代表开启 
general_log=1

#设置日志的文件名 ， 如果没有指定， 默认的文件名为 host_name.log 
general_log_file=file_name

```

在 mysql 的配置文件 /usr/my.cnf 中配置如下内容 ： 

![1554128184632](D:/documents/notes/pictures/20230315/234.png) 



配置完毕之后，在数据库执行以下操作 ：

```
select * from tb_book;
select * from tb_book where id = 1;
update tb_book set name = 'lucene入门指南' where id = 5;
select * from tb_book where id < 8;

```



执行完毕之后， 再次来查询日志文件 ： 

![1554128089851](D:/documents/notes/pictures/20230315/235.png) 



### 2.4 慢查询日志

慢查询日志记录了所有执行时间超过参数 long_query_time 设置值并且扫描记录数不小于 min_examined_row_limit 的所有的SQL语句的日志。long_query_time 默认为 10 秒，最小为 0， 精度可以到微秒。



#### 2.4.1 文件位置和格式

慢查询日志默认是关闭的 。可以通过两个参数来控制慢查询日志 ：

```
# 该参数用来控制慢查询日志是否开启， 可取值： 1 和 0 ， 1 代表开启， 0 代表关闭
slow_query_log=1 

# 该参数用来指定慢查询日志的文件名
slow_query_log_file=slow_query.log

# 该选项用来配置查询的时间限制， 超过这个时间将认为值慢查询， 将需要进行日志记录， 默认10s
long_query_time=10

```



#### 2.4.2 日志的读取

和错误日志、查询日志一样，慢查询日志记录的格式也是纯文本，可以被直接读取。

1） 查询long_query_time 的值。

![1554130333472](D:/documents/notes/pictures/20230315/236.png) 



2） 执行查询操作

```sql
select id, title,price,num ,status from tb_item where id = 1;
```

![1554130448709](D:/documents/notes/pictures/20230315/237.png)

由于该语句执行时间很短，为0s ， 所以不会记录在慢查询日志中。



```
select * from tb_item where title like '%阿尔卡特 (OT-927) 炭黑 联通3G手机 双卡双待165454%' ;

```

![1554130532577](D:/documents/notes/pictures/20230315/238.png) 

该SQL语句 ， 执行时长为 26.77s ，超过10s ， 所以会记录在慢查询日志文件中。



3） 查看慢查询日志文件

直接通过cat 指令查询该日志文件 ： 

![1554130669360](D:/documents/notes/pictures/20230315/239.png) 



如果慢查询日志内容很多， 直接查看文件，比较麻烦， 这个时候可以借助于mysql自带的 mysqldumpslow 工具， 来对慢查询日志进行分类汇总。 

![1554130856485](D:/documents/notes/pictures/20230315/240.png) 



## 3. Mysql复制

### 3.1 复制概述

复制是指将主数据库的DDL 和 DML 操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。

MySQL支持一台主库同时向多台从库进行复制， 从库同时也可以作为其他从服务器的主库，实现链状复制。



### 3.2 复制原理

MySQL 的主从复制原理如下。

![1554423698190](D:/documents/notes/pictures/20230315/241.png) 

从上层来看，复制分成三步：

- Master 主库在事务提交时，会把数据变更作为时间 Events 记录在二进制日志文件 Binlog 中。
- 主库推送二进制日志文件 Binlog 中的日志事件到从库的中继日志 Relay Log 。

- slave重做中继日志中的事件，将改变反映它自己的数据。



### 3.3 复制优势

MySQL 复制的有点主要包含以下三个方面：

- 主库出现问题，可以快速切换到从库提供服务。

- 可以在从库上执行查询操作，从主库中更新，实现读写分离，降低主库的访问压力。

- 可以在从库中执行备份，以避免备份期间影响主库的服务。



### 3.4 搭建步骤

#### 3.4.1 master

1） 在master 的配置文件（/usr/my.cnf）中，配置如下内容：

```properties
#mysql 服务ID,保证整个集群环境中唯一
server-id=1

#mysql binlog 日志的存储路径和文件名
log-bin=/var/lib/mysql/mysqlbin

#错误日志,默认已经开启
#log-err

#mysql的安装目录
#basedir

#mysql的临时目录
#tmpdir

#mysql的数据存放目录
#datadir

#是否只读,1 代表只读, 0 代表读写
read-only=0

#忽略的数据, 指不需要同步的数据库
binlog-ignore-db=mysql

#指定同步的数据库
#binlog-do-db=db01
```

2） 执行完毕之后，需要重启Mysql：

```sql
service mysql restart ；
```

3） 创建同步数据的账户，并且进行授权操作：

```sql
grant replication slave on *.* to 'itcast'@'192.168.192.131' identified by 'itcast';	

flush privileges;
```

4） 查看master状态：

```sql
show master status;
```

![1554477759735](D:/documents/notes/pictures/20230315/242.png) 

字段含义：

```
File : 从哪个日志文件开始推送日志文件 
Position ： 从哪个位置开始推送日志
Binlog_Ignore_DB : 指定不需要同步的数据库
```



#### 3.4.2 slave

1） 在 slave 端配置文件中，配置如下内容：

```properties
#mysql服务端ID,唯一
server-id=2

#指定binlog日志
log-bin=/var/lib/mysql/mysqlbin
```

2）  执行完毕之后，需要重启Mysql：

```
service mysql restart；
```

3） 执行如下指令 ：

```sql
change master to master_host= '192.168.192.130', master_user='itcast', master_password='itcast', master_log_file='mysqlbin.000001', master_log_pos=413;
```

指定当前从库对应的主库的IP地址，用户名，密码，从哪个日志文件开始的那个位置开始同步推送日志。

4） 开启同步操作

```
start slave;

show slave status;
```

![1554479387365](D:/documents/notes/pictures/20230315/243.png) 

5） 停止同步操作

```
stop slave;
```



#### 3.4.3 验证同步操作

1） 在主库中创建数据库，创建表，并插入数据 ：

```sql
create database db01;

user db01;

create table user(
	id int(11) not null auto_increment,
	name varchar(50) not null,
	sex varchar(1),
	primary key (id)
)engine=innodb default charset=utf8;

insert into user(id,name,sex) values(null,'Tom','1');
insert into user(id,name,sex) values(null,'Trigger','0');
insert into user(id,name,sex) values(null,'Dawn','1');
```

2） 在从库中查询数据，进行验证 ：

在从库中，可以查看到刚才创建的数据库：

![1554544658640](D:/documents/notes/pictures/20230315/244.png) 

在该数据库中，查询user表中的数据：

![1554544679538](D:/documents/notes/pictures/20230315/245.png) 



## 4. 综合案例

### 4.1 需求分析

在业务系统中，需要记录当前业务系统的访问日志，该访问日志包含：操作人，操作时间，访问类，访问方法，请求参数，请求结果，请求结果类型，请求时长 等信息。记录详细的系统访问日志，主要便于对系统中的用户请求进行追踪，并且在系统 的管理后台可以查看到用户的访问记录。

记录系统中的日志信息，可以通过Spring 框架的AOP来实现。具体的请求处理流程，如下：

![1555075760661](D:/documents/notes/pictures/20230315/246.png) 



### 4.2 搭建案例环境

#### 4.2.1 数据库表

```sql
CREATE DATABASE mysql_demo DEFAULT CHARACTER SET utf8mb4 ；

CREATE TABLE `brand` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) DEFAULT NULL COMMENT '品牌名称',
  `first_char` varchar(1) DEFAULT NULL COMMENT '品牌首字母',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;



CREATE TABLE `item` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '商品id',
  `title` varchar(100) NOT NULL COMMENT '商品标题',
  `price` double(10,2) NOT NULL COMMENT '商品价格，单位为：元',
  `num` int(10) NOT NULL COMMENT '库存数量',
  `categoryid` bigint(10) NOT NULL COMMENT '所属类目，叶子类目',
  `status` varchar(1) DEFAULT NULL COMMENT '商品状态，1-正常，2-下架，3-删除',
  `sellerid` varchar(50) DEFAULT NULL COMMENT '商家ID',
  `createtime` datetime DEFAULT NULL COMMENT '创建时间',
  `updatetime` datetime DEFAULT NULL COMMENT '更新时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='商品表';



CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `username` varchar(45) NOT NULL,
  `password` varchar(96) NOT NULL,
  `name` varchar(45) NOT NULL,
  `birthday` datetime DEFAULT NULL,
  `sex` char(1) DEFAULT NULL,
  `email` varchar(45) DEFAULT NULL,
  `phone` varchar(45) DEFAULT NULL,
  `qq` varchar(32) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;


CREATE TABLE `operation_log` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'ID',
  `operate_class` varchar(200) DEFAULT NULL COMMENT '操作类',
  `operate_method` varchar(200) DEFAULT NULL COMMENT '操作方法',
  `return_class` varchar(200) DEFAULT NULL COMMENT '返回值类型',
  `operate_user` varchar(20) DEFAULT NULL COMMENT '操作用户',
  `operate_time` varchar(20) DEFAULT NULL COMMENT '操作时间',
  `param_and_value` varchar(500) DEFAULT NULL COMMENT '请求参数名及参数值',
  `cost_time` bigint(20) DEFAULT NULL COMMENT '执行方法耗时, 单位 ms',
  `return_value` varchar(200) DEFAULT NULL COMMENT '返回值',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB  DEFAULT CHARSET=utf8mb4;

```



#### 4.2.2 pom.xml 

```xml
<properties>
  <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
  <maven.compiler.source>1.7</maven.compiler.source>
  <maven.compiler.target>1.7</maven.compiler.target>

  <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
  <maven.compiler.source>1.8</maven.compiler.source>
  <maven.compiler.target>1.8</maven.compiler.target>
  <spring.version>5.0.2.RELEASE</spring.version>
  <slf4j.version>1.6.6</slf4j.version>
  <log4j.version>1.2.12</log4j.version>
  <mybatis.version>3.4.5</mybatis.version>
</properties>

<dependencies> <!-- spring -->
  <dependency>
    <groupId>org.aspectj</groupId>
    <artifactId>aspectjweaver</artifactId>
    <version>1.6.8</version>
  </dependency>

  <dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <version>1.16.16</version>
  </dependency>

  <dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-context</artifactId>
    <version>${spring.version}</version>
  </dependency>

  <dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-context-support</artifactId>
    <version>${spring.version}</version>
  </dependency>

  <dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-orm</artifactId>
    <version>${spring.version}</version>
  </dependency>

  <dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-test</artifactId>
    <version>${spring.version}</version>
  </dependency>

  <dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-webmvc</artifactId>
    <version>${spring.version}</version>
  </dependency>

  <dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-tx</artifactId>
    <version>${spring.version}</version>
  </dependency>

  <dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>4.12</version>
    <scope>test</scope>
  </dependency>

  <dependency>
    <groupId>javax.servlet</groupId>
    <artifactId>javax.servlet-api</artifactId>
    <version>3.1.0</version>
    <scope>provided</scope>
  </dependency>

  <dependency>
    <groupId>javax.servlet.jsp</groupId>
    <artifactId>jsp-api</artifactId>
    <version>2.0</version>
    <scope>provided</scope>
  </dependency>


  <dependency>
    <groupId>log4j</groupId>
    <artifactId>log4j</artifactId>
    <version>${log4j.version}</version>
  </dependency>

  <dependency>
    <groupId>org.mybatis</groupId>
    <artifactId>mybatis</artifactId>
    <version>${mybatis.version}</version>
  </dependency>

  <dependency>
    <groupId>org.mybatis</groupId>
    <artifactId>mybatis-spring</artifactId>
    <version>1.3.0</version>
  </dependency>

  <dependency>
    <groupId>c3p0</groupId>
    <artifactId>c3p0</artifactId>
    <version>0.9.1.2</version>
  </dependency>

  <dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>5.1.5</version>
  </dependency>

  <dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-core</artifactId>
    <version>2.9.0</version>
  </dependency>

  <dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.9.0</version>
  </dependency>

  <dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-annotations</artifactId>
    <version>2.9.0</version>
  </dependency>
</dependencies>




 <build>
   <plugins>
     <plugin>
       <groupId>org.apache.tomcat.maven</groupId>
       <artifactId>tomcat7-maven-plugin</artifactId>
       <version>2.2</version>
       <configuration>
         <port>8080</port>
         <path>/</path>
         <uriEncoding>utf-8</uriEncoding>
       </configuration>
     </plugin>
   </plugins>
 </build>
```

#### 4.2.3 web.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://java.sun.com/xml/ns/javaee"
       xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd"
       version="2.5">

    <!-- 解决post乱码 -->
    <filter>
        <filter-name>CharacterEncodingFilter</filter-name>
        <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>
        <init-param>
            <param-name>encoding</param-name>
            <param-value>utf-8</param-value>
        </init-param>
        <init-param>
            <param-name>forceEncoding</param-name>
            <param-value>true</param-value>
        </init-param>
    </filter>
    <filter-mapping>
        <filter-name>CharacterEncodingFilter</filter-name>
        <url-pattern>/*</url-pattern>
    </filter-mapping>

    <context-param>
        <param-name>contextConfigLocation</param-name>
        <param-value>classpath:applicationContext.xml</param-value>
    </context-param>
    <listener>
        <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>
    </listener>


    <servlet>
        <servlet-name>springmvc</servlet-name>
        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>
        <!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载-->
        <init-param>
            <param-name>contextConfigLocation</param-name>
            <param-value>classpath:springmvc.xml</param-value>
        </init-param>
    </servlet>
    <servlet-mapping>
        <servlet-name>springmvc</servlet-name>
        <url-pattern>*.do</url-pattern>
    </servlet-mapping>

    <welcome-file-list>
      <welcome-file>log-datalist.html</welcome-file>
    </welcome-file-list>
</web-app>
```

#### 4.2.4 db.properties

```properties
jdbc.driver=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://192.168.142.128:3306/mysql_demo
jdbc.username=root
jdbc.password=itcast
```

#### 4.2.5 applicationContext.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:aop="http://www.springframework.org/schema/aop"
       xmlns:tx="http://www.springframework.org/schema/tx"
       xmlns:context="http://www.springframework.org/schema/context"
	   xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
                           http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd
                            http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd
                            http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd">

    <!-- 加载配置文件 -->
    <context:property-placeholder location="classpath:db.properties"/>

    <!-- 配置 spring 创建容器时要扫描的包 -->
    <context:component-scan base-package="cn.itcast">
        <context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller">	
        </context:exclude-filter>
    </context:component-scan>

    <!-- 配置 MyBatis 的 Session 工厂 -->
    <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean">
        <property name="dataSource" ref="dataSource"/>
        <property name="typeAliasesPackage" value="cn.itcast.pojo"/>
     </bean>

    <!-- 配置数据源 -->
    <bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource">
        <property name="driverClass" value="${jdbc.driver}"></property>
        <property name="jdbcUrl" value="${jdbc.url}"></property>
        <property name="user" value="${jdbc.username}"></property>
        <property name="password" value="${jdbc.password}"></property>
    </bean>

    <!-- 配置 Mapper 扫描器 -->
    <bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">
        <property name="basePackage" value="cn.itcast.mapper"/>
    </bean>

    <!-- 配置事务管理器 -->
    <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
        <property name="dataSource" ref="dataSource"/>
    </bean>

    <!-- 配置事务的注解驱动 -->
    <tx:annotation-driven transaction-manager="transactionManager"></tx:annotation-driven>
</beans>
```

#### 4.2.6 springmvc.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:mvc="http://www.springframework.org/schema/mvc"
       xmlns:context="http://www.springframework.org/schema/context"
       xmlns:aop="http://www.springframework.org/schema/aop"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
            http://www.springframework.org/schema/beans/spring-beans.xsd
            http://www.springframework.org/schema/mvc
            http://www.springframework.org/schema/mvc/spring-mvc.xsd
            http://www.springframework.org/schema/aop
            http://www.springframework.org/schema/aop/spring-aop.xsd
            http://www.springframework.org/schema/context
            http://www.springframework.org/schema/context/spring-context.xsd">

    <context:component-scan base-package="cn.itcast.controller"></context:component-scan>

    <mvc:annotation-driven></mvc:annotation-driven>

    <aop:aspectj-autoproxy />

</beans>
```

#### 4.2.7 导入基础工程

![1555076434270](D:/documents/notes/pictures/20230315/247.png) 



### 4.3 通过AOP记录操作日志

#### 4.3.1 自定义注解

通过自定义注解，来标示方法需不需要进行记录日志，如果该方法在访问时需要记录日志，则在该方法上标示该注解既可。

```java
@Inherited
@Documented
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface OperateLog {
}
```

#### 4.3.2 定义通知类

```java
@Component
@Aspect
public class OperateAdvice {
   
   private static Logger log = Logger.getLogger(OperateAdvice.class);
   
   @Autowired
   private OperationLogService operationLogService;
   

   @Around("execution(* cn.itcast.controller.*.*(..)) && @annotation(operateLog)")
   public Object insertLogAround(ProceedingJoinPoint pjp , OperateLog operateLog) throws Throwable{
      System.out.println(" ************************ 记录日志 [start]  ****************************** ");
      
      OperationLog op = new OperationLog();
      
      DateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

      op.setOperateTime(sdf.format(new Date()));
      op.setOperateUser(DataUtils.getRandStr(8));
      
      op.setOperateClass(pjp.getTarget().getClass().getName());
      op.setOperateMethod(pjp.getSignature().getName());
      
      //获取方法调用时传递的参数
      Object[] args = pjp.getArgs();
      op.setParamAndValue(Arrays.toString(args));

      long start_time = System.currentTimeMillis();

      //放行
      Object object = pjp.proceed();

      long end_time = System.currentTimeMillis();
      op.setCostTime(end_time - start_time);

      if(object != null){
         op.setReturnClass(object.getClass().getName());
         op.setReturnValue(object.toString());
      }else{
         op.setReturnClass("java.lang.Object");
         op.setParamAndValue("void");
      }

      log.error(JsonUtils.obj2JsonString(op));

      operationLogService.insert(op);
      
      System.out.println(" ************************** 记录日志 [end]  *************************** ");
      
      return object;
   }
   
}
```

#### 4.3.3 方法上加注解

在需要记录日志的方法上加上注解@OperateLog。

```java
@OperateLog
@RequestMapping("/insert")
public Result insert(@RequestBody Brand brand){
    try {
        brandService.insert(brand);
        return new Result(true,"操作成功");
    } catch (Exception e) {
        e.printStackTrace();
        return new Result(false,"操作失败");
    }
}
```



### 4.4 日志查询后端代码实现

#### 4.4.1 Mapper接口

```java
public interface OperationLogMapper {

    public void insert(OperationLog operationLog);

    public List<OperationLog> selectListByCondition(Map dataMap);

    public Long countByCondition(Map dataMap);

}
```

#### 4.4.2 Mapper.xml 映射配置文件

```xml
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" >
<mapper namespace="cn.itcast.mapper.OperationLogMapper" >

    <insert id="insert" parameterType="operationLog">
        INSERT INTO operation_log(id,return_value,return_class,operate_user,operate_time,param_and_value,
        operate_class,operate_method,cost_time)
      VALUES(NULL,#{returnValue},#{returnClass},#{operateUser},#{operateTime},#{paramAndValue},
        #{operateClass},#{operateMethod},#{costTime})
    </insert>

    <select id="selectListByCondition" parameterType="map" resultType="operationLog">
      select
        id ,
        operate_class as operateClass ,
        operate_method as operateMethod,
        return_class as returnClass,
        operate_user as operateUser,
        operate_time as operateTime,
        param_and_value as paramAndValue,
        cost_time as costTime,
        return_value as returnValue
      from operation_log
      <include refid="oplog_where"/>
      limit #{start},#{size}
    </select>


    <select id="countByCondition" resultType="long" parameterType="map">
        select count(*) from operation_log
        <include refid="oplog_where"/>
    </select>


    <sql id="oplog_where">
        <where>
            <if test="operateClass != null and operateClass != '' ">
                and operate_class = #{operateClass}
            </if>
            <if test="operateMethod != null and operateMethod != '' ">
                and operate_method = #{operateMethod}
            </if>
            <if test="returnClass != null and returnClass != '' ">
                and return_class = #{returnClass}
            </if>
            <if test="costTime != null">
                and cost_time =  #{costTime}
            </if>
        </where>
    </sql>

</mapper>
```

#### 4.4.3 Service

```java
@Service
@Transactional
public class OperationLogService {

    //private static Logger logger = Logger.getLogger(OperationLogService.class);

    @Autowired
    private OperationLogMapper operationLogMapper;

    //插入数据
    public void insert(OperationLog operationLog){
        operationLogMapper.insert(operationLog);
    }

    //根据条件查询
    public PageResult selectListByCondition(Map dataMap, Integer pageNum , Integer pageSize){

       if(paramMap ==null){
            paramMap = new HashMap();
        }
        paramMap.put("start" , (pageNum-1)*rows);
        paramMap.put("rows",rows);

        Object costTime = paramMap.get("costTime");
        if(costTime != null){
            if("".equals(costTime.toString())){
                paramMap.put("costTime",null);
            }else{
                paramMap.put("costTime",new Long(paramMap.get("costTime").toString()));
            }
        }

        System.out.println(dataMap);


        long countStart = System.currentTimeMillis();
        Long count = operationLogMapper.countByCondition(dataMap);
        long countEnd = System.currentTimeMillis();
        System.out.println("Count Cost Time : " + (countEnd-countStart)+" ms");


        List<OperationLog> list = operationLogMapper.selectListByCondition(dataMap);
        long queryEnd = System.currentTimeMillis();
        System.out.println("Query Cost Time : " + (queryEnd-countEnd)+" ms");


        return new PageResult(count,list);

    }

}
```

#### 4.4.4 Controller

```java
@RestController
@RequestMapping("/operationLog")
public class OperationLogController {

    @Autowired
    private OperationLogService operationLogService;

    @RequestMapping("/findList")
    public PageResult findList(@RequestBody Map dataMap, Integer pageNum , Integer pageSize){
        PageResult page = operationLogService.selectListByCondition(dataMap, pageNum, pageSize);
        return page;
    }

}
```



### 4.5 日志查询前端代码实现

前端代码使用 BootStrap + AdminLTE 进行布局， 使用Vuejs 进行视图层展示。

#### 4.5.1 js

```html
<script>
   var vm = new Vue({
       el: '#app',
       data: {
           dataList:[],
           searchEntity:{
               operateClass:'',
               operateMethod:'',
               returnClass:'',
               costTime:''
           },

           page: 1,  //显示的是哪一页
           pageSize: 10, //每一页显示的数据条数
           total: 150, //记录总数
           maxPage:8  //最大页数
       },
       methods: {
           pageHandler: function (page) {
               this.page = page;
               this.search();
           },

           search: function () {
               var _this = this;
               this.showLoading();
               axios.post('/operationLog/findList.do?pageNum=' + _this.page + "&pageSize=" + _this.pageSize, _this.searchEntity).then(function (response) {
                   if (response) {
                       _this.dataList = response.data.dataList;
                       _this.total = response.data.total;
                       _this.hideLoading();
                   }
               })
           },

           showLoading: function () {
               $('#loadingModal').modal({backdrop: 'static', keyboard: false});
           },

           hideLoading: function () {
               $('#loadingModal').modal('hide');
           },
       },

       created:function(){
           this.pageHandler(1);
       }
   });

</script>
```

#### 4.5.2 列表数据展示

```html
<tr v-for="item in dataList">
    <td><input name="ids" type="checkbox"></td>
    <td>{{item.id}}</td>
    <td>{{item.operateClass}}</td>
    <td>{{item.operateMethod}}</td>
    <td>{{item.returnClass}}</td>
    <td>{{item.returnValue}}</td>
    <td>{{item.operateUser}}</td>
    <td>{{item.operateTime}}</td>
    <td>{{item.costTime}}</td>
    <td class="text-center">
        <button type="button" class="btn bg-olive btn-xs">详情</button>
        <button type="button" class="btn bg-olive btn-xs">删除</button>
    </td>
</tr>
```

4.5.3 分页插件

```html
<div class="wrap" id="wrap">
    <zpagenav v-bind:page="page" v-bind:page-size="pageSize" v-bind:total="total"
              v-bind:max-page="maxPage"  v-on:pagehandler="pageHandler">
    </zpagenav>
</div>
```

### 4.6 联调测试

可以通过postman来访问业务系统，再查看数据库中的日志信息，验证能不能将用户的访问日志记录下来。

![1555077276426](D:/documents/notes/pictures/20230315/248.png) 



### 4.7 分析性能问题

系统中用户访问日志的数据量，随着时间的推移，这张表的数据量会越来越大，因此我们需要根据业务需求，来对日志查询模块的性能进行优化。

1） 分页查询优化

由于在进行日志查询时，是进行分页查询，那也就意味着，在查看时，至少需要查询两次：

A. 查询符合条件的总记录数。--> count 操作

B. 查询符合条件的列表数据。--> 分页查询 limit 操作

通常来说，count() 都需要扫描大量的行（意味着需要访问大量的数据）才能获得精确的结果，因此是很难对该SQL进行优化操作的。如果需要对count进行优化，可以采用另外一种思路，可以增加汇总表，或者redis缓存来专门记录该表对应的记录数，这样的话，就可以很轻松的实现汇总数据的查询，而且效率很高，但是这种统计并不能保证百分之百的准确 。对于数据库的操作，“快速、精确、实现简单”，三者永远只能满足其二，必须舍掉其中一个。

2） 条件查询优化

针对于条件查询,需要对查询条件,及排序字段建立索引。

3） 读写分离

通过主从复制集群，来完成读写分离，使写操作走主节点， 而读操作，走从节点。

4） MySQL服务器优化

5） 应用优化





### 4.8 性能优化 - 分页

#### 4.8.1 优化count

创建一张表用来记录日志表的总数据量：

```SQL
create table log_counter(
	logcount bigint not null
)engine = innodb default CHARSET = utf8;
```

在每次插入数据之后，更新该表 ：

```xml
<update id="updateLogCounter" >
    update log_counter set logcount = logcount + 1
</update>
```

在进行分页查询时, 获取总记录数，从该表中查询既可。

```xml
<select id="countLogFromCounter" resultType="long">
    select logcount from log_counter limit 1
</select>
```



#### 4.8.2 优化 limit

在进行分页时，一般通过创建覆盖索引，能够比较好的提高性能。一个非常常见，而又非常头疼的分页场景就是 "limit 1000000,10" ，此时MySQL需要搜索出前1000010 条记录后，仅仅需要返回第 1000001 到 1000010 条记录，前1000000 记录会被抛弃，查询代价非常大。 

![1555081714638](D:/documents/note/assets/database/1555081714638.png) 

当点击比较靠后的页码时，就会出现这个问题，查询效率非常慢。

优化SQL：

```sql
select * from operation_log limit 3000000 , 10;
```

将上述SQL优化为 : 

```SQL
select * from operation_log t , (select id from operation_log order by id limit 3000000,10) b where t.id = b.id ;
```

```xml
<select id="selectListByCondition" parameterType="map" resultType="operationLog">
  select
    id ,
    operate_class as operateClass ,
    operate_method as operateMethod,
    return_class as returnClass,
    operate_user as operateUser,
    operate_time as operateTime,
    param_and_value as paramAndValue,
    cost_time as costTime,
    return_value as returnValue
  from operation_log t,
    
  (select id from operation_log 
  <where>
    <include refid="oplog_where"/>
  </where>
  order by id limit #{start},#{rows}) b  where t.id = b.id  
</select>
```





### 4.9 性能优化 - 索引

![1555152703824](D:/documents/note/assets/database/1555152703824.png)

当根据操作人进行查询时， 查询的效率很低，耗时比较长。原因就是因为在创建数据库表结构时，并没有针对于 操作人 字段建立索引。

```SQL
CREATE INDEX idx_user_method_return_cost ON operation_log(operate_user,operate_method,return_class,cost_time);
```

同上 ， 为了查询效率高，我们也需要对 操作方法、返回值类型、操作耗时 等字段进行创建索引，以提高查询效率。

```SQL
CREATE INDEX idx_optlog_method_return_cost ON operation_log(operate_method,return_class,cost_time);

CREATE INDEX idx_optlog_return_cost ON operation_log(return_class,cost_time);

CREATE INDEX idx_optlog_cost ON operation_log(cost_time);

```



### 4.10 性能优化 - 排序

在查询数据时，如果业务需求中需要我们对结果内容进行了排序处理 , 这个时候,我们还需要对排序的字段建立适当的索引, 来提高排序的效率 。



### 4.11 性能优化 - 读写分离

#### 4.11.1 概述

在Mysql主从复制的基础上，可以使用读写分离来降低单台Mysql节点的压力，从而来提高访问效率，读写分离的架构如下：

![1555235426739](D:/documents/notes/pictures/20230315/249.png) 

对于读写分离的实现，可以通过Spring AOP 来进行动态的切换数据源，进行操作 ：

#### 4.11.2 实现方式

db.properties

```properties
jdbc.write.driver=com.mysql.jdbc.Driver
jdbc.write.url=jdbc:mysql://192.168.142.128:3306/mysql_demo
jdbc.write.username=root
jdbc.write.password=itcast

jdbc.read.driver=com.mysql.jdbc.Driver
jdbc.read.url=jdbc:mysql://192.168.142.129:3306/mysql_demo
jdbc.read.username=root
jdbc.read.password=itcast
```

applicationContext-datasource.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:aop="http://www.springframework.org/schema/aop"
       xmlns:tx="http://www.springframework.org/schema/tx"
       xmlns:context="http://www.springframework.org/schema/context"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd
        http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd
        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd">


    <!-- 配置数据源 - Read -->
    <bean id="readDataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource" destroy-method="close"  lazy-init="true">
        <property name="driverClass" value="${jdbc.read.driver}"></property>
        <property name="jdbcUrl" value="${jdbc.read.url}"></property>
        <property name="user" value="${jdbc.read.username}"></property>
        <property name="password" value="${jdbc.read.password}"></property>
    </bean>


    <!-- 配置数据源 - Write -->
    <bean id="writeDataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"  destroy-method="close"  lazy-init="true">
        <property name="driverClass" value="${jdbc.write.driver}"></property>
        <property name="jdbcUrl" value="${jdbc.write.url}"></property>
        <property name="user" value="${jdbc.write.username}"></property>
        <property name="password" value="${jdbc.write.password}"></property>
    </bean>


    <!-- 配置动态分配的读写 数据源 -->
    <bean id="dataSource" class="cn.itcast.aop.datasource.ChooseDataSource" lazy-init="true">
        <property name="targetDataSources">
            <map key-type="java.lang.String" value-type="javax.sql.DataSource">
                <entry key="write" value-ref="writeDataSource"/>
                <entry key="read" value-ref="readDataSource"/>
            </map>
        </property>

        <property name="defaultTargetDataSource" ref="writeDataSource"/>

        <property name="methodType">
            <map key-type="java.lang.String">
                <entry key="read" value=",get,select,count,list,query,find"/>
                <entry key="write" value=",add,create,update,delete,remove,insert"/>
            </map>
        </property>
    </bean>

</beans>
```

ChooseDataSource

```java
public class ChooseDataSource extends AbstractRoutingDataSource {

    public static Map<String, List<String>> METHOD_TYPE_MAP = new HashMap<String, List<String>>();

    /**
     * 实现父类中的抽象方法，获取数据源名称
     * @return
     */
    protected Object determineCurrentLookupKey() {
        return DataSourceHandler.getDataSource();
    }

    // 设置方法名前缀对应的数据源
    public void setMethodType(Map<String, String> map) {
        for (String key : map.keySet()) {
            List<String> v = new ArrayList<String>();
            String[] types = map.get(key).split(",");
            for (String type : types) {
                if (!StringUtils.isEmpty(type)) {
                    v.add(type);
                }
            }
            METHOD_TYPE_MAP.put(key, v);
        }
        System.out.println("METHOD_TYPE_MAP : "+METHOD_TYPE_MAP);
    }
}
```

DataSourceHandler

```java
public class DataSourceHandler {

    // 数据源名称
    public static final ThreadLocal<String> holder = new ThreadLocal<String>();

    /**
     * 在项目启动的时候将配置的读、写数据源加到holder中
     */
    public static void putDataSource(String datasource) {
        holder.set(datasource);
    }

    /**
     * 从holer中获取数据源字符串
     */
    public static String getDataSource() {
        return holder.get();
    }
}
```

DataSourceAspect

```java
@Aspect
@Component
@Order(-9999)
@EnableAspectJAutoProxy(proxyTargetClass = true)
public class DataSourceAspect {

    protected Logger logger = LoggerFactory.getLogger(this.getClass());

    /**
     * 配置前置通知,使用在方法aspect()上注册的切入点
     */
    @Before("execution(* cn.itcast.service.*.*(..))")
    @Order(-9999)
    public void before(JoinPoint point) {
        
        String className = point.getTarget().getClass().getName();
        String method = point.getSignature().getName();
        logger.info(className + "." + method + "(" + Arrays.asList(point.getArgs())+ ")");

        try {
            for (String key : ChooseDataSource.METHOD_TYPE_MAP.keySet()) {
                for (String type : ChooseDataSource.METHOD_TYPE_MAP.get(key)) {
                    if (method.startsWith(type)) {
                        System.out.println("key : " + key);
                        DataSourceHandler.putDataSource(key);
                        break;
                    }
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
        }

    }
}
```

通过 @Order(-9999) 注解来控制事务管理器, 与该通知类的加载顺序 , 需要让通知类 , 先加载 , 来判定使用哪个数据源 .



#### 4.11.3 验证

在主库和从库中，执行如下SQL语句，来查看是否读的时候， 从从库中读取 ； 写入操作的时候，是否写入到主库。

```sql
show status like 'Innodb_rows_%' ;
```

![1555235982584](D:/documents/notes/pictures/20230315/250.png) 



#### 4.11.4 原理

![1555235982584](D:/documents/notes/pictures/20230315/251.png)



### 4.12 性能优化 - 应用优化

#### 4.12.1 缓存

可以在业务系统中使用redis来做缓存，缓存一些基础性的数据，来降低关系型数据库的压力，提高访问效率。



#### 4.12.2 全文检索

如果业务系统中的数据量比较大（达到千万级别），这个时候，如果再对数据库进行查询，特别是进行分页查询，速度将变得很慢（因为在分页时首先需要count求合计数），为了提高访问效率，这个时候，可以考虑加入Solr 或者 ElasticSearch全文检索服务，来提高访问效率。



#### 4.13.3 非关系数据库

也可以考虑将非核心（重要）数据，存在 MongoDB 中，这样可以提高插入以及查询的效率。









# mariadb



## 数据库系统管理

```sql
use mysql;

# 查看用户及允许登录的 ip
select user,host from mysql.user;

# 授权用户 root 在所有 ip 登录，拥有所有库的所有表的权限
# 用户不存在时会先创建用户
# grant option：允许该用户在登录数据库时，能给其它用户进行授权操作
grant all privileges on *.* to 'root'@'%' identified by 'mariadb' with grant option;

# 新建数据库用户，用户允许在ip(localhost)登陆，(允许所有ip则替换为%)
create user 'username'@'ip' identified by 'password';

# 修改密码
# https://mariadb.com/kb/en/set-password/
set password for 'username'@'ip' = password('password');
# https://mariadb.com/kb/en/alter-user/
alter user 'username'@'ip' identitied via mysql_native_password using password('pwd2');

# 删除用户并删除授权
drop user 'username'@'ip';

# 刷新权限
FLUSH PRIVILEGES;
```



## Linux 系统安装

### 下载安装包

https://mariadb.org/download



### debian 10 安装 mariadb 10.6.10

#### 文件准备

```shell
#MariaDB Server Version: MariaDB Server 10.6.10
#Operating System: Linux
#Architecture: x86_64
#Init System: Systemd
#点击下载


#解压缩到目录/opt/mysql
sudo tar -zxvf mariadb-10.6.10-linux-systemd-x86_64.tar.gz -C /opt/mariadb
sudo mv /opt/mariadb/mariadb-10.6.10-linux-systemd-x86_64 /opt/mariadb/10.6.10

#建立data文件夹用于存放数据库文件
sudo mkdir /opt/mariadb/data/10.6.10
sudo mkdir /opt/mariadb/share/10.6.10
sudo mkdir /opt/mariadb/temp/10.6.10
sudo mkdir /var/logs/mariadb/10.6.10

#创建软链接（方便操作）
sudo ln -s /opt/mariadb/10.6.10 /opt/mariadb/mariadb
sudo ln -s /opt/mariadb/data/10.6.10 /opt/mariadb/my-data
sudo ln -s /opt/mariadb/share/10.6.10 /opt/mariadb/my-share
sudo ln -s /opt/mariadb/temp/10.6.10 /opt/mariadb/my-temp
sudo ln -s /var/logs/mariadb/10.6.10 /var/logs/mariadb/logs
```



#### 配置

```shell
# 新建/编辑文件:/etc/mariadb.cnf
[client]
port=3307
socket=/opt/mariadb/my-temp/mariadb.sock
default-character-set=utf8mb4
[mysql]
default-character-set=utf8mb4
[mysqld]
port=3307
socket=/opt/mariadb/my-temp/mariadb.sock
user=mariadb
basedir=/opt/mariadb/mariadb
datadir=/opt/mariadb/my-data
pid-file=/opt/mariadb/my-temp/maradb.pid
tmpdir=/opt/mariadb/my-temp/
slow_query_log_file=/var/logs/mariadb/logs/slow-query.log
init_connect='SET collation_connection = utf8_unicode_ci'
init_connect='SET NAMES utf8'
max_connections=200
max_connect_errors=10
character-set-server=utf8mb4
default-storage-engine=InnoDB
innodb_file_per_table=NO
ft_min_word_len=1
skip-name-resolve
skip-character-set-client-handshake
[mysqld_safe]
open-files-limit=8192
log-error=/var/logs/mariadb/logs/.err
```



#### 更新文件所属

```shell
#添加 mariadb 用户组(-r 创建系统工作组,系统工作组的组 ID 小于 500)
sudo groupadd -r mariadb
#添加 mariadb 用户(-r 建立系统帐号,-s /bin/false 参数指定 mariadb 用户仅拥有所有权,而没有登录权限)
sudo useradd -r -g mariadb -s /bin/false mariadb
#修改当前目录拥有者为新建的 mariadb 用户
sudo chown -R mariadb:mariadb /opt/mariadb
sudo chown -R mariadb:mariadb /var/logs/mariadb
```



#### 数据库安装及初始化

```shell
#安装数据库
sudo /opt/mariadb/mariadb/scripts/mariadb-install-db --defaults-file=/etc/mariadb.cnf
#如果报错
#/opt/mysql/8.0.31/bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory
#则执行下面两句
#sudo apt install numactl
#sudo apt install libaio1 libaio-dev

#正常安装之后会显示如下结果：
# Installing MariaDB/MySQL system tables in '/opt/mariadb/my-data' ...
# OK
# 
# To start mysqld at boot time you have to copy
# support-files/mysql.server to the right place for your system
# 
# 
# Two all-privilege accounts were created.
# One is root@localhost, it has no password, but you need to
# be system 'root' user to connect. Use, for example, sudo mysql
# The second is mariadb@localhost, it has no password either, but
# you need to be the system 'mariadb' user to connect.
# After connecting you can set the password, if you would need to be
# able to connect as any of these users with a password and without sudo
# 
# See the MariaDB Knowledgebase at https://mariadb.com/kb
# 
# You can start the MariaDB daemon with:
# cd '/opt/mariadb/mariadb' ; /opt/mariadb/mariadb/bin/mysqld_safe --datadir='/opt/mariadb/my-data'
# 
# You can test the MariaDB daemon with mysql-test-run.pl
# cd '/opt/mariadb/mariadb/mysql-test' ; perl mysql-test-run.pl
# 
# Please report any problems at https://mariadb.org/jira
# 
# The latest information about MariaDB is available at https://mariadb.org/.
# 
# Consider joining MariaDB's strong and vibrant community:
# https://mariadb.org/get-involved/

# 启动数据库
sudo /opt/mariadb/mariadb/bin/mariadbd --defaults-file=/etc/mariadb.cnf

# 初始化数据库
sudo /opt/mariadb/mariadb/bin/mariadb-secure-installation  --defaults-file=/etc/mariadb.cnf --basedir=/opt/mariadb/mariadb
# /opt/mariadb/mariadb/bin/mariadb: error while loading shared libraries: libncurses.so.5: cannot open shared object file: No such file or directory
# 如果发生上述错误则执行以下语句
# sudo apt install libncurses5

#print: /opt/mariadb/mariadb/bin/my_print_defaults
#
#NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB
#      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!
#
#In order to log into MariaDB to secure it, we'll need the current
#password for the root user. If you've just installed MariaDB, and
#haven't set the root password yet, you should just press enter here.
#
#Enter current password for root (enter for none): 
#OK, successfully used password, moving on...
#
#Setting the root password or using the unix_socket ensures that nobody
#can log into the MariaDB root user without the proper authorisation.
#
#You already have your root account protected, so you can safely answer 'n'.
#
#Switch to unix_socket authentication [Y/n] n
# ... skipping.
#
#You already have your root account protected, so you can safely answer 'n'.
#
#Change the root password? [Y/n] Y
#New password: 
#Re-enter new password: 
#Password updated successfully!
#Reloading privilege tables..
# ... Success!
#
#
#By default, a MariaDB installation has an anonymous user, allowing anyone
#to log into MariaDB without having to have a user account created for
#them.  This is intended only for testing, and to make the installation
#go a bit smoother.  You should remove them before moving into a
#production environment.
#
#Remove anonymous users? [Y/n] Y
# ... Success!
#
#Normally, root should only be allowed to connect from 'localhost'.  This
#ensures that someone cannot guess at the root password from the network.
#
#Disallow root login remotely? [Y/n] n
# ... skipping.
#
#By default, MariaDB comes with a database named 'test' that anyone can
#access.  This is also intended only for testing, and should be removed
#before moving into a production environment.
#
#Remove test database and access to it? [Y/n] Y
# - Dropping test database...
# ... Success!
# - Removing privileges on test database...
# ... Success!
#
#Reloading the privilege tables will ensure that all changes made so far
#will take effect immediately.
#
#Reload privilege tables now? [Y/n] Y
# ... Success!
#
#Cleaning up...
#
#All done!  If you've completed all of the above steps, your MariaDB
#installation should now be secure.
#
#Thanks for using MariaDB!

# 停止数据库
sudo ps -a
sudo kill -9 pid
```



#### 修改启动脚本

```sh
# 备份并修改文件 /opt/mariadb/mariadb/support-files/mysql.server
my_conf="/etc/mariadb.cnf"
# conf=/etc/my.cnf 
conf=$my_conf

# extra_args=""
# if test -r "$basedir/my.cnf"
# then
#   extra_args="--defaults-extra-file= $basedir/my.cnf"
# else
#   if test -r "$datadir/my.cnf"
#   then
#     extra_args="--defaults-extra-file= $datadir/my.cnf"
#   fi
# fi

# parse_server_arguments `$print_defaults $extra_args --mysqld mysql.server`
parse_server_arguments `$print_defaults -c $my_conf --mysqld mysql.server`
# $:wq/mysqld_safe --datadir="$datadir" --pid-file="$mariadbd_pid_file_path" "$@" & 
$bindir/mysqld_safe --defaults-file="$my_conf" "$@" &
```



#### 数据库启动服务

```shell
#将 maradb 进程放入系统进程中
sudo ln -s /opt/mariadb/mariadb/support-files/mysql.server /etc/init.d/mariadbd
#注册服务 如果文件有更新则运行 sudo systemctl daemon-reload
sudo update-rc.d mariadbd defaults

#启动、停止、重新启动mysql服务
sudo service mariadbd start
sudo service mariadbd stop
sudo service mariadbd restart
#或
sudo systemctl start mariadbd.service
sudo systemctl stop mariadbd.service
sudo systemctl restart mariadbd.service
sudo systemctl status mariadbd.service
```



#### 连接数据库

```shell
# 创建文件 /opt/mariadb/mariadb/bin/my-mariadb
# 输入如下
#!/bin/sh
/opt/mariadb/mariadb/bin/mariadb --defaults-file=/etc/mariadb.cnf "$@"

# 修改文件权限和 mariadb 一样
sudo chmod -R 755 /opt/mariadb/mariadb/bin/my-mariadb

#在 /usr/bin 下建立指向 mariadb 的软连接之后使用随机密码登录 mariadb 数据库
# 如果自定义了配置文件则使用 my-mariadb，否则使用 mariadb
sudo ln -s /opt/mariadb/mariadb/bin/my-mariadb /usr/bin/mariadb

# 连接数据库
mariadb -u root -p
```





#### 配置端口监听

```shell
ss -pl
# 看到mysql那行，如果输出 172.0.0.1:mysql 则现在只监听了 localhost 的连接
# 解决方法:修改/etc/mysql/my.cnf文件注释掉bind-address  = 127.0.0.1这行然后重启即可
# 重新ss -pl(*:mysql)说明可以远程连接了
```



#### 注意事项

```shell
# 安装完成后记得授权用户连接的 ip
```



### my.cnf

```
[client]
port = 3306
socket = /tmp/mariadb.sock
[mysqld]
port = 3306
socket = /tmp/mariadb.sock
user = mariadb
basedir = /usr/local/mariadb
datadir = /data/mariadb/db_file

skip-name-resolve               #禁止解析，可有效增加速度
back_log             = 50       #侦听队列中保持的连接数
max_connections     = 1500     #最大并发连接数
max_connect_errors  = 10       #最大错误连接数
table_open_cache    = 2048     #所有线程的打开表数
                              #+ 需要确保 open-files-limit 设置为至少4096

max_allowed_packet   = 16M   #服务器可以处理的查询数据包的最大大小
max_heap_table_size  = 64M   #单个HEAP表，在内存中的最大允许大小
read_buffer_size     = 2M    #用于执行全表扫描的缓冲区大小
read_rnd_buffer_size = 16M   #排序后按排序顺序读取行的缓冲区
sort_buffer_size     = 8M    #ORDER BY和GROUP BY查询执行排序缓冲区
join_buffer_size     = 8M    #该缓冲区用于优化完整的JOIN(不带索引的JOIN)

thread_cache_size  = 8     #在缓存中保留多少个线程以供重用
thread_concurrency = 8     #线程并发
thread_stack       = 240K  #要使用的线程堆栈大小

query_cache_size   = 64M   #SELECT 查询缓存区大小
query_cache_limit  = 2M    #缓存SELECT查询结果的最大值，小于此值的才缓存

slow_query_log             #开启慢查询日志。
slow_query_log_file = /data/mysql/log/slow-query.log
long_query_time = 2        #超过此时间的都属于慢查询

ft_min_word_len        = 1        #最小索引长度。涉及中文查看的，最好设置为1。
default-storage-engine = InnoDB   #创建新表时默认使用的存储引擎类型


tmp_table_size    = 64M    #临时表的最大大小
binlog_cache_size = 1M     #事务期间用于保存二进制日志的SQL语句的缓存大小
                          #+ 来自事务的所有语句都缓存在二进制日志缓存中，
                         #+ 并在COMMIT之后立即写入二进制日志，
                        #+ 如果事务大于此值，则使用磁盘上的临时文件。


transaction_isolation = REPEATABLE-READ   #默认事务隔离级别
#+ 可用级别：
#+ READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, SERIALIZABLE
init_connect = 'SET collation_connection = utf8_unicode_ci'
init_connect = 'SET NAMES utf8'
#init_connect 主要功能是：普通用户连接数据库时隐式执行的sql。

character-set-server  = utf8    #指定服务端字符集
collation-server  = utf8_unicode_ci   #指定服务端字符集的排序规则
skip-character-set-client-handshake = 1    #忽略启动或连接数据库时字符集的设置，使用默认的服务器字符集
log_slave_updates         #开启复制功能
slave_skip_errors = all   #主从复制时，跳过的错误。
#+ 通常，当主从复制发生错误时，复制会停止；此选项将跳过错误，使复制SQL线程继续复制。
#+ 共包含四个值：OFF、[list of error codes]、all、ddl_exist_errors。
server-id        = 1
log-bin          = /data/mysql/user/bin_log/mysql-bin3306
binlog_format    = mixed   #二进制日志记录格式
binlog_ignore_db = mysql   #不将数据库名为mysql的语句和事务写入二进制日志中(不够详细)
expire_logs_days = 7       #binlog日志文件保留时间，超过这个时间会被自动删除
auto_increment_offset    = 1   #自增列的起点，默认为 1。取值范围是1 .. 65535
auto_increment_increment = 2   #自增步长，默认为 1。取值范围是1 .. 65535
#在主主同步配置时，需要将两台服务器的 auto_increment_increment 都配置为 2，
#+ 而要把 auto_increment_offset 分别配置为 1 和 2。
#+ 这样才可以避免两台服务器同时做更新时自增长字段的值之间发生冲突。
#密钥缓冲区的大小，用于缓存MyISAM表的索引块。
#+ 不要设置为大于可用内存的30％，不使用MyISAM表也应将其设置为8-64M。
key_buffer_size = 32M

#高速缓存树的大小，为了获得最佳性能，请勿将其设置为大于 key_buffer_size。
bulk_insert_buffer_size = 64M

#当MariaDB需要在REPAIR、OPTIMIZE、ALTER table语句中重建索引，
#+ 以及在将数据填充到空表中时，会分配这个缓冲区。
myisam_sort_buffer_size = 128M

#重新创建索引时允许使用的临时文件的最大大小
myisam_max_sort_file_size = 10G

#如果一个表有多个索引，MyISAM可以使用多个线程通过并行排序来修复它们
myisam_repair_threads = 1

#自动检查并修复未正确关闭的MyISAM表
myisam_recover
#InnoDB用于存储元数据信息的附加内存池；如果超过此值，将从 OS 分配它。
innodb_additional_mem_pool_size = 16M

#InnoDB使用缓冲池来缓存索引和行数据。最多设置为物理内存大小的80％。
innodb_buffer_pool_size = 2G

#InnoDB表空间文件存储的位置，默认是 MariaDB 数据目录。
#innodb_data_home_dir = <directory>

#用于异步IO操作的IO线程数。
#+ 在Unix上，此值硬编码为8，但在Windows磁盘上，I O可能会受益于更大的数量。
innodb_write_io_threads = 8
innodb_read_io_threads = 8

#如果您遇到InnoDB表空间损坏的情况，请将其设置为非零值可能会帮助您转储表。
#+ 从值1开始并增加它，直到能够成功转储该表为止。
#innodb_force_recovery=1

#InnoDB内核中允许的线程数。
innodb_thread_concurrency = 16

#InnoDB 在事务提交后的日志写入频率
#+ 值为0时，log buffer会每秒写入到日志文件并flush到磁盘。
#+ 值为1时，每次事务提交时，log buffer会被写入到日志文件并刷写到磁盘。
#+ 默认值就是1，这是最安全的配置；但由于每次事务都需要进行磁盘I/O，所以也最慢。
#+ 值为2时，每次事务提交会写入日志文件，但并不会立即刷写到磁盘，日志文件会每秒刷写一次到磁盘。
#+ 这时如果mysqld进程崩溃，由于日志已经写入到系统缓存，所以并不会丢失数据；
#+ 但是在操作系统崩溃的情况下，通常会导致最后 1s 的日志丢失。
innodb_flush_log_at_trx_commit = 1


#innodb在关闭的时候该做什么工作
#+ 值为0时，在innodb关闭的时候，需要purge all, merge insert buffer,flush dirty pages。
#+ 这是最慢的一种关闭方式，但是restart的时候也是最快的。
#+ 值为1时，在innodb关闭的时候，它不需要purge all，merge insert buffer，
#+ 只需要flush dirty page,在缓冲池中的一些数据脏页会刷新到磁盘。
#+ 值为2时，在innodb关闭的时候，它不需要purge all，merge insert buffer，
#+ 也不进行flush dirty page，只将log buffer里面的日志刷新到日志文件log files,
#+ MySQL下次启动时，会执行恢复操作。
#innodb_fast_shutdown


#InnoDB用于缓冲日志数据的缓冲区大小。
#+ 装满后，InnoDB将不得不将其刷新到磁盘。
#+ 因为无论如何每秒都要刷新一次，所以将其过大也没有意义。
innodb_log_buffer_size = 8M

#日志组中每个日志文件的大小。
innodb_log_file_size = 256M

#日志组中的文件总数，通常，2-3就足够了。
innodb_log_files_in_group = 3

#InnoDB日志文件的位置。 默认值为MariaDB数据目录。
#innodb_log_group_home_dir

#InnoDB缓冲池中脏页的最大允许百分比。
innodb_max_dirty_pages_pct = 90

#InnoDB事务在回滚之前应等待多长时间才能授予锁。
#+ InnoDB自动在其自己的锁表中检测到事务死锁并回滚该事务，
#+ 如果您在同一事务中使用LOCK TABLES命令或InnoDB以外的其他事务安全存储引擎，
#+ 则可能会出现死锁，InnoDB无法注意到该死锁。
#+在这种情况下，超时对于解决这种情况很有用。
innodb_lock_wait_timeout = 120


#修改 InnoDB 为独立表空间模式，即每个数据库的每个表都会生成一个数据空间。
#+ MyISAM 引擎的表会分别创建三个文件：表结构、表索引、表数据空间；
#+ 我们可以将某个数据库目录直接迁移到其他数据库也可以正常工作。
#+ InnoDB 默认会将所有的InnoDB引擎的表数据存储在一个共享空间中(例如：ibdata1)，
#+ 这样在增删数据库的时候，ibdata1 文件不会自动收缩，
#+ 单表也不能在不同的数据库中移动，只能使mysqldump 导出，然后再导入。
innodb_file_per_table = ON
[mysqldump]
#+ 最大允许传输包的大小。、
#+ 也就是查询出结果后，把结果发送到客户端时，每个网络包的最大大小。
#+ 默认为16M，最大为1G，修改时需要设为1024的整数倍。
max_allowed_packet = 16M
[mysql]
#no-auto-rehash   #关闭自动补全表名和列名功能
auto-rehash       #开启自动补全表名和列名功能
[mysqlhotcopy]
#保持活动超时时间
interactive-timeout
[mysqld_safe]
#每个进程允许的打开文件数量
open-files-limit = 8192

log-error=/data/mysql/log/mysqld.log
pid-file=/data/mysql/temp/mysqld.pid

```



### .server

```shell

```



### 修改连接密码

`alter user 'root'@'localhost' identified by 'mariadb';`



### 忘记连接密码

- 编辑 ``.cnf` 文件
- 在mysqld下加入 `skip-grant-tables`
- 重启数据库后 `mariadb -u root -p` 连接数据库，输入密码直接回车
- 修改完密码后去掉 `skip-grant-tables` 再重启数据库





# Oracle

## 数据库系统管理



## 常用操作



### 序列

```sql
-- 查询序列
select sequence_name ,last_number, min_value,max_value,increment_by from user_sequences where sequence_name = '';

-- 当刚创建好序列后，不能用 currval 直接查询当前序列的值，必须先用 nextval 查询下一个序列的值，之后才可以使用currval查询当前序列的值
select 序列名.currval from dual;

-- 查询下一个序列的值,并且序列增长到下一个值
select 序列名.nextval from dual;

-- 设置序列为当前ID最大值+1
declare
  MAX_ID number(10);
  CURRENT_VALUE number(10);
BEGIN
	SELECT MAX(ID) INTO MAX_ID FROM 表;
	loop
  		select 序列名.nextval INTO CURRENT_VALUE from dual;
  		exit when CURRENT_VALUE >= MAX_ID;
	end loop;
END ;

-- 设置序列值
alter sequence 序列名 increment by 20 nocache;
select 序列名.nextval from dual;
alter sequence 序列名 increment by 1 nocache;
```



### ipv4-数字互转

###### ipv4 转数字

```sql
create or replace function ip2number(ip varchar2)
return number
is
  ip_num_hex varchar2(80);
begin
  if (regexp_like(ip, '^(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})$')) then
     ip_num_hex := lpad(trim(to_char(regexp_replace(ip, '^(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})$', '\1'), 'XX')),2,'0') ||
                   lpad(trim(to_char(regexp_replace(ip, '^(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})$', '\2'), 'XX')),2,'0') ||
                   lpad(trim(to_char(regexp_replace(ip, '^(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})$', '\3'), 'XX')),2,'0') ||
                   lpad(trim(to_char(regexp_replace(ip, '^(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})$', '\4'), 'XX')),2,'0');
     return to_number(ip_num_hex, 'XXXXXXXX');
  else
     return -1;
  end if;
exception
when others then
  return -99999999999;
end;
```



###### 数字转 ipv4

```sql
-- ipv4数字转ipv4
create or replace function number2ip(num number)
return varchar2
is
    ip_num_hex varchar2(80);
begin
    if num is null or num < 0 or num > 4294967295 then
        return null;
    else
        -- 左位移 = num * 2^位移位数，右侧补0，左侧丢弃
        -- 右位移 = num / 2^位移位数，左侧补0，右侧丢弃
        -- POWER(2, 24) = 16777216
        -- POWER(2, 16) = 65536
        -- POWER(2, 8) = 256
        return BITAND(num/16777216, 255) || '.' || BITAND(num/65536, 255) || '.' || BITAND(num/256, 255) || '.' || BITAND(num, 255);
    end if;
exception
when others then
    return null;
end;
```



### 函数/存储过程



#### 根据4、5G网元名获取地市代码

```sql
create or replace function ne_name2city_code(neName varchar2)
return varchar2
is

begin
	if (neName like '%gdAMF%') then -- GDSIGNAL.D_NW_NE_AMF_IPADDR
		return SUBSTR(neName, 7, INSTR(neName, 'gdAMF') - 7);
	elsif (neName like '%UPF%') then -- GDSIGNAL.D_NW_NE_UPF_IPADDR\GDSIGNAL.D_NW_NE_SGW
		return SUBSTR(neName, 1, INSTR(neName, 'UPF') - 1);
	elsif (neName like '%UPH%') then -- GDSIGNAL.D_NW_NE_UPF_IPADDR
		return SUBSTR(neName, 1, INSTR(neName, 'UPH') - 1);
	elsif (neName like '%MME%') then -- GDSIGNAL.D_NW_NE_MME
		return SUBSTR(neName, 1, INSTR(neName, 'MME') - 1);
	elsif (neName like '%SAEGW%') then -- GDSIGNAL.D_NW_NE_SGW
		return SUBSTR(neName, 1, INSTR(neName, 'SAEGW') - 1);
	elsif (neName like '%GGSN%') then -- GDSIGNAL.D_NW_NE_SGW
		return SUBSTR(neName, 1, INSTR(neName, 'GGSN') - 1);
	else
		return NULL;
	end if;
exception
when others then
  return NULL;
end;
```



### 常用 SQL

```sql
-- 表改名
ALTER TABLE table_name RENAME TO new_table_name;
-- 列改名
ALTER TABLE table_name RENAME COLUMN column_name TO new_column_name;
-- 修改空字段的数据类型
ALTER TABLE table_name MODIFY column_name TYPE;
-- 普通用户修改密码
alter user 用户名 identified by 新密码 replace 旧密码;
-- 排序时 NULL 排在最后
ORDER BY 字段 DESC NULLS LAST
```



#### 连接表更新

```sql
UPDATE F_HX_OATEZT_SM_SSCENE_H data_join
SET
    (data_join.CITY_OID, data_join.CITY_NAME)
    =
    (SELECT city_join.CITY_OID, city_join.CITY_NAME_CH FROM D_HX_CITY city_join WHERE ne_name2city_code(data_join.NE_NAME) = city_join.NE_CITY_CODE)
WHERE data_join.CITY_OID IS NULL
```



### 数据备份

#### 复制数据

```sql
-- 只复制数据，不复制主键及索引
CREATE TABLE 备份表 AS SELECT * FROM 原表;
```



### 时间

#### 时间加减

```sql
-- 加减 1 秒(86400)，分钟(1440)，小时(24)，天(1)
to_date('2022-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') + 1/86400

-- 加减小时
to_date('2022-03-31 23:59:59','yyyy-mm-dd hh24:mi:ss') + INTERVAL '-1' HOUR
-- 加减天
to_date('2022-03-31 23:59:59','yyyy-mm-dd hh24:mi:ss') + INTERVAL '-1' DAY
-- 加减月(如下月日期不存在会报错，1月31日加1个月会报错)
to_date('2022-04-01 23:59:59','yyyy-mm-dd hh24:mi:ss') + INTERVAL '-1' MONTH

-- numtodsinterval 常用的单位有 ('day','hour','minute','second')
to_date('2022-03-31 23:59:59','yyyy-mm-dd hh24:mi:ss')-numtodsinterval(1,'hour')
-- numtoyminterval 常用的单位有 ('year','month')，(如下月日期不存在会报错，1月31日加1个月会报错)
to_date('2022-03-31 23:59:59','yyyy-mm-dd hh24:mi:ss')+numtoyminterval(1,'month')

-- 加减月(1月31日加1个月不会报错)
add_months(to_date('2022-01-31 23:59:59','yyyy-mm-dd hh24:mi:ss'),1)
```



#### 字符串转时间

```sql
to_date('2022-03-31 23:59:59','yyyy-mm-dd hh24:mi:ss')
to_date(20220331235959,'yyyy-mm-dd hh24:mi:ss')
to_timestamp('2022-08-25 10:07:41.433000', 'syyyy-mm-dd hh24:mi:ss:ff6')
```



#### 时间转字符串

```sql
to_char(created_time,'yyyy-mm-dd hh24:mi:ss')
```



#### 时间转时间戳(13位)

```sql
round((to_date('2022081913', 'yyyymmddhh24') - to_date('1970-01-01 08:00:00', 'syyyy-mm-dd hh24:mi:ss')) * (1000 * 24 * 60 * 60))
```



#### 时间戳转时间

```sql
to_date('1970-01-01 08:00:00', 'syyyy-mm-dd hh24:mi:ss') + 1660885200000 / (1000 * 24 * 60 * 60)
```



#### 随机时间

```sql
# 缺点：可能会产生不存在的时间，例如：如果日随机1-30，则可能会产生：2月30号天只能随机
# 优点：可以对年、月、日、小时、分钟、秒单独进行控制
SELECT
    DBMS_RANDOM.VALUE(2022,2022) || '-' ||
    lpad(TRUNC(DBMS_RANDOM.VALUE(1,12)), 2, '0') || '-' ||
    lpad(TRUNC(DBMS_RANDOM.VALUE(1,28)), 2, '0') || ' ' ||
    lpad(TRUNC(DBMS_RANDOM.VALUE(0,23)), 2, '0') || ':' ||
    lpad(TRUNC(DBMS_RANDOM.VALUE(0,59)), 2, '0') || ':' ||
    lpad(TRUNC(DBMS_RANDOM.VALUE(0,59)), 2, '0')
FROM DUAL;

# 缺点：不能单独控制年、月、日，只能是两个日期之间的日期
# 优点：可以避免产生不存在的时间，例如：2月30号
SELECT
    to_date(
        TRUNC(
            DBMS_RANDOM.VALUE(
                to_number(to_char(to_date('20220701','yyyymmdd'),'J')),
                to_number(to_char(to_date('20221001','yyyymmdd'),'J'))
            )
        ),'J') +
    DBMS_RANDOM.VALUE(1,86400)/86400
FROM DUAL;
```



### 连续的数字、字符、时间

#### generate_series

```sql
-- sys.odcinumberlist 是 Oracle 预定义的变长数组类型；PIPELINED 表示定义管道表函数
CREATE OR REPLACE FUNCTION generate_series (pstart IN NUMBER, pstop IN NUMBER, pstep IN NUMBER DEFAULT 1)
RETURN sys.odcinumberlist DETERMINISTIC PIPELINED
AS
BEGIN
  IF (pstep = 0) THEN
    raise_application_error(-20001, 'step size cannot equal zero!');
  END IF;

  IF (pstart > pstop AND pstep > 0) OR (pstart < pstop AND pstep < 0) THEN
    RETURN;
  END IF;

  FOR i IN 0 .. floor(abs((pstop-pstart)/pstep)) LOOP
    PIPE ROW (pstart + i * pstep);
  END LOOP;

  RETURN;
END generate_series;

```



#### 数字

```sql
-- 1-6
SELECT ROWNUM FROM DUAL CONNECT BY LEVEL <= 6;
-- 或
SELECT * FROM TABLE(generate_series(1, 6));
-- 或，通用表表达式（Common Table Expression）的递归调用可以用于生成各种数列
WITH t(n) AS (
  SELECT 1 FROM DUAL
	UNION ALL
  SELECT n+1 FROM t WHERE n < 6
)
SELECT n FROM t;

-- 3-6
SELECT LEVEL FROM DUAL WHERE LEVEL >=3 CONNECT BY LEVEL <= 6;

-- 2-15 的等差数列
SELECT LEVEL FROM DUAL WHERE LEVEL >=2 AND MOD(LEVEL-2, 3) = 0 CONNECT BY LEVEL <= 15;

-- 15 - 1.4 的等差数列
SELECT ((LEVEL-1) * -2.5 + 15) FROM DUAL WHERE LEVEL >= 1 CONNECT BY ((LEVEL-1) * -2.5 + 15) >= 1.4;
SELECT * FROM TABLE(generate_series(15, 1.4, -2.5));
```



#### 字符

```sql
SELECT CHR(ROWNUM + 64) FROM DUAL CONNECT BY LEVEL <= 5;
-- 或
SELECT CHR(column_value) FROM TABLE(generate_series(65, 70));
-- 或
WITH t(n) AS (
  SELECT 65 FROM DUAL
  UNION ALL
  SELECT n+1 FROM t WHERE n <= 70
)
SELECT CHR(n) FROM t;
```



#### 时间

```sql
SELECT
	to_date('2020-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') + ((LEVEL - 1) / 24)
FROM DUAL
CONNECT BY LEVEL <= 12;
-- 或
SELECT to_date('2020-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') + ((column_value - 1) / 24)
FROM TABLE(generate_series(1, 12));
-- 或
WITH ts(v) AS (
  SELECT TIMESTAMP '2020-01-01 00:00:00' FROM DUAL
  UNION ALL
  SELECT v + 1/24 FROM ts WHERE v < TIMESTAMP '2020-01-01 12:00:00'
)
SELECT v FROM ts;
```



### 分组

```sql
select * from(
        select gar_logs.*, row_number() OVER (partition by username ORDER BY created_time desc) as rownum from gar_logs
)t where rownum = 1
```



### 分组合并非分组字段

```sql
LISTAGG(INDICATOR_NAME, '、' ) WITHIN GROUP (ORDER BY INDICATOR_NAME NULLS LAST )AS INDICATOR_NAMES
```



### LISTAGG 拼接结果过长

```sql
CREATE TYPE tab_varchar2 AS TABLE OF VARCHAR2(4000);

CREATE OR REPLACE FUNCTION concat_array(p tab_varchar2) RETURN CLOB IS
l_result CLOB;
BEGIN
	FOR cc IN (SELECT column_value FROM TABLE(p) ORDER BY column_value) LOOP
		l_result := l_result ||'、'|| cc.column_value;
	END LOOP;
	return l_result;
END;

SELECT
	TABLE_NAME,
	concat_array(CAST(COLLECT(COMMENTS) AS tab_varchar2)) COMMENTS
FROM all_col_comments
WHERE TABLE_NAME IN ()
GROUP BY TABLE_NAME;
```



### 获取表信息(注释、字段)

```sql
-- 表注释
select * from user_tab_comments;
-- 表字段注释
select * from user_col_comments;
-- 以上两个只能获取自己用户的表的注释信息，如果要访问自己能够访问的其他用户的表，则需要使用：
-- 表注释
select * from all_tab_comments;
-- 表字段注释
select * from all_col_comments;

-- 表字段
SELECT * FROM ALL_TAB_COLUMNS WHERE TABLE_NAME = ''
```



### 临时代码块

```sql
declare
    VAR_TIME number(10);
    VAR_DAY number(10);
    VAR_HOUR number(10);
    VAR_COUNT number(10);
begin
    VAR_TIME := 20230501;
    for i in 1 .. 10 loop
        VAR_DAY := VAR_TIME + i;
        DBMS_OUTPUT.PUT_LINE('天：'||VAR_DAY);
        for i in 0 .. 23 loop
            VAR_HOUR := VAR_DAY||'00' + i;
            DBMS_OUTPUT.PUT_LINE('小时：'||VAR_HOUR);
            SELECT COUNT(1) INTO VAR_COUNT FROM F_HX_N1N2_CITY_H WHERE HOUR_ID = VAR_HOUR;
            	IF VAR_COUNT <= 0 THEN
            		PROC_F_HX_N1N2_CITY_H(VAR_HOUR);
            END IF;
        end loop;
    end loop;
end;
```



### 空间统计

```sql
-- 统计表占用大小
select segment_name,sum(bytes)/1024/1024 from User_Extents group by segment_name;

-- 查看表的分区
select partition_name from user_tab_partitions where table_name='表名';

-- 统计表分区占用大小
select segment_name,partition_name,Sum(bytes)/1024/1024 from user_segments where segment_name= '表名' and partition_name ='分区名' group by segment_name,partition_name;

-- 统计表空间占用大小
select tablespace_name，sum(bytes)/1024/1024/1024 from user_segments group by tablespace_name;
```



### 查看连接数据库连接数

```sql
SELECT MACHINE,count(1) FROM (
    select LOGON_TIME,MACHINE,OSUSER from V$SESSION where username is not NULL  ORDER BY LOGON_TIME DESC
)
GROUP BY MACHINE;
```



### 修改数据非空字段的数据类型，并且不更改字段位置

```sql
alter table tb add name_bak NUMBER(19,4);

update tb set name_bak=name;

update tb set name=NULL;

alter table tb modify (name NUMBER(19,4));

update tb set name=name_bak;

alter table tb drop column name_bak;
```



### 存储过程定义使用数组

```sql
DECLARE
  /**
   *声明一个最多容纳100个数的varry数组，注意，它的下标是从1开始的。
   *即 binary_integer
   */
  type array_type is varray(100) of varchar(100);
  /**
   *分别定义一个直接赋值的和两个未赋值的数组。
   *注意：一定要初始化，但可以不赋值。对于没有赋值的这种数组，在用之前
   *也一定要先确定容量。
   */
  v_val_array array_type := array_type('one','two');
  v_val_array2 array_type := array_type();
  v_val_array3 array_type := array_type();
BEGIN
   /**
    *获取第一个varry数组中的值
    *varry的下标从1开始
    */
    dbms_output.put_line('v_val_array中下标1的值：'||v_val_array(1));
    dbms_output.put_line('v_val_array中下标2的值：'||v_val_array(2));


   /**
    *获取第二个varry数组中的值
    *因为第二个varry没有初始化长度，所以通过extend方法，
    *为该数组加一个空位
    */
    v_val_array2.extend;
    v_val_array2(1) :='aaa';
    v_val_array2.extend;
    v_val_array2(2) :='bbb';
    v_val_array2.extend;
    v_val_array2(3) :='ccc';
    dbms_output.put_line('v_val_array2中下标1的值：'||v_val_array2(1));
    dbms_output.put_line('v_val_array2中下标2的值：'||v_val_array2(2));
    dbms_output.put_line('v_val_array2中下标3的值：'||v_val_array2(3));

     /**
    *获取第三个varry数组中的值
    *因为第三个varry没有初始化长度，所以通过extend方法
    *初始化空位
    */

  /**
    *获取第二个varry数组中的值
    *因为第二个varry没有初始化长度，所以通过extend方法，
    *为该数组初始化长度
    */
    v_val_array3.extend(v_val_array2.count());
    v_val_array3(1) :='ddd';
    v_val_array3(2) :='eee';
    v_val_array3(3) :='fff';
    dbms_output.put_line('v_val_array3中下标1的值：'||v_val_array3(1));
    dbms_output.put_line('v_val_array3中下标2的值：'||v_val_array3(2));
    dbms_output.put_line('v_val_array3中下标3的值：'||v_val_array3(3));
END;
```



### 批量插入数据

```sql
INSERT ALL
INTO 表 (字段1, 字段2) VALUES (1, 'a')
INTO 表 (字段1, 字段2) VALUES (2, 'b')
SELECT 1 FROM DUAL;
-- insert all into其实是根据子查询执行了每个insert into子句，注意到上面SQL中每个into子句用的值都是字面量，子查询"select 1 from dual"返回1条记录，支持每个insert into子句插入指定的1条记录
```



### CONNECT BY

```sql
-- 假设 T 表里有 N 条数据
SELECT ID,LEVEL FROM T CONNECT BY LEVEL < M;
-- 那么会生成 N 棵高度为 M 的子树，然后先根遍历

-- 生成 1 - M
SELECT ROWNUM FROM DUAL CONNECT BY LEVEL <= M;
```



### 补充数字前导0

```sql
SELECT TO_CHAR(1, 'FM09') FROM DUAL;
SELECT lpad(1,2,'0') FROM DUAL;
```



### 插入数据时自增 id

```sql
CREATE SEQUENCE TASK_SEQ_ID
    MINVALUE 1
    NOMAXVALUE
    INCREMENT BY 1
    START WITH 1
NOCACHE;

CREATE OR REPLACE TRIGGER TASK_AUTO_ID BEFORE INSERT ON TASK_TABLE FOR EACH ROW WHEN(NEW.ID IS NULL)
BEGIN
    SELECT TASK_SEQ_ID.nextval INTO :NEW.ID FROM DUAL;
END;
```



### 插入数据时更新时间

```sql
CREATED_TIME DATE DEFAULT SYSDATE,
UPDATE_TIME DATE,

CREATE OR REPLACE TRIGGER F_HX_OATZET_ALARM_CONFIG_IUD BEFORE INSERT OR UPDATE ON F_HX_OATZET_ALARM_CONFIG FOR EACH ROW
BEGIN
    IF INSERTING THEN
        :NEW.CREATED_TIME := SYSDATE;
    ELSIF UPDATING THEN
        :NEW.UPDATE_TIME := SYSDATE;
END IF;
END;
```



### 删除索引

```sql
DROP INDEX F_HX_ASS_IND_POST_TASK_PK;
ALTER TABLE F_HX_ASS_IND_POST_TASK DROP CONSTRAINT F_HX_ASS_IND_POST_TASK_PK;
```



### to_char



```sql
select TRIM(to_char(TRUNC(2355/100), '09')) || ':' || TRIM(to_char(MOD(2355, 100), '09')) from dual;
```



### 按周统计数据

```sql
select
	week_num,
	to_char(to_date('2024-01-01 00:00:00', 'syyyy-mm-dd hh24:mi:ss') + week_num*7,'yyyy-mm-dd') AS created_time,
	count(1)
from(
	select
    	to_char(CREATED_TIME,'yyyy-mm-dd') AS created_time,
    	TO_CHAR(CREATED_TIME, 'D') AS day_of_week_number,
    	FLOOR(FLOOR(CREATED_TIME - to_date('2024-01-01 00:00:00', 'syyyy-mm-dd hh24:mi:ss')) / 7) AS week_num
    from TABLE
)
group by week_num
order by week_num asc
```



### 清空表过程

```sql
CREATE OR REPLACE PROCEDURE PROC_HX_CLEAN_TABLE(VAR_TABLE IN VARCHAR2) AS
    PART_NAME VARCHAR(30);
    VAR_IF_PART NUMBER;
    EXE_SQL VARCHAR2(1000);
    type t_part is table of VARCHAR2(50);
    VAR_PART t_part;
BEGIN
	SELECT COUNT(1) INTO VAR_IF_PART FROM user_tables WHERE table_name = VAR_TABLE AND UPPER(partitioned) = 'YES';
	if VAR_IF_PART>0 then
        PART_NAME := 'PART_2025000000';
        EXE_SQL := 'ALTER TABLE '||VAR_TABLE||' ADD PARTITION '||PART_NAME||' VALUES (2025000000)';
        dbms_output.put_line(EXE_SQL);
        execute immediate EXE_SQL;
		SELECT PARTITION_NAME bulk collect into VAR_PART FROM USER_TAB_PARTITIONS WHERE TABLE_NAME = VAR_TABLE;
		for j in 1 .. VAR_PART.count loop
		    if VAR_PART(j) != PART_NAME then
                EXE_SQL := 'ALTER TABLE '||VAR_TABLE||' DROP PARTITION '||VAR_PART(j);
                dbms_output.put_line(EXE_SQL);
                execute immediate EXE_SQL;
		    end if;
		end loop;
	else
		EXE_SQL := 'TRUNCATE TABLE '||VAR_TABLE;
		dbms_output.put_line(EXE_SQL);
		execute immediate EXE_SQL;
	end if;
END PROC_HX_CLEAN_TABLE;
```



### 遍历分区表分区

```sql
declare
    VAR_TABLE VARCHAR2(255);
    VAR_IF_PART NUMBER;
    EXE_SQL VARCHAR2(1000);
    type t_part is table of VARCHAR2(50);
    VAR_PART t_part;
begin
    VAR_TABLE := 'F_HX_NINE_DAY_API_LOG';
    SELECT COUNT(1) INTO VAR_IF_PART FROM user_tables WHERE table_name = VAR_TABLE AND UPPER(partitioned) = 'YES';
    if VAR_IF_PART>0 then
        DBMS_OUTPUT.PUT_LINE('分区表');
        SELECT PARTITION_NAME bulk collect into VAR_PART FROM USER_TAB_PARTITIONS WHERE TABLE_NAME = VAR_TABLE;
        for j in 1 .. VAR_PART.count loop
            DBMS_OUTPUT.PUT_LINE(VAR_PART(j));
        end loop;
    else
        DBMS_OUTPUT.PUT_LINE('非分区表');
    end if;
end;
```



## 查询优化



### FROM 表顺序

ORACLE在解析sql语句的时候对FROM子句后面的表名是从右往左解析的，是先扫描最右边的表，然后在扫描左边的表，然后用左边的表匹配数据，匹配成功后就合并。 所以，在对多表查询中，一定要把小表写在最右边。

```sql
--tableA：100w条记录  tableB：1w条记录 
--执行速度十秒
select count(*) from tableA, tableB;

--执行速度百秒甚至更高
select count(*) from tableB, tableA;
还有一种是三张表的查询，例如

select count(1) from tableA a,tableB b ,tableC c where a.id = b.id and a.id= c.id;
上面中tableA 为交叉表，根据oracle对From子句从右向左的扫描方式，应该把交叉表放在最末尾，然后才是最小表，所以上面的应该这样写:

--tableA a 交叉表,tabelB b 100w,tableC c 1w
select count(1) from tableB b ,tableC c ,tableA a where a.id=b.id and a.id=c.id;
```



### WHERE 条件顺序

ORACLE对where子句后面的条件过滤是自下向上，从右向左扫描的，所以和From子句一样一样的，把过滤条件排个序，按过滤数据的大小，自然就是最少数据的那个条件写在最下面，最右边，依次类推。

```sql
--不可取性能低下 
select * from tableA a where a.id > 500 and a.lx = '2b' and a.id < (select count( 1 )from tableA where id = a.id) 

--性能高 
select * from tableA a where a.id < (select count( 1 ) from tableA where id = a.id) and a.id > 500 and a.lx = '2b' 
```



### SELECT 字段

使用select的时候少用\*，写上字段名，因为ORACLE的查询器会把\*转换为表的全部列名，这个会浪费时间的，所以在大表中少用。



### commit

存储过程中需要注意的，多用commit了，既可以释放资源，但是要谨慎。



### 合理使用JOIN

使用合适的JOIN类型（INNER JOIN, LEFT JOIN, RIGHT JOIN）来减少不必要的数据处理。

```sql
--不推荐
SELECT e.emp_id, e.emp_name, d.dept_name
FROM employees e
LEFT JOIN departments d ON e.emp_department = d.dept_id
WHERE d.dept_name = 'IT';

--推荐
SELECT e.emp_id, e.emp_name, d.dept_name
FROM employees e
JOIN departments d ON e.emp_department = d.dept_id
WHERE d.dept_name = 'IT';
```



### 分析执行计划

```sql
EXPLAIN PLAN FOR
SELECT emp_id, emp_name
FROM employees
WHERE emp_department = 'IT';

SELECT * FROM TABLE(DBMS_XPLAN.DISPLAY);
```



### 表分区

```sql
CREATE TABLE sales (
    sale_id NUMBER PRIMARY KEY,
    sale_date DATE,
    amount NUMBER
)
PARTITION BY RANGE (sale_date) (
    PARTITION p_2021 VALUES LESS THAN (TO_DATE('2022-01-01', 'YYYY-MM-DD')),
    PARTITION p_2022 VALUES LESS THAN (TO_DATE('2023-01-01', 'YYYY-MM-DD'))
);

-- 查询特定分区的数据
SELECT sale_id, amount
FROM sales
WHERE sale_date BETWEEN TO_DATE('2021-01-01', 'YYYY-MM-DD') AND TO_DATE('2021-12-31', 'YYYY-MM-DD');
```



```sql
-- 创建分区表
create table TABLE_NAME
(
    HOUR_ID                 NUMBER(19),
    NAME					VARCHAR2(255)
) tablespace SOC_HANXIN_02
partition by list (HOUR_ID)(
    PARTITION PART_2021010100 VALUES (2021010100)
);

-- 创建分区
-- 清除分区
PART_NAME := 'PART_'||VAR_HOUR;
SELECT COUNT(*) INTO PART_COUNT FROM USER_TAB_PARTITIONS WHERE TABLE_NAME='TABLE_NAME' AND PARTITION_NAME=PART_NAME;
if PART_COUNT>0 then
    execute immediate 'ALTER TABLE TABLE_NAME DROP PARTITION '||PART_NAME;
end if;

-- 新增分区
execute immediate 'ALTER TABLE TABLE_NAME ADD PARTITION '||PART_NAME||' VALUES ('||VAR_HOUR||')';

-- 压缩分区
execute immediate 'ALTER TABLE TABLE_NAME MOVE PARTITION '||PART_NAME||' compress for query high parallel 8';


```



使用MyBatis创建分区

```xml
<select id="initPartition">
    <choose>
        <when test="isPartition">
            declare
                PART_COUNT NUMBER;
                PART_NAME VARCHAR(30);
            BEGIN
                PART_NAME := 'PART_'||#{targetTime};
                SELECT COUNT(*) INTO PART_COUNT FROM USER_TAB_PARTITIONS WHERE TABLE_NAME='TABLE_NAME' AND PARTITION_NAME=PART_NAME;
                if PART_COUNT>0 then
                	execute immediate 'ALTER TABLE TABLE_NAME DROP PARTITION '||PART_NAME;
                end if;
                execute immediate 'ALTER TABLE TABLE_NAME ADD PARTITION '||PART_NAME||' VALUES ('||#{targetTime}||')';
            END;
        </when>
        <otherwise>
            DELETE FROM TABLE_NAME WHERE HOUR_ID = #{targetTime}
        </otherwise>
    </choose>
</select>

<select id="compressPartition">
    <choose>
        <when test="isPartition">
            declare
                PART_NAME VARCHAR(30);
            BEGIN
                PART_NAME := 'PART_'||#{targetTime};
                execute immediate 'ALTER TABLE TABLE_NAME MOVE PARTITION '||PART_NAME||' COMPRESS for query high PARALLEL 8';
            END;
        </when>
        <otherwise>
            declare
            VAT_COUNT NUMBER(10);
            BEGIN
            SELECT 1 INTO VAT_COUNT FROM DUAL;
            END;
        </otherwise>
    </choose>
</select>
```



### 使用缓存

利用数据库缓存机制，避免重复读取相同的数据。

在Oracle中，可以使用`RESULT_CACHE`提示：

```sql
SELECT /*+ RESULT_CACHE */ emp_id, emp_name
FROM employees
WHERE emp_department = 'IT';
```



# PostgreSql



## 常用操作



#### 常用 SQL

```sql
-- 增加字段
ALTER TABLE table_name ADD COLUMN column_name text NOT NULL DEFAULT '';
```



#### 时间



##### 字符串转时间戳

```sql
to_timestamp('2022-03-31 23:59:59.999', 'yyyy-mm-dd hh24:mi:ss.ms')
```



##### 时间戳转字符串

```sql
to_char(created_time,'yyyy-mm-dd hh24:mi:ss.ms')
```



#### 序列

```sql
-- 查看序列
select * from gar_users_id_seq;
-- 设置序列到某个值
select setval('gar_users_id_seq',2200);
```



#### 分组

```sql
select * from(
        select gar_logs.*, row_number() OVER (partition by username ORDER BY created_time desc) as rownum from gar_logs
)t where rownum = 1
```



#### 分组合并非分组字段

```sql
string_agg(roles_permissions_join.permission_name, ',') as permission_name
```



#### 查看数据库账号

```sql
select u.usename, u.usesysid, 
	case 
		when u.usesuper and u.usecreatedb then 'superuser, create database'
		when u.usesuper then 'superuser'
		when u.usecreatedb then 'create database'
	end as attributes 
from pg_catalog.pg_user u
```



# H2

## 数据类型

| 类型描述               | 对应数据库中类型   | 值范围                                                       | 对应 Java 范围                        | 说明                               |
| ---------------------- | ------------------ | ------------------------------------------------------------ | ------------------------------------- | ---------------------------------- |
| 整数                   | INT                | -2147483648 到 2147483647                                    | Integer                               |                                    |
| 布尔型                 | BOOLEAN            | TRUE 或 FALSE                                                | Boolean                               |                                    |
| 微整数                 | TINYINT            | -128 到 127                                                  | Byte                                  |                                    |
| 小整数                 | SMALLINT           | -32768 到 32767                                              | Short                                 |                                    |
| 大整数                 | BIGINT             | -9223372036854775808 到 9223372036854775807                  | Long                                  |                                    |
| 标识符                 | IDENTITY           | -9223372036854775808 到 9223372036854775807                  | Long                                  | 使用的值不能再重用，即使事务回滚。 |
| 货币数                 | DECIMAL            | 固定整数位和小数位。这个数据类型经常用于存储货币等类型的值。 | BigDecimal                            |                                    |
| 双精度实数             | DOUBLE             | 浮点数。不能应用到表示货币等值，因为有四舍五入的问题。       | Double                                |                                    |
| 实数                   | REAL               | 单精度浮点数。不能应用到表示货币等值，因为有四舍五入的问题。 | Float                                 |                                    |
| 时间                   | TIME               | 格式为 hh:mm:ss                                              | Time                                  |                                    |
| 日期                   | DATE               | 格式为 yyyy-MM-dd                                            | Date                                  |                                    |
| 时间戳                 | TIMESTAMP          | 格式为 yyyy-MM-dd hh:mm:ss[.nnnnnnnnn]                       | Timestamp(Date 也支持)                |                                    |
| 二进制                 | BINARY             | 表示一个字节数组。针对更长的数组，使用 BLOB 类型。最大的尺寸为 2 GB，当使用这种数据类型时，整个对象都会保存在内存中，在内存中的尺寸是一个精确的指定值，只有实际的数据会被持久化。对于大的文本数据，还是使用 BLOB 和 CLOB 更合适。 | byte[]                                |                                    |
| 其他类型               | OTHER              | 这个类型允许存储可序列化的JAVA对象。在内部，使用的是一个字节数组。序列化和反序列化只在客户端端完成。反序列化仅在 getObject 被调用时才被调用。JAVA操作因为安全的原因并不能在数据库引擎内被执行。可以使用 PreparedStatement.setObject 存储对象。 | Object (或者是任何子类)               |                                    |
| 可变字符串             | VARCHAR            | Unicode 字符串。使用两个单引号(’’) 表示一个引用。最大的长度是Integer.MAX_VALUE，字符串的实际长度是精确指定的，仅实际的数据会被持久化。当使用这种数据类型时，整个文本都会保存在内存中。更多的文本数据，使用 CLOB 更合适。 | String                                |                                    |
| 不区分大小写可变字符串 | VARCHAR_IGNORECASE | 与 VARCHAR 类型类似，只是在比较时不区分大小写。存储时是混合大小写存储的。当使用这种数据类型时，整个文本都会保存在内存中。更多的文本数据，使用 CLOB 更合适。 | String                                |                                    |
| 字符                   | CHAR               | 这个类型支持是针对其他数据库或老的应用的兼容性。与VARCHAR 的不同是尾空格将被忽略并且不会被持久化。 Unicode 字符串。使用两个单引号(’’) 表示一个引用。最大的长度是Integer.MAX_VALUE，字符串的实际长度是精确指定的，仅实际的数据会被持久化。当使用这种数据类型时，整个文本都会保存在内存中。更多的文本数据，使用 CLOB 更合适。 | String                                |                                    |
| 二进制大对象           | BLOB               | 类似于BINARY，但是针对的是非常大的值如文件或是图片。跟BINARY不同的是，大对象并不完全保存在内存中。使用 PreparedStatement.setBinaryStream 存储对象，详细请参见 CLOB 和 高级 / 大对象。 | Blob (java.io.InputStream 也支持)     |                                    |
| 文本大对象             | CLOB               | CLOB类似于 VARCHAR，但是针对的是非常大的值。与 VARCHAR不同的是，CLOB 对象并不完全保存在内存中，而是使用的流。CLOB 可以用于文档或文本，如果XML、HTML文档，文本文件、未限制尺寸的备忘录等。使用 PreparedStatement.setCharacterStream 存储对象。详细请参见 高级 / 大对象。VARCHAR 用于相对较小的文本（如200个字符以内）。小的 CLOB 值被就地存储，但是也比 VARCHAR 要大。 | Clob (java.io.Reader 也支持)          |                                    |
| 通用唯一标识符         | UUID               | UUID（Universally unique identifier），是一个128BIT的值，使用 PreparedStatement.setBytes 或 setString 去存储值。 | UUID                                  |                                    |
| 数组                   | ARRAY              | 一组值，可以使用值列表 (1, 2) 或 PreparedStatement.setObject(…, new Object[] {…}) 存储对象。 | Object[] (没有任何原始类型数组被支持) |                                    |







# 面试题

https://article.itxueyuan.com/eoJEMj

## MySQL数据库的四类索引:

　　index ---- 普通索引,数据可以重复，没有任何限制。
　　unique  ---- 唯一索引,要求索引列的值必须唯一，但允许有空值；如果是组合索引，那么列值的组合必须唯一。

　　primary key ---- 主键索引,是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值，一般是在创建表的同时创建主键索引。

　　**组合索引 ----** 在多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。

　　fulltext ---- 全文索引,是对于大表的文本域：char，varchar，text列才能创建全文索引，主要用于查找文本中的关键字，并不是直接与索引中的值进行比较。fulltext更像是一个搜索引擎，配合match against操作使用，而不是一般的where语句加like。

　　注:全文索引目前只有MyISAM存储引擎支持全文索引，InnoDB引擎5.6以下版本还不支持全文索引

　　所有存储引擎对每个表至少支持16个索引，总索引长度至少为256字节，索引有两种存储类型，包括B型树索引和哈希索引。

　　索引可以提高查询的速度，但是创建和维护索引需要耗费时间，同时也会影响插入的速度，如果需要插入大量的数据时，最好是先删除索引，插入数据后再建立索引。

## **索引生效条件**

## 　　假设index（a,b,c）

- 最左前缀匹配：模糊查询时，使用%匹配时：’a%‘会使用索引，’%a‘不会使用索引
- 条件中有or，索引不会生效
- a and c，a生效，c不生效
- b and c，都不生效
- a and b > 5 and c,a和b生效，c不生效。

## 四大特性

- 原子性：不可分割的操作单元，事务中所有操作，要么全部成功；要么撤回到执行事务之前的状态
- 一致性：如果在执行事务之前数据库是一致的，那么在执行事务之后数据库也还是一致的；
- 隔离性：事务操作之间彼此独立和透明互不影响。事务独立运行。这通常使用锁来实现。一个事务处理后的结果，影响了其他事务，那么其他事务会撤回。事务的100%隔离，需要牺牲速度。
- 持久性：事务一旦提交，其结果就是永久的。即便发生系统故障，也能恢复。



## 连接

原文链接：https://blog.csdn.net/zjt980452483/java/article/details/82945663

### 内连接查询  inner join

关键字：inner  join   on

语句：select * from a_table a inner join b_table b on a.a_id = b.b_id;

说明：组合两个表中的记录，返回关联字段相符的记录，也就是返回两个表的交集（阴影）部分。



### 左连接查询 left join

关键字：left join on / left outer join on

语句：SELECT  * FROM a_table a left join b_table b ON a.a_id = b.b_id;

说明： left join 是left outer join的简写，它的全称是左外连接，是外连接中的一种。 左(外)连接，左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录。右表记录不足的地方均为NULL。


### 右连接 right join

关键字：right join on / right outer join on

语句：SELECT  * FROM a_table a right outer join b_table b on a.a_id = b.b_id;

说明：right join是right outer join的简写，它的全称是右外连接，是外连接中的一种。与左(外)连接相反，右(外)连接，左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来。左表记录不足的地方均为NULL。



### 全连接 union

关键字：union /union all

语句：(select colum1,colum2...columN from tableA ) union (select colum1,colum2...columN from tableB )

         或 (select colum1,colum2...columN from tableA ) union all (select colum1,colum2...columN from tableB )；

union语句注意事项：

         1.通过union连接的SQL它们分别单独取出的列数必须相同；
    
         2.不要求合并的表列名称相同时，以第一个sql 表列名为准；
    
         3.使用union 时，完全相等的行，将会被合并，由于合并比较耗时，一般不直接使用 union 进行合并，而是通常采用union all 进行合并；
    
         4.被union 连接的sql 子句，单个子句中不用写order by ，因为不会有排序的效果。但可以对最终的结果集进行排序；
    
           (select id,name from A order by id) union all (select id,name from B order by id); //没有排序效果
    
           (select id,name from A ) union all (select id,name from B ) order by id; //有排序效果



## 数据库三范式：

- 第一范式：1NF是对属性的原子性约束，要求字段具有原子性，不可再分解；(只要是关系型数据库都满足1NF)
- 第二范式：2NF是在满足第一范式的前提下，非主键字段不能出现部分依赖主键；解决：消除复合主键就可避免出现部分以来，可增加单列关键字。
- 第三范式：3NF是在满足第二范式的前提下，非主键字段不能出现传递依赖，比如某个字段a依赖于主键，而一些字段依赖字段a，这就是传递依赖。解决：将一个实体信息的数据放在一个表内实现。



## 脏读、幻读、不可重复读

**脏读: 是指事务T1将某一值修改，然后事务T2读取该值，此后T1因为某种原因撤销对该值的修改，这就导致了T2所读取到的数据是无效的。**

**不可重复读 ：是指在数据库访问时，一个事务范围内的两次相同查询却返回了不同数据。**在一个事务内多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么在第一个事务中的两次读数据之间，由于第二个事务的修改，第一个事务两次读到的的数据可能是不一样的。这样在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。

**幻读:** 是指当事务不是独立执行时发生的一种现象，比如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么就会发生，操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 

### **不可重复读&幻读区别:**

如果使用锁机制来实现这两种隔离级别，在可重复读中，该sql第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，**不能通过行锁来避免**。需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。

**不可重复读重点在于update和delete，而幻读的重点在于insert。如何通过锁机制来解决他们产生的问题**



## CHAR和VARCHAR的区别：

- CHAR和VARCHAR类型在存储和检索方面有所不同
- CHAR列长度固定为创建表时声明的长度，长度值范围是1到255
- 当CHAR值被存储时，它们被用空格填充到特定长度，检索CHAR值时需删除尾随空格。

 

## Mysql中的锁类型

- MyISAM支持表锁，InnoDB支持表锁和行锁，默认为行锁
- 表级锁：开销小，加锁快，不会出现死锁。锁定粒度大，发生锁冲突的概率最高，并发量最低
- 行级锁：开销大，加锁慢，会出现死锁。锁力度小，发生锁冲突的概率小，并发度最高

## 存储过程

我们常用的操作数据库语言SQL语句在执行的时候需要要先编译，然后执行，而存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中，用户通过指定存储过程的名字并给定参数（如果该存储过程带有参数）来调用执行它。

**优点：**

(1).存储过程增强了SQL语言的功能和灵活性。存储过程可以用流控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。

(2).存储过程允许标准组件是编程。存储过程被创建后，可以在程序中被多次调用，而不必重新编写该存储过程的SQL语句。而且数据库专业人员可以随时对存储过程进行修改，对应用程序源代码毫无影响。

(3).存储过程能实现较快的执行速度。如果某一操作包含大量的Transaction-SQL代码或分别被多次执行，那么存储过程要比批处理的执行速度快很多。因为存储过程是预编译的。在首次运行一个存储过程时查询，优化器对其进行分析优化，并且给出最终被存储在系统表中的执行计划。而批处理的Transaction-SQL语句在每次运行时都要进行编译和优化，速度相对要慢一些。

(4).存储过程能过减少网络流量。针对同一个数据库对象的操作（如查询、修改），如果这一操作所涉及的Transaction-SQL语句被组织程存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大增加了网络流量并降低了网络负载。

(5).存储过程可被作为一种安全机制来充分利用。系统管理员通过执行某一存储过程的权限进行限制，能够实现对相应的数据的访问权限的限制，避免了非授权用户对数据的访问，保证了数据的安全。

## delete、drop、truncate区别

- truncate 和 delete只删除数据，不删除表结构 ,drop删除表结构，并且释放所占的空间。
- **删除数据的速度，**drop> truncate > delete
- delete属于DML语言，需要事务管理，commit之后才能生效。drop和truncate属于DDL语言，操作立刻生效，不可回滚。
- **使用场合：**
  - 当你不再需要该表时， 用 drop;
  - 当你仍要保留该表，但要删除所有记录时， 用 truncate;
  - 当你要删除部分记录时（always with a where clause), 用 delete.

 

**注意：** 对于**有主外键关系的表**，不能使用truncate而应该**使用不带where子句的delete语句**，由于truncate不记录在日志中，不能够激活触发器